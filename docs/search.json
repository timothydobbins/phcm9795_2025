[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "",
    "text": "Course introduction\nWelcome to PHCM9795 Foundations of Biostatistics.\nThis introductory course in biostatistics aims to provide students with core biostatistical skills to analyse and present quantitative data from different study types. These are essential skills required in your degree and throughout your career.\nWe hope you enjoy the course and will value your feedback and comment throughout the course.",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Course information",
    "text": "Course information\nBiostatistics is a foundational discipline needed for the analysis and interpretation of quantitative information and its application to population health policy and practice.\nThis course is central to becoming a population health practitioner as the concepts and techniques developed in the course are fundamental to your studies and practice in population health. In this course you will develop an understanding of, and skills in, the core concepts of biostatistics that are necessary for analysis and interpretation of population health data and health literature.\nIn designing this course, we provide a learning sequence that will allow you to obtain the required graduate capabilities identified for your program. This course is taught with an emphasis on formulating a hypothesis and quantifying the evidence in relation to a specific research question. You will have the opportunity to analyse data from different study types commonly seen in population health research.\nThe course will allow those of you who have covered some of this material in your undergraduate and other professional education to consolidate your knowledge and skills. Students exposed to biostatistics for the first time may find the course challenging at times. Based on student feedback, the key to success in this course is to devote time to it every week. We recommend that you spend an average of 10-15 hours per week on the course, including the time spent reading the course notes and readings, listening to lectures, and working through learning activities and completing your assessments. Please use the resources provided to assist you, including online support.",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "index.html#units-of-credit",
    "href": "index.html#units-of-credit",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Units of credit",
    "text": "Units of credit\nThis course is a core course of the Master of Public Health, Master of Global Health and Master of Infectious Diseases Intelligence programs and associated dual degrees, comprising 6 units of credit towards the total required for completion of the study program. A value of 6 UOC requires a minimum of 150 hours work for the average student across the term.",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "index.html#course-aim",
    "href": "index.html#course-aim",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Course aim",
    "text": "Course aim\nThis course aims to provide students with the core biostatistical skills to apply appropriate statistical techniques to analyse and present population health data.",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn successful completion of this course, you will be able to:\n\nSummarise and visualise data using statistical software.\nDemonstrate an understanding of statistical inference by interpreting p-values and confidence intervals.\nApply appropriate statistical tests for different types of variables given a research question, and interpret computer output of these tests appropriately.\nDetermine the appropriate sample size when planning a research study.\nPresent and interpret statistical findings appropriate for a population health audience.",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "index.html#change-log",
    "href": "index.html#change-log",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Change log",
    "text": "Change log",
    "crumbs": [
      "Course introduction"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "",
    "text": "Learning objectives\nBy the end of this module, you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#learning-objectives",
    "href": "01-intro.html#learning-objectives",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "",
    "text": "Understand the difference between descriptive and inferential statistics\nDistinguish between different types of variables\nPresent and report continuous data numerically and graphically\nCompute summary statistics to describe the centre and spread of data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#optional-readings",
    "href": "01-intro.html#optional-readings",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapters 2 and 3. [UNSW Library Link]\nBland (2015); Chapter 4. [UNSW Library Link]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#an-introduction-to-statistics",
    "href": "01-intro.html#an-introduction-to-statistics",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.1 An introduction to statistics",
    "text": "1.1 An introduction to statistics\nThe dictionary of statistics (Upton and Cook, 2008) defines statistics simply as: “The science of collecting, displaying, and analysing data.”\nStatistics is a branch of mathematics, and there are two main divisions within the field of statistics: mathematical statistics and applied statistics. Mathematical statistics deals with development of new methods of statistical inference and requires detailed knowledge of abstract mathematics for its implementation. Applied statistics applies the methods of mathematical statistics to specific subject areas, such as business, psychology, medicine and sociology.\nBiostatistics can be considered as the “application of statistical techniques to the medical and health fields”. However, biostatistics sometimes overlaps with mathematical statistics. For instance, given a certain biostatistical problem, if the standard methods do not apply then existing methods must be modified to develop a new method.\n\nScope of Biostatistics\nResearch is essential in the practice of health care. Biostatistical knowledge helps health professionals in deciding whether to prescribe a new drug for the treatment of a disease or to advise a patient to give up drinking alcohol. To practice evidence-based healthcare, health professionals must keep abreast of the latest research, which requires understanding how the studies were designed, how data were collected and analysed, and how the results were interpreted. In clinical medicine, biostatistical methods are used to determine the accuracy of a measurement, the efficacy of a drug in treating a disease, in comparing different measurement techniques, assessing diagnostic tests, determining normal values, estimating prognosis and monitoring patients. Public health professionals are concerned about the administration of medical services or ensuring that an intervention program reduces exposure to certain risk factors for disease such as life-style factors (e.g. smoking, obesity) or environmental contaminants. Knowledge of biostatistics helps determine them make decisions by understanding, from research findings, whether the prevalence of a disease is increasing or whether there is a causal association between an environmental factor and a disease.\nThe value of biostatistics is to transform (sometimes vast amounts of) data into meaningful information, that can be used to solve problems, and then be translated into practice (i.e. to inform public health policy and decision making). When undertaking research having a biostatistician as part of a multidisciplinary team from the outset, together with scientists, clinicians, epidemiologists, healthcare specialists is vital, to ensure the validity of the research being undertaken and that information is interpreted appropriately.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#what-are-data",
    "href": "01-intro.html#what-are-data",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.2 What are data?",
    "text": "1.2 What are data?\nAccording to the Australian Bureau of Statistics, “data are measurements or observations that are collected as a source of information”.1 Note that technically, the word data is a plural noun. This may sound a little odd, but it means that we say “data are …” when discussing a set of measurements.\nOther definitions that we use in this course are:\n\nobservation, (or record, or unit record): one individual in the population being studied\nvariable: a characteristic of an individual being measured. For example, height, weight, eye colour, income, country of birth are all types of variables.\ndataset: the complete collection of all observations\n\n\nTypes of variables\nWe can categorise variables into two main types: numeric or categorical.\nNumerical variables (also called quantitative variables) comprise data that must be represented by a number, which can be either measured or counted.\nContinuous variables can take any value within a defined range.\nFor example, age, height, weight or blood pressure, are continuous variables because we can make any divisions we want on them, and they can be measured as small as the instrument allows. As an illustration, if two people have the same blood pressure measured to the nearest millimetre of mercury, we may get a difference between them if the blood pressure is measured to the nearest tenth of millimetre. If they are still the same (to the nearest tenth of a millimetre), we can measure them with even finer gradations until we can see a difference.\nDiscrete variables can only take one of a distinct set of values (usually whole numbers). For discrete variables, observations are based on a quantity where both ordering and magnitude are important, such that numbers represent actual measurable quantities rather than mere labels.\nFor example, the number of cancer cases in a specified area emerging over a certain period, the number of motorbike accidents in Sydney, the number of times a woman has given birth, the number of beds in a hospital are all discrete variables. Notice that a natural ordering exists among the data points, that is, a hospital with 100 beds has more beds than a hospital with 75 beds. Moreover, a difference between 40 and 50 beds is the same as the difference between 80 and 90 beds.\nCategorical variables comprise data that describe a ‘quality’ or ‘characteristic’. Categorical variables, sometimes called qualitative variables, do not have measurable numeric values. Categorical variables can be nominal or ordinal.\nA nominal variable consists of unordered categories. For example, gender, race, ethnic group, religion, eye colour etc. Both the order and magnitude of a nominal variable are unimportant.\nIf a nominal variable takes on one of two distinct categories, such as black or white then it is called a binary or dichotomous variable. Other examples would be smoker or non-smoker; exposed to arsenic or not exposed.\nA nominal variable can also have more than two categories, such as blood group, with categories of: Group A, Group B, Group AB and Group O.\nOrdinal variables consist of ordered categories where differences between categories are important, such as socioeconomic status (low, medium, high) or student evaluation rating could be classified according to their level of satisfaction: (highly satisfied, satisfied and unsatisfied). Here a natural order exists among the categories.\nNote that categorical variables are often stored in data sets using numbers to represent categories. However, this is for convenience only, and these variable must not be analysed as if they were numeric variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#descriptive-and-inferential-statistics",
    "href": "01-intro.html#descriptive-and-inferential-statistics",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.3 Descriptive and inferential statistics",
    "text": "1.3 Descriptive and inferential statistics\nWhen analysing a set of data, it is important to consider the aims of the analysis and whether these are descriptive or inferential. Essentially, descriptive statistics summarise data from a single sample or population, and present a “snap-shot” of those data. Inferential statistics use sample data to make statements about larger populations.\n\nDescriptive statistics\nDescriptive statistics provide a ‘picture’ of the characteristics of a population, such as the average age, or the proportion of people born in Australia. Two common examples of descriptive statistics are reports summarising a nation’s birth statistics, and death statistics.\n\nBirths\nThe Australian Institute of Health and Welfare produces comprehensive reports on the characteristics of Australia’s mothers and babies using the most recent year of data from the National Perinatal Data Collection. The National Perinatal Data Collection comprises all registered births in Australia.\nThe most recent report, published in 2024, summarises Australian births from 2022. (Australian Institute of Health and Welfare (2024)).\nOne headline from the report is that “More First Nations mothers are accessing antenatal care in the first trimester (up from 51% in 2013 to 71% in 2022)”. The report presents further descriptive statistics, such as the average maternal age (31.2 years) and the proportion of women giving birth by caesarean (39%).\n\n\nDeaths\nIn another example, consider characteristics of all deaths in Australia in 2023 (Australian Bureau of Statistics (Thu, 10/10/2024 - 11:30)).\n\n“COVID-19 was the ninth leading cause of death in 2023, after ranking third in 2022.”\n\nThe report presents the leading causes of death in 2023:\n\n“The leading cause of death was ischaemic heart disease, accounting for 9.2% of deaths. The gap between ischaemic heart disease and dementia (the second leading cause of death) has continued to narrow over time, with only 237 deaths separating the top two leading causes in 2023.”\n\nThe top five causes of death are also presented as a graph, enabling a simple comparison of the changes in rates of death between 2014 and 2023.\n\n\n\n\n\n\n\n\nFigure 1.1: Leading causes of death, age-standardised death rates, 2014-2023\n\n\n\n\n\n\n\n\nInferential statistics\nInferential statistics use data collected from a sample to make conclusions (inferences) about the whole population from which the sample was drawn. For example, the Australian Institute of Health and Welfare’s Australia’s health reports (eg Australian Institute of Health and Welfare (2025)) use a representative sample to make estimates of the health of the whole of Australia. We will revisit inferential statistics in later modules.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#summarising-continuous-data",
    "href": "01-intro.html#summarising-continuous-data",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.4 Summarising continuous data",
    "text": "1.4 Summarising continuous data\nIn the first two Modules, we will focus on ways to summarise and present data. We will see that the choice of presentation will depend on the type of variable being summarised. In this Module, we will focus on continuous variables, and will focus on categorical data in Module 2.\n\nSummarising a single continuous variable numerically\nWhen summarising continuous data numerically, there are two things we want to know:\n\nWhat is the average value? And,\nHow variable (or spread out) are the data?\n\nWe will use a sample of 35 ages (in whole years) to illustrate how to calculate the average value and measures of variability:\n59 41 44 43 31 47 53 59 35 60 54 61 67 52 43 46 39 69 50 64 57 39 54 50 51 31 48 49 70 44 60 51 37 53 34\n\nMeasures of central tendency\n\n\nMean\nThe most commonly used measure of the central tendency of the data is the mean, calculated as:\n\\[\\bar{x} = \\frac{\\sum x}{n}\\]\nFrom the age example: \\(\\bar{x}\\) = 1745/35 = 49.9. Thus, the mean age of this sample is 49.9 years.\n\n\nMedian\nOther measures of central tendency include the median and mode. The median is the middle value of the data, the value at which half of the measurements lie above it and half of the measurements lie below it.\nTo estimate the median, the data are ordered from the lowest to highest values, and the middle value is used. If the middle value is between two data points (if there are an even number of observations), the median is an average of the two values.\nUsing our example, we could rank the ages from smallest to largest, and locate the middle value (which has been bolded):\n31 31 34 35 37 39 39 41 43 43 44 44 46 47 48 49 50 50 51 51 52 53 53 54 54 57 59 59 60 60 61 64 67 69 70\nHere, the median age is 50 years.\nNote that, in practice, the median is usually calculated by software automatically, and there is no need to rank our data.\n\n\nDescribing the spread of the data\nIn addition to measuring the centre of the data, we also need an estimate of the variability, or spread, of the data points.\n\n\nRange\nThe absolute measure of the spread of the data is the range, that is the difference between the highest and lowest values in the dataset.\nRange = highest data value – lowest data value\nUsing the age example, Range = 70 - 31 = 39 years.\nThe range is most usefully reported as the actual lowest and highest values e.g. Range: 31 to 70 years.\nThe range is not always ideal as it only describes the extreme values, without considering how the bulk of the data is distributed between them.\n\n\nVariance and standard deviation\nMore useful statistics to describe the spread of the data around a mean value are the variance and standard deviation. These measures of variability depend on the difference between individual observations and the mean value (deviations). If all values are equal to the mean there would be no variability at all, all deviations would be zero; conversely large deviations indicate greater variability.\nOne way of combining deviations in a single measure is to first square the deviations and then average the squares. Squaring is done because we are equally interested in negative deviations and positive deviations; if we averaged without squaring, negative and positive deviations would ‘cancel out’. This measure is called the variance of the set of observations. It is ‘the average squared deviation from the mean’. Because the variance is in ‘square’ units and not in the units of the measurement, a second measure is derived by taking the square root of the variance. This is the standard deviation (SD), and is the most commonly used measure of variability in practice, as it is a more intuitive interpretation since it is in the same units as the units of measurement.\nThe formula for the variance of a sample (\\(s^2\\)) is:\n\\[ s^2 = \\frac{\\sum(x - \\bar{x})^2}{n-1} \\]\nNote that the deviations are first squared before they are summed to remove the negative values; once summed they are divided by the sample size minus 1.\nThe sample standard deviation is the square root of the of the sample variance:\n\\[s = \\sqrt{s^2}\\] For the age example, we would calculate the sample variance using statistical software. The sample standard deviation is estimated as: \\(s = 10.47 \\text{ years}\\).\nCharacteristics of the standard deviation:\n\nIt is affected by every measurement\nIt is in the same units as the measurements\nIt can be converted to measures of precision (standard error and 95% confidence intervals) (Module 3)\n\n\n\nInterquartile range\nThe inter-quartile range (IQR) describes the range of measurements in the central 50% of values lie. This is estimated by calculating the values that cut the data at the bottom 25% and top 25%. The IQR is the preferred measure of spread when the median has been used to describe central tendency.\nIn the age example, the IQR is estimated as 43 to 58 years.\n\n\nPopulation values: mean, variance and standard deviation\nThe examples above show how the sample mean, range, variance and standard deviation are calculated from the sample of ages from 35 people. If we had information on the age of the entire population that the sample was drawn from, we could calculate all the summary statistics described above (for the sample) for the population.\nThe equation for calculating the population mean is the same as that of sample mean, though now we denote the population mean as \\(\\mu\\):\n\\[ \\mu = \\frac{\\sum{x}}{N} \\]\nWhere \\(\\sum{x}\\) represents the sum of the values in the population, and \\(N\\) represents the total number of measurements in the population.\nTo calculate the population variance (\\(\\sigma^2\\)) and standard deviation(\\(\\sigma\\)), we use a slightly modified version of the equation for \\(s^2\\):\n\\[ \\sigma^2 = \\frac{\\sum(x - \\mu)^2}{N} \\]\nwith a population standard deviation of: \\(\\sigma = \\sqrt{\\sigma^2}\\).\nIn practice, we rarely have the information for the entire population to be able to calculate the population mean and standard deviation. Theoretically, however, these statistics are important for two main purposes:\n\nthe characteristics of the normal distribution (the most important probability distribution discussed in later modules) are defined by the population mean and standard deviation;\nwhile calculating sample sizes (discussed in later modules) we need information about the population standard deviation, which is usually obtained from the existing literature.\n\n\n\n\nSummarising a single continuous variable graphically\nAs well as calculating measures of central tendency and spread to describe the characteristics of the data, a graphical plot can be helpful to better understand the characteristics and distribution of the measurements obtained. Histograms, density plots and box plots are excellent ways to display continuous data graphically.\n\nFrequency histograms\nA frequency histogram is a plot of the number of observations that fall within defined ranges of non-overlapping intervals (called bins). Examples of frequency histograms are given in Figure 1.2.\n\n\n\n\n\n\n\n\nFigure 1.2: Density plots of age (left) and serum bilirubin (right) from PBC data\n\n\n\n\n\nSome features of a frequency histogram:\n\nThe area under each rectangle is proportional to the frequency\nThe rectangles are drawn without gaps between them (that is, the rectangles touch)\nThe data are ‘binned’ into discrete intervals (usually of equal width)\n\nA slight variation on the frequency histogram is the density histogram, which plots the density on the y-axis. The density is a technical term, which is similar to the relative frequency, but is scaled so that the sum of the area of the bars is equal to 1.\nBoth the frequency and density histograms are useful for understanding how the data is distributed across the range of values. Taller bars indicate regions where the data is more densely concentrated, while shorter bars represent areas with fewer data points.\n\n\nDensity plot\nA density plot can be thought of as a smoothed version of a density histogram. Like histograms, density plots show areas where there are a lot of observations and areas where there are relatively few observations. Figure 1.3 illustrates example density plots for the same data as plotted in Figure 1.2.\n\n\n\n\n\n\n\n\nFigure 1.3: Histogram of age (left) and serum bilirubin (right) from a sample of data\n\n\n\n\n\nLike histograms, density plots allow you to see the overall shape of a distribution. They are most useful when there are only a small number of observations being plotted. When plotting small datasets, the shape of a histogram can depend on how the bins are defined. This is less of an issue if a density plot is used.\n\n\nBoxplots\nAnother way to inspect the distribution of data is by using a box plot. In a box plot:\n\nthe line across the box shows the median value\nthe limits of the box show the 25-75% range (i.e. the inter-quartile range (IQR) where the middle 50% of the data lie)\nthe bars (or whiskers) indicate the most extreme values (highest and lowest) that fall within 1.5 times the interquartile range from each end of the box\n\nthe upper whisker is the highest value falling within 75th percentile plus 1.5 × IQR\nthe lower whisker is the lowest value falling within 25th percentile minus 1.5 × IQR\n\nany values in the dataset lying outside the whiskers are plotted individually.\n\nFigure 1.4 presents two example boxplots for age and serum bilirubin.\n\n\n\n\n\n\n\n\nFigure 1.4: Box plot of age (left) and serum bilirubin (right) from PBC study data\n\n\n\n\n\n\n\n\nThe shape of a distribution\nHistograms and density plots allow us to consider the shape of a distribution, and in particular, whether a distribution is symmetric or skewed.\nIn a histogram, if the rectangles fall in a roughly symmetric shape around a single midpoint, we say that the distribution is symmetric. Similarly, if a density plot looks roughly symmetric around a single point, the distribution is symmetric.\nIf the histogram or density plot has a longer tail to the right, then the data are said to be positively skewed (or skewed to the right); if the histogram or density plot has an extended tail to the left, then the data are negatively skewed (or skewed to the left).\n\nThe skewness of a distribution is defined by the location of the longer tail in a histogram or density plot, not the location of the peak of the data.\n\nFrom Figure 1.2 and Figure 1.3, we can see that the distribution for age is roughly symmetric, while the distribution for serum bilirubin is highly positively skewed (or skewed to the right).\nWhile it is technically possible to determine the shape of a distribution using a boxplot, a histogram or density plot gives a more complete illustration of a distribution and would be the preferred method of assessing shape.\n\n\nWhich measure of central tendency to use\nWe introduced the mean and median in Section 1.4.1.1 as measures of central tendency. We need to assess the shape of a distribution to answer which is the more appropriate measure to use.\nIf a distribution is symmetric, the mean and median will be approximately equal. However, the mean is the preferred measure of central tendency as it makes use of every data point, and has more useful mathematical properties.\nThe mean is not a good measure of central tendency for skewed distributions, as the calculation will be influenced by the observations in the tail of the distribution. The median is the preferred statistic for describing central tendency in a skewed distribution.\nIf the data exhibits a symmetric distribution, we use the standard deviation as the measure of spread. Otherwise, the interquartile range is preferred.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-eda-cts",
    "href": "01-intro.html#sec-eda-cts",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.5 Exploratory data analysis for continuous data",
    "text": "1.5 Exploratory data analysis for continuous data\nBefore conducting any formal analysis, it is good practice to undertake exploratory data analysis. This analysis step gives you a high-level overview of your data: what do your data look like, what are the main features, what shape is the distribution, and are there any unusual values.\nExploring continuous data is best done by examining plots: specifically density plots and boxplots. Density plots are useful in determining the shape of a distribution, to help you decide what summary measures to use, and what type of analysis to conduct. Boxplots can be useful in identifying any unusual points - often called outliers.Outliers can be problematic and the decision to include them or omit them from further analyses can be difficult.\nAfter detecting any outliers or extreme values, do not automatically exclude them from the analysis. First, it is important to check the original data collection form or questionnaire to rule out the possibility of a data entry error. If the outlier is not a data entry error, it is then important to decide whether the observation is biologically possible. This step will usually need to be answered by a topic expert. If the outlier is biologically possible, it must be included in the analysis. Only if the outlier is biologically impossible should it be set to missing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#learning-outcomes",
    "href": "01-intro.html#learning-outcomes",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of these notes, you will be able to:\n\nnavigate the jamovi interface\ninput and import data into jamovi\nuse jamovi menus to summarise data\nperform basic data transformations\nassign variable and value labels\nunderstand the difference between saving data and saving jamovi output\ncopy jamovi output to a standard word processing package",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#introduction",
    "href": "01-intro.html#introduction",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.6 Introduction",
    "text": "1.6 Introduction\nFrom the jamovi website:\n\njamovi is a new “3rd generation” statistical spreadsheet. Designed from the ground up to be easy to use, jamovi is a compelling alternative to costly statistical products such as SPSS and SAS.\n\n\njamovi is built on top of the R statistical language, giving you access to the best the statistics community has to offer. Would you like the R code for your analyses? jamovi can provide that too.\n\n\njamovi will always be free and open - that’s one of our core values - because jamovi is made by the scientific community, for the scientific community.\n\nThe notes provided in this course will cover the basics of using jamovi: there is much more to jamovi than we will cover in this course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#part-1-an-introduction-to-jamovi",
    "href": "01-intro.html#part-1-an-introduction-to-jamovi",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.7 Part 1: An introduction to jamovi",
    "text": "1.7 Part 1: An introduction to jamovi\nIn this very brief section, we will introduce jamovi by calculating the average of six ages. Open the jamovi package in the usual way (note that while jamovi is available for Windows, MacOS and linux, most of the screenshots in these notes will be based on the macOS version.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#installing-jamovi",
    "href": "01-intro.html#installing-jamovi",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.8 Installing jamovi",
    "text": "1.8 Installing jamovi\njamovi can be downloaded for no cost at https://www.jamovi.org/download.html for Windows, macOS and linux. At the time of writing, Version 2.6.26 solid is the appropriate version to use. Download and install jamovi in the usual way.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#a-simple-jamovi-analysis",
    "href": "01-intro.html#a-simple-jamovi-analysis",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.9 A simple jamovi analysis",
    "text": "1.9 A simple jamovi analysis\nWhen you first open jamovi, it will look something like the following.\n On the left-hand side of the window is the spreadsheet view, and the right is where results of statistical analyses will appear. The spreadsheet is where data can be entered or changed.\n\n\n\n\n\n\nTASK\n\n\n\nEnter the following six ages into jamovi, starting at the top-left cell, by typing each number and then hitting Enter:\n20 25  23  29  21  27\nIf you make a mistake, simply click the incorrect cell, and enter the correct value.\n\n\nYour screen should look like this:\n\nThere are two things to note here:\n\nData in jamovi are entered down a column: columns represent variables, and rows represent observations. So our six observations of age are entered in one column.\njamovi has given the name of A to our column of ages.\n\nLet’s rename our variable from A to Age (years). There are a number of ways of doing this: here we will click the Data tab which allows us to change aspects of our dataset, and then click Setup. By default, the first column is selected - you can choose any column simply by clicking it.\n\nIn this window, you can change many variable properties, such as variable names and variable types. To change the variable name, click the name of the variable, currently A, at the top of the window. Replace A with Age (years):\n\nNote that jamovi has assumed these data are Nominal data; this can be changed to Continuous by choosing the appropriate item in the Measure type drop-down menu:\n\nOnce you have finished naming all your variables, you can click the Up arrow to close the Setup tab to view the spreadsheet again.\njamovi is very flexible with its naming convention, and allows variable names that many other statistical packages would not allow. Often, the only way to get publication quality graphs or tables is to name your variables as completely as you can. However, I would recommend the following conventions when choosing variable names in jamovi:\n\ntry to keep your variable names relatively short;\nvariable names should start with a letter;\nvariable names are case-sensitive (so age, Age and AGE could represent three different variables)\n\n\n\n\n\n\n\nTASK\n\n\n\nRename the variable A with the name Age (years), and define age as a Continuous variable.\n\n\nNow that we have entered our six ages, let’s calculate the mean age. Choose Analyses &gt; Exploration &gt; Descriptives. The Descriptives dialog box will appear. Move the variable Age into the Variables box by clicking Age (years) and then clicking the right-arrow icon. The window should appear as:\n\nYou can see the results of the analysis in the right-hand pane.\n\n\n\n\n\n\nTASK\n\n\n\nCalculate summary statisitics for Age (years) and confirm: there are 6 observations, with a mean age of 24.2 years, a standard deviation of 3.49 years, a minimum of 20 and a maximum of 29 years.\n\n\n\nThe jamovi environment\nNow that we have seen a simple example of how to use jamovi, let’s describe the jamovi environment. There are two main views in jamovi: the Spreadsheet view, available by clicking the Data tab, and the Analysis view, available by clicking the Analyses tab. We will tend to use these two tabs through the course.\nThe unique thing about jamovi is that all analyses are updated whenever the data are changed in the spreadsheet. While this can be convient, care must be taken not to make any unintended changes to your data while in the spreadsheet view.\njamovi also has a way of opening and saving data, using the three lines in the upper-left corner: . This collection of commands lets you open data, save data and export data and output.\n\n\n\n\n\n\nTip\n\n\n\nThe Special Import command is hardly ever used. You should use Open to open all types of data in jamovi.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#part-2-obtaining-summary-statistics-for-continuous-data",
    "href": "01-intro.html#part-2-obtaining-summary-statistics-for-continuous-data",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.10 Part 2: Obtaining summary statistics for continuous data",
    "text": "1.10 Part 2: Obtaining summary statistics for continuous data\nIn this exercise (spanning Modules 1 and 2), we will analyse data to complete a descriptive table from a research study. The data come from a study in primary biliary cirrhosis, a condition of the liver, from Therneau and Grambsch (2010), Modeling Survival Data: Extending the Cox Model. By the end of this exercise, we will have completed the following table.\n\n\nSummary of 418 participants from the PBC study (Therneau and\nGrambsch, 2000)Characteristic SummaryAge (years)Mean (SD) or Median [IQR]SexMalen (%)Femalen (%)AST* (U/ml)Mean (SD) or Median [IQR]Serum bilirubinMean (SD) or Median [IQR]StageIn (%)IIn (%)IIIn (%)IIIVn (%)Vital status at study endAlive: no transplantn (%)Alive: transplantn (%)Deceasedn (%)* asparate aminotransferase\n\n\n\n\n\n\n\n\nTASK\n\n\n\nDownload the table shell, saved on Moodle as PBC Table1.docx, and the information file called mod01_pbc_info.txt.\n\n\n\nOpening a data file\nTyping data directly into jamovi is not common; we usually open data that have been saved as a file. jamovi can open many types of files, including text (txt), comma separated (csv), Microsoft Excel (xlsx), R (rds), Stata (dta) and more. Here, we will open a dataset that has been stored as an R data file (which has the .rds suffix).\n\n\n\n\n\n\nTASK\n\n\n\nLoad the sample data set called mod01_pbc.rds into jamovi using the following steps:\n\nLocate the data set called mod01_pbc.rds on Moodle or the PHCM9795 home-page. Click the file to download it, and then save it in a folder you will be able to locate later - for example, your OneDrive folder.\nIn jamovi, click the three-bar icon, then choose Open. jamovi usually searches in the most recently used folder, so most times you will need to click Browse. Browse to where you stored the dataset and click Open.\n\nConfirm that there are 418 rows by examining the Row count at the bottom of the screen.\nExamine the pbc_info.txt file for a description of each variable.\n\n\n\n\nAssigning meaningful variable names\nAs we saw earlier, jamovi has can allow quite useful variable names, which will appear when creating output. For example, the variable entered as bili could be named Serum bilirubin (mg/dl).\n\n\n\n\n\n\nTASK\n\n\n\nAssign meaningful variable names to the variables used in Table 1. You should to refer to the file pbc_info.txt to determine what each variable represents.\n\n\n\n\nSummarising continuous variables\nAs we saw in Part 1, continuous variables can be summarised using Analyses &gt; Exploration. There are three continuous variables that we would like to summarise: age, AST and serum bilirubin. Each of these can be listed in the Exploration dialog box, as shown below. The summaries are calculated automatically:\n\nBy default, the exploration command presents the number of observations, the number of missing observations, the mean, median, standard deviation, minimum and maximum. We may be interested in obtaining the interquartile range as well, so we select the Statistics arrow, and choose Percentiles:\n\nFor each of our three continuous variables, we need to decide whether to present the mean and standard deviation, or the median and interquartile range. This decision can be made after examining a density plot (and perhaps a boxplot) for each variable.\n\n\nProducing a density plot\nTo produce a density plot, click the arrow next to Plots and choose Density. Plots will be produced for each variable listed in the Variables box. The density plots will be produced one after another, but they have been presented horizontally here:\n\n\n\nProducing a boxplot\nProducing boxplots is done by ticking the Box plot box. By default, jamovi labels each of the points that it considers to be an outlier with its row number; this can be turned off if desired.\n\n\n\n\n\n\nTASK\n\n\n\nObtain density plots and boxplots for age, AST and bilirubin.\nBased on these plots, decide whether the mean or the median is the appropriate summary to use for each variable.\n\n\n\n\nSaving your work from jamovi\nNow that you have made some changes to the pbc data and conducted some analyses, it is good practice to save your work. jamovi uses its own file format to save both data and output, using files with .omv suffix. All changes to your data will be saved, as well as all existing output. However, work saved by jamovi will only be able to be opened by jamovi - you will not easily be able to share your data or your output with colleagues who do not have jamovi. To save a jamovi session, choose Save in the three-lines tab.\nIf you want to share work with colleagues who do not have jamovi, you can use Export to save your data in another file format (recognising that variable and value labels will not be exported), or save your output as a pdf or html file.\n\n\nCopying output from jamovi\nAn easy way to share output between your colleagues is to copy the output into a word processor package (e.g. Microsoft Word). To copy output from jamovi, you can right-click2 the output with your mouse, and choose Export. This will copy the output as plain text for pasting into a Word document. If you select a single table for copying, you can also Copy table or Copy table as HTML. Whichever way you copy output into Word, you will need to make sure your output conforms with all style guides required for your final publication.\n\n\n\n\n\n\nTASK\n\n\n\nComplete Table 1 for continuous variables using the output generated in this exercise. You should decide on whether to present continuous variables by their means or medians, and present the most appropriate measure of spread. Include footnotes to indicate if any variables contain missing observations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#setting-a-value-to-missing",
    "href": "01-intro.html#setting-a-value-to-missing",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.11 Setting a value to missing",
    "text": "1.11 Setting a value to missing\nAs we saw in Section 1.5, it is important to explore our data to identify any unusual observations. If an error is found, the best method for correcting the error is to go back to the original data e.g. the hard copy questionnaire, to obtain the original value, entering the correct value into jamovi. If the original data is not available or the original data is also incorrect, the erroneous value is often excluded from the dataset.\nConsider a sample dataset: mod01_weight_1000.rds, which contains the weights of 1000 people. A density plot and a boxplot should be examined before we start analysing these data:\n\n\n\n\n\n\n\n\n\n\nThere is a clear outlying point shown in the boxplot. Although not obvious, the same point is shown in the density plot as a small blip around 700kg. Obviously this point is unusual, and we should investigate. You will need to decide if any usual values are a data entry error or are biologically plausible. If an extreme value or “outlier”, is biologically plausible, it should be included in all analyses.\nNotice the boxed number in the boxplot: this is the record number in jamovi’s spreadsheet. Click Data to view the spreadsheet, and scroll to record number 58:\n\nWe see that there is a very high value of 700.2kg. A value as high as 700kg is likely to be a data entry error (e.g. error in entering an extra zero) and is not a plausible weight value. Here, you should check your original data.\nIf you do not have access to the original data, it would be safest to set this value as missing. You do change this in jamovi by clicking the datapoint and pressing Delete or Backspace. This has set this weight to missing.\nNote: if an extreme value lies within the range of biological possibility it should not be set to missing.\nThe same process could be used to replace the incorrect value with the correct value. For example, if you do source the original medical records, you might find that the original weight was recorded in medical records as 70.2kg. We could use the same process to replace 700.2 by 70.2 by entering the correct value in the cell.\nOnce you have checked your data for errors, you are ready to start analysing your data.\n\n\n\n\n\n\nImportant\n\n\n\nWhenever you start changing data in jamovi, you should always keep an original, unedited copy of your data. You can do this using Save As to save your edited work, leaving the original data unchanged.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#learning-outcomes-1",
    "href": "01-intro.html#learning-outcomes-1",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of this Module, you will be able to:\n\nunderstand the difference between R and RStudio\nnavigate the RStudio interface\ninput and import data into R\nuse R to summarise data\nperform basic data transformations\nunderstand the difference between saving R data and saving R output\ncopy R output to a standard word processing package",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#part-1-an-introduction-to-r",
    "href": "01-intro.html#part-1-an-introduction-to-r",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.12 Part 1: An introduction to R",
    "text": "1.12 Part 1: An introduction to R\n“R is a language and environment for statistical computing and graphics.” Link. It is an open-source programming language, used mainly for statistics (including biostatistics) and data science.\nThe aim of these notes is to introduce the R language within the RStudio environment, and to introduce the commands and procedures that are directly relevant to this course. There is so much more to R than we can cover in these notes. Relevant information will be provided throughout the course, and we will provide further references that you can explore if you are interested.\n\nR vs RStudio\nAt its heart, R is a programming language. When you install R on your computer, you are installing the language and its resources, as well as a very basic interface for using R. You can write and run R code using the basic R app, but it’s not recommended.\nRStudio is an “Integrated Development Environment” that runs R while also providing useful tools to help you write code and analyse data. You can think of R as an engine which does the work, and RStudio as a car that uses the engine, but also provides useful tools like GPS navigation and reversing cameras that help you drive.\nNote: even though we recommend that you use RStudio, you still need install R. RStudio will not run without R installed.\nIn summary, we recommend you use RStudio to write R code.\n\n\nInstalling R and RStudio\n\nTo install R on your computer\n\nDownload the R installer from:\n\nfor Windows: https://cran.r-project.org/bin/windows/base/\nfor MacOS: https://cran.r-project.org/bin/macosx/\n\nInstall R by running the installer and following the installation instructions. The default settings are fine.\n\nNote for macOS: if you are running macOS 10.8 or later, you may need to install an additional application called XQuartz, which is available at https://www.xquartz.org/. Download the latest installer (XQuartz-2.8.5.dmg as of May 2024), and install it in the usual way.\n\nOpen the R program. You should see a screen similar to below:\n\n\n\n\n\n\n\n\n\n\nNear the bottom of the R screen, you will find the “&gt;” symbol which represents the command line. If you type 1 + 2 into the command line and then hit enter you should get:\n[1] 3\nThis is R performing your calculation, with the [1] indicating that the solution to 1 + 2 is a single number (the number 3).\nAt this point, close R - we will not interact with R like this in the future. You can close R by typing quit() at the command prompt, followed by the return key, or in the usual way of closing an application in your operating system. There is no need to save anything here if prompted.\n\n\nTo install RStudio on your computer\n\nMake sure you have already installed R, and verified that it is working.\nDownload the RStudio desktop installer at: https://posit.co/download/rstudio-desktop/. The website should detect your operating system and link to the appropriate installer for your computer.\nInstall RStudio by running the installer and following the installation instructions. The default settings are fine.\nOpen RStudio, which will appear similar to the screenshot below:\n\n\n\n\n\n\n\n\n\n\nLocate the command line symbol “&gt;” at the bottom of the left-hand panel. Type 1 + 2 into the command line and hit enter, and you will see:\n[1] 3\nThis confirms that RStudio is running correctly, and can use the R language to correctly calculate the sum between 1 and 2!\nRStudio currently comprises three window panes, and we will discuss these later.\n\n\n\n\n\n\nTASK\n\n\n\nInstall R and RStudio and confirm they are both working correctly.\n\n\n\n\n\nRecommended setup\nI will provide a recommended setup for R and RStudio in this section. You are free to use alternative workflows and setup, but this setup works well in practice.\n\nRStudio preferences\nBy default, RStudio will retain data, scripts and other objects when you quit your RStudio session. Relying on this can cause headaches, so I recommend that you set up RStudio so that it does not preserve your workspace between sessions. Open the RStudio options:\n\nMac: Edit &gt; Settings\nWindows: Tools &gt; Global Options\n\nand deselect “Restore .RData into workplace at startup”, and choose: “Save workspace to .RData on exit: Never”.\n\n\n\n\n\n\n\n\n\n\n\nSet up a project\nA project in RStudio is a folder that RStudio recognises as a place to store R scripts, data files, figures that are common to an analysis project. Setting up a folder allows much more simple navigation and specification of data files and output. More detail can be found in Chapter 8 of the excellent text: R for Data Science. Using projects is not necessary, but I recommend working with projects from day one.\nWe will create a project called PHCM9795 to store all the data you will use and scripts that you will write in this course. First, think about where you want to store your project folder: this could be somewhere in your Documents folder.\nStep 1: Choose File &gt; New Project… in RStudio to open the Create Project dialog box:\n\n\n\n\n\n\n\n\n\nStep 2: Click the first option to create a project in a New directory\n\n\n\n\n\n\n\n\n\nStep 3: Click the first option: New Project. Give the project a name, by typing PHCM9795 in the “Directory name”, and choose where you want to store the project by clicking the Browse button.\n\n\n\n\n\n\n\n\n\nStep 4: Click Create to create your project.\nYou will now have a new folder in your directory, which contains only one file: PHCM9795.Rproj, and the two right-hand panes of RStudio will appear as below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTASK\n\n\n\nCreate a new project called PHCM9795.\n\n\nThe top-right menu bar is showing that you are working within the PHCM9795 project, and the bottom-right window is showing the contents of that window: the single PHCM9795.Rproj file. We will add some more files to this project later.\n\n\n\nA simple R analysis\nIn this very brief section, we will introduce R by calculating the average of six ages.\nTo begin, open a new R Script by choosing File &gt; New file &gt; R Script . A script (or a program) is a collection of commands that are sequentially processed by R. You can also type Ctrl+Shift+N in Windows, or Command+Shift+N in MacOS to open a new script in RStudio, or click the New File button at the top of the RStudio window.\nYou should now see four window panes, as below. In the top-left window, type the following (replacing my name with yours, and including today’s date):\n\n# Author: Timothy Dobbins\n# Date: 5 April 2024\n# Purpose: My first R script\n\nage &lt;- c(20, 25, 23, 29, 21, 27)\nsummary(age)\n\nNote: R is case-sensitive, so you should enter the text exactly as written in these notes.\nYour screen should look something like:\n\n\n\n\n\n\n\n\n\nTo run your script, choose Code &gt; Run Region &gt; Run All. You will see your code appear in the bottom-left window, with the following output:\n\n&gt; # Author: Timothy Dobbins\n&gt; # Date: 5 April 2024\n&gt; # Purpose: My first R script\n&gt; \n&gt; age &lt;- c(20, 25, 23, 29, 21, 27)\n\n&gt; summary(age)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  20.00   21.50   24.00   24.17   26.50   29.00 \n\nWe will explain the key parts of this script later, but for now, you have entered six ages and calculated the mean age (along with five other summary statistics).\n\n\n\n\n\n\nTASK\n\n\n\nType the code above into the top-left window, and run the script.\nSave your script within the PHCM9795 project by using File &gt; Save As, using the name my_first_analysis.R.\n\n\n\n\nThe RStudio environment\nNow that we have seen a simple example of how to use R within RStudio, let’s describe the RStudio environment. Let’s assume that you have just run your first R script, and you have four windows as below:\n\n\n\n\n\n\n\n\n\nThe top-left window is call the Source window, and is where you write and edit your R scripts. Scripts can be saved by clicking File &gt; Save As or by clicking on the symbol of a floppy disk at the top of the script. The file will have an extension of .R, for example script.R. Remember to give your script a meaningful title and remember to periodically save as you go.\nIn RStudio, the name of the script will be black when it has been saved, and will change to red if you have any unsaved changes.\nThe Console window, at the bottom left, contains the command line which is indicated with the symbol &gt;. You can type commands here, but anything executed directly from the console is not saved and therefore is lost when the session ends (when you exit RStudio). You should always run your commands from a script file which you can save and use again later. When you run commands from a script, the output and any notes/errors are shown in the console. The Terminal and Jobs tabs will not be used in this course.\nThe Environment window at the top-right shows a list of objects that have been created during your session. When you close your RStudio session these objects will disappear. We will not use the History or Connections tabs in this course.\nThe bottom right corner contains some useful tabs, in particular the Help tab. When you are troubleshooting errors or learning how to use a function, the Help tab should be the first place you visit. Here you can search the help documents for all the packages you have installed. Whenever you create plots in R, these will be shown in the Plots tab. The Packages tab contains a list of installed packages and indicates which ones are currently in use (we will learn about packages later). Packages which are loaded, i.e. in use, are indicated with a tick. Some packages are in use by default when you begin a new session. You can access information about a package by clicking on its name. The Files tab provides a shortcut to access your files. The Viewer tab will not be used in this course.\n\n\nSome R basics\nWhile we use R as a statistics package, R is a programming language. In order to use R effectively, we need to define some basics.\n\nScripts\nWhile R can be run completely from the command line, issuing commands one-by-one, it is most commonly run using scripts. A script is simply a list of commands that are processed in order. The simple analysis we conducted earlier is a very simple script. Some things to know about R scripts:\n\nanything appearing after a # is a comment, and is ignored by R. The first three lines of our script are there for ourselves (either as writers of code, or readers of code). I include comments at the beginning of each of my scripts to describe:\n\nwho wrote the script (useful if someone else uses your script and wants to ask questions about it);\nwhen the script was written;\nwhat the script does. This last point may seem odd, but it’s useful to describe what this script does, and why it might differ to other scripts being used in the analysis. This is particularly useful if your scripts become long and complex.\n\nR is case-sensitive. So age, AGE and Age could refer to three separate variables (please don’t do this!)\nuse blank lines and comments to separate sections of your script\n\n\n\nObjects\nIf you do some reading about R, you may learn that R is an “object-oriented programming language”. When we enter or import data into R, we are asking R to create objects from our data. These objects can be manipulated and transformed by functions, to obtain useful insights from our data.\nObjects in R are created using the assignment operator. The most common form of the assignment operator looks like an arrow: &lt;- and is typed as the &lt; and - symbols. The simplest way of reading &lt;- is as the words “is defined as”. Note that it possible to use -&gt; and even = as assignment operators, but their use is less frequent.\nLet’s see an example:\n\nx &lt;- 42\n\nThis command creates a new object called x, which is defined as the number 42 (or in words, “x is defined as 42”). Running this command gives no output in the console, but the new object appears in the top-right Environment panel. We can view the object in the console by typing its name:\n\n# Print the object x\nx\n\n[1] 42\n\n\nNow we see the contents of x in the console.\nThis example is rather trivial, and we rarely assign objects of just one value. In fact, we created an object earlier, called age, which comprised six values.\n\n\nData structures\nThere are two main structures we will use to work with data in this course: vectors and data frames. A vector is a combination of data values, all of the same type. For example, our six ages that we entered earlier is a vector. You could think of a vector as a column of data (even though R prints vectors as rows!) And technically, even an object with only one value is a vector, a vector of size 1.\nThe easiest way of creating a vector in R is by using the c() function, where c stands for ‘combine’. In our previous Simple Analysis in R (Section 1.12.4), we wrote the command:\n\nage &lt;- c(20, 25, 23, 29, 21, 27)\n\nThis command created a new object called age, and combined the six values of age into one vector.\nJust as having a vector of size 1 is unusual, having just one column of data to analyse is also pretty unusual. The other structure we will describe here is a data frame which is essentially a collection of vectors, each of the same size. You could think of a data frame as being like a spreadsheet, with columns representing variables, and rows representing observations.\nThere are other structures in R, such as matrices and lists, which we won’t discuss in this course. And you may come across the term tibble, which is a type of data frame.\n\n\nFunctions\nIf objects are the nouns of R, functions are the verbs. Essentially, functions transform objects. Functions can transform your data into summary statistics, graphical summaries or analysis results. For example, we used the summary() function to display summary statistics for our six ages.\nR functions are specified by their arguments (or inputs). The arguments that can be supplied for each function can be inspected by examining the help notes for that function. To obtain help for a function, we can submit help(summary) (or equivalently ?summary) in the console, or we can use the Help tab in the bottom-right window of RStudio. For example, the first part of the help notes for summary appear as:\n\n\n\n\n\n\n\n\n\nThe help notes in R can be quite cryptic, but the Usage section details what inputs should be specified for the function to run. Here, summary requires an object to be specified. In our case, we specified age, which is our object defined as the vector of six ages.\nMost help pages also include some examples of how you might use the function. These can be found at the very bottom of the help page.\n\n\n\n\n\n\n\n\n\nThe summary() function is quite simple, in that it only requires one input, the object to be summarised. More complex functions might require a number of inputs. For example, the help notes for the descriptives() function in the jmv package show a large number of inputs can be specified. Instructions for installing the jmv package will be provided below, this help-screen is included for illustration only.\n\n\n\n\n\n\n\n\n\nThere are two things to note here. First, notice that the first two inputs are listed with no = symbol, but all other inputs are listed with = symbols (with values provided after the = symbol). This means that everything apart from data and vars have default values. We are free to not specify values for these inputs if we are happy with the defaults provided. For example, by default the variance is not calculated (as variance = FALSE). To obtain the variance as well as the standard deviation, we can change this default to variance = TRUE:\n\n# Only the standard deviation is provided as the measure of variability\ndescriptives(data=pbc, vars=age)\n\n# Additionally request the variance to be calculated\ndescriptives(data=pbc, vars=age, variance=TRUE)\n\nSecond, for functions with multiple inputs, we can specify the input name and its value, or we can ignore the input name and specify just the input values in the order listed in the Usage section. So the following are equivalent:\n\n# We can specify that the dataset to be summarised is pbc,\n#   and the variable to summarise is age:\ndescriptives(data=pbc, vars=age)\n\n# We can omit the input name, as long as we keep the inputs in the correct order - \n#   that is, dataset first, variable second:\ndescriptives(pbc, age)\n\n# We can change the order of the inputs, as long as we specify the input name:\ndescriptives(vars=age, data=pbc)\n\nIn this course, we will usually provide all the input names, even when they are not required. As you become more familiar with R, you will start to use the shortcut method.\n\n\nThe curse of inconsistency\nAs R is an open-source project, many people have contributed to its development. This has led to a frustrating part of R: some functions require a single object to be specified, but some require you to specify a data frame and select variables for analysis. Let’s see an example.\nThe help for summary() specifies the usage as: summary(object, ...). This means we need to specify a single object to be summarised. An object could be a single column of data (i.e. a vector), or it could be a data frame. If we have a data frame called pbc which contains many variables, the command summary(pbc) would summarise every variable in the data frame.\nWhat if we only wanted to summarise the age of the participants in the data frame? To select a single variable from a data frame, we can use the following syntax: dataframe$variable. So to summarise just age from this data frame, we would use: summary(pbc$age).\nCompare this with the descriptives() function in the jmv package. We saw earlier that the two required inputs for descriptives() are data (the data frame to be analysed) and vars (the variables to be analysed). So to summarise age from the pbc data frame, we would specify descriptives(data=pbc, vars=age).\nThis inconsistency will seem maddening at first, and will continue to be maddening! Reading the usage section of the help pages is a useful way to determine whether you should specify an object (like pbc$age) or a data frame and a list of variables.\n\n\n\nPackages\nA package is a collection of functions, documentation (and sometimes datasets) that extend the capabilities of R. Packages have been written by R users to be freely distributed and used by others. R packages can be obtained from many sources, but the most common source is CRAN: the Comprehensive R Archive Network.\nA useful way of thinking about R is that R is like a smartphone, with packages being like apps which are downloaded from CRAN (similar to an app-store). When you first install R, it comes with a basic set of packages (apps) installed. You can do a lot of things with these basic packages, but sometimes you might want to do things differently, or you may want to perform some analyses that can’t be done using the default packages. In these cases, you can install a package.\nLike installing an app on a smartphone, you only need to install a package once. But each time you want to use the package, you need to load the package into R.\n\nHow to install a package\nThere are a couple of ways to install a package. You can use the install.packages() function if you know the exact name of the package. Let’s use an example of installing the skimr package, which gives a very nice, high-level overview of any data frame. We can install skimr by typing the following into the console:\n\ninstall.packages(\"skimr\")\n\nNote the use of the quotation marks.\nAlternatively, RStudio offers a graphical way of installing packages that can be accessed via Tools &gt; Install Packages, or via the Install button at the top of the Packages tab in the bottom-right window. You can begin typing the name of the package in the dialog box that appears, and RStudio will use predictive text to offer possible packages:\n\n\n\n\n\n\n\n\n\nWhile writing code is usually the recommended way to use R, installing packages is an exception. Using Tools &gt; Install Packages is perfectly fine, because you only need to install a package once.\n\n\nHow to load a package\nWhen you begin a new session in RStudio, i.e. when you open RStudio, only certain core packages are automatically loaded. You can use the library() function to load a package that you has previously been installed. For example, now that we have installed skimr, we need to load it before we can use it:\n\nlibrary(skimr)\n\nNote that quotation marks are not required for the library() function (although they can be included if you really like quotation marks!).\n\n\n\n\n\n\nTASK\n\n\n\nInstall the packages jmv and skimr using Tools &gt; Install packages, or by typing into the console:\ninstall.packages(\"jmv\")\ninstall.packages(\"skimr\")\n\n\n\n\nInstalling vs loading packages\nPackage installation:\n\nuse the install.packages() function (note the ‘s’) or Tools &gt; Install packages\nthe package name must be surrounded by quotation marks\nonly needs to be done once\n\nPackage loading\n\nuse the library() function\nthe package name does not need to be surrounded by quotation marks\nmust be done for each R session\n\n\n\n\nWhat is this thing called the tidyverse?\nIf you have done much reading about R, you may have come across the tidyverse:\n\n“The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.” https://www.tidyverse.org/\n\nPackages in the tidyverse have been designed with a goal to make using R more consistent by defining a “grammar” to manipulate data, examine data and draw conclusions from data. While the tidyverse is a common and powerful set of packages, we will not be teaching the tidyverse in this course for two main reasons:\n\nThe data we provide have been saved in a relatively tidy way, and do not need much manipulation for analyses to be conducted. The cognitive load in learning the tidyverse in this course is greater than the benefit that could be gained.\nThere are many resources (online, in print etc) that are based on base R, and do not use the tidyverse. It would be difficult to understand these resources if we taught only tidyverse techniques. In particular, the dataframe$variable syntax is an important concept that should be understood before moving into the tidyverse.\n\nIn saying all of this, I think the tidyverse is an excellent set of packages, which I frequently use. At the completion of this course, you will be well equipped to teach yourself tidyverse using many excellent resources such as: Tidyverse Skills for Data Science and R for Data Science.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#part-2-obtaining-summary-statistics-for-continuous-data-1",
    "href": "01-intro.html#part-2-obtaining-summary-statistics-for-continuous-data-1",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.13 Part 2: Obtaining summary statistics for continuous data",
    "text": "1.13 Part 2: Obtaining summary statistics for continuous data\nIn this exercise (spanning Modules 1 and 2), we will analyse data to complete a descriptive table from a research study. The data come from a study in primary biliary cirrhosis, a condition of the liver, from Modeling Survival Data: Extending the Cox Model Therneau and Grambsch (2010). By the end of this exercise, we will have completed the following table.\n\n\nSummary of 418 participants from the PBC study (Therneau and\nGrambsch, 2000)Characteristic SummaryAge (years)Mean (SD) or Median [IQR]SexMalen (%)Femalen (%)AST* (U/ml)Mean (SD) or Median [IQR]Serum bilirubinMean (SD) or Median [IQR]StageIn (%)IIn (%)IIIn (%)IIIVn (%)Vital status at study endAlive: no transplantn (%)Alive: transplantn (%)Deceasedn (%)* asparate aminotransferase\n\n\nThis table is available in Table1.docx, saved on Moodle.\n\n\n\n\n\n\nTASK\n\n\n\nDownload the table shell, saved on Moodle as PBC Table1.docx, and the information file called mod01_pbc_info.txt.\n\n\n\nSet up your data\nWe created a project in the previous step. We will now create a folder to store all the data for this course. Storing the data within the project makes life much easier!\nCreate a new folder by clicking the New Folder icon in the Files tab at the bottom-right:\n\n\n\n\n\n\n\n\n\nCall the new folder data.\nClick on this folder to open it, and then create two new folders: activities and examples.\nDownload the “Data sets: for learning activities” from Moodle, and use Windows Explorer or MacOS Finder to save these data sets in activities. Save the “Data sets: example data from course notes” into the examples folder.\nYour activities folder should look like:\n\n\n\n\n\n\n\n\n\nClick the two dots next to the up-arrow at the top of the folder contents to move back up the folder structure. Note that you need to click the dots, and not the up-facing green arrow!\n\n\nReading a data file\nTyping data directly into R is not common; we usually read data that have been previously saved. In this example, we will read an .rds file using the readRDS() function, which has only one input: the location of the file.\n\n\n\n\n\n\nTASK\n\n\n\n1 - Confirm that the mod01_pdc.rds file is in the activities sub-folder within the data folder (as per the previous steps).\n2 - Load the skimr package, and use the readRDS() function to read the file into R, assigning it to a data frame called pbc. Because we set up our project, we can locate our data easily by telling R to use the file: \"data/activities/mod01_pdc.rds\", which translates as: the file mod01_pdc.rds which is located in the activities sub-folder within the data folder.\n\nlibrary(skimr)\n\npbc &lt;- readRDS(\"data/activities/mod01_pbc.rds\")\n\n3 - We can now use the summary() function to examine the pbc dataset:\n\nsummary(pbc)\n\n       id             time          status            trt       \n Min.   :  1.0   Min.   :  41   Min.   :0.0000   Min.   :1.000  \n 1st Qu.:105.2   1st Qu.:1093   1st Qu.:0.0000   1st Qu.:1.000  \n Median :209.5   Median :1730   Median :0.0000   Median :1.000  \n Mean   :209.5   Mean   :1918   Mean   :0.8301   Mean   :1.494  \n 3rd Qu.:313.8   3rd Qu.:2614   3rd Qu.:2.0000   3rd Qu.:2.000  \n Max.   :418.0   Max.   :4795   Max.   :2.0000   Max.   :2.000  \n                                                 NA's   :106    \n      age             sex           ascites            hepato      \n Min.   :26.28   Min.   :1.000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:42.83   1st Qu.:2.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :51.00   Median :2.000   Median :0.00000   Median :1.0000  \n Mean   :50.74   Mean   :1.895   Mean   :0.07692   Mean   :0.5128  \n 3rd Qu.:58.24   3rd Qu.:2.000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :78.44   Max.   :2.000   Max.   :1.00000   Max.   :1.0000  \n                                 NA's   :106       NA's   :106     \n    spiders           edema             bili             chol       \n Min.   :0.0000   Min.   :0.0000   Min.   : 0.300   Min.   : 120.0  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 0.800   1st Qu.: 249.5  \n Median :0.0000   Median :0.0000   Median : 1.400   Median : 309.5  \n Mean   :0.2885   Mean   :0.1005   Mean   : 3.221   Mean   : 369.5  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.: 3.400   3rd Qu.: 400.0  \n Max.   :1.0000   Max.   :1.0000   Max.   :28.000   Max.   :1775.0  \n NA's   :106                                        NA's   :134     \n    albumin          copper          alkphos             ast        \n Min.   :1.960   Min.   :  4.00   Min.   :  289.0   Min.   : 26.35  \n 1st Qu.:3.243   1st Qu.: 41.25   1st Qu.:  871.5   1st Qu.: 80.60  \n Median :3.530   Median : 73.00   Median : 1259.0   Median :114.70  \n Mean   :3.497   Mean   : 97.65   Mean   : 1982.7   Mean   :122.56  \n 3rd Qu.:3.770   3rd Qu.:123.00   3rd Qu.: 1980.0   3rd Qu.:151.90  \n Max.   :4.640   Max.   :588.00   Max.   :13862.4   Max.   :457.25  \n                 NA's   :108      NA's   :106       NA's   :106     \n      trig           platelet        protime          stage      \n Min.   : 33.00   Min.   : 62.0   Min.   : 9.00   Min.   :1.000  \n 1st Qu.: 84.25   1st Qu.:188.5   1st Qu.:10.00   1st Qu.:2.000  \n Median :108.00   Median :251.0   Median :10.60   Median :3.000  \n Mean   :124.70   Mean   :257.0   Mean   :10.73   Mean   :3.024  \n 3rd Qu.:151.00   3rd Qu.:318.0   3rd Qu.:11.10   3rd Qu.:4.000  \n Max.   :598.00   Max.   :721.0   Max.   :18.00   Max.   :4.000  \n NA's   :136      NA's   :11      NA's   :2       NA's   :6      \n\n\nAn alternative to the summary() function is the skim() function in the skimr package, which produces summary statistics as well as rudimentary histograms:\n\nskim(pbc)\n\n\n\n\n\n\n\n\n\n\n\n\nThe summary() and skim() functions are useful to give a quick overview of a dataset: how many variables are included, how variables are coded, which variables contain missing data and a crude histogram showing the distribution of numeric variables.\n\n\nSummarising continuous variables\nOne of the most flexible functions for summarising continuous variables is the descriptives() function from the jmv package. The function is specified as descriptives(data=, vars=) where:\n\ndata specifies the dataframe to be analysed\nvars specifies the variable(s) of interest, with multiple variables combined using the c() function\n\nWe can summarise the three continuous variables in the pbc data: age, AST and serum bilirubin, as shown below.\n\nlibrary(jmv)\n\ndescriptives(data=pbc, vars=c(age, ast, bili))\n\n\n DESCRIPTIVES\n\n Descriptives                                                \n ─────────────────────────────────────────────────────────── \n                         age         ast         bili        \n ─────────────────────────────────────────────────────────── \n   N                          418         312          418   \n   Missing                      0         106            0   \n   Mean                  50.74155    122.5563     3.220813   \n   Median                51.00068    114.7000     1.400000   \n   Standard deviation    10.44721    56.69952     4.407506   \n   Minimum               26.27789    26.35000    0.3000000   \n   Maximum               78.43943    457.2500     28.00000   \n ─────────────────────────────────────────────────────────── \n\n\nBy default, the descriptives function presents the mean, median, standard deviation, minimum and maximum. We can request additional statistics, such as the quartiles (which are called the percentiles, or pc, in the descriptives function):\n\ndescriptives(data=pbc, vars=c(age, ast, bili), pc=TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                \n ─────────────────────────────────────────────────────────── \n                         age         ast         bili        \n ─────────────────────────────────────────────────────────── \n   N                          418         312          418   \n   Missing                      0         106            0   \n   Mean                  50.74155    122.5563     3.220813   \n   Median                51.00068    114.7000     1.400000   \n   Standard deviation    10.44721    56.69952     4.407506   \n   Minimum               26.27789    26.35000    0.3000000   \n   Maximum               78.43943    457.2500     28.00000   \n   25th percentile       42.83231    80.60000    0.8000000   \n   50th percentile       51.00068    114.7000     1.400000   \n   75th percentile       58.24093    151.9000     3.400000   \n ─────────────────────────────────────────────────────────── \n\n\n\n\nProducing a density plot\nWe can add dens=TRUE to the descriptives function to produce a density plot for each listed variable:\n\ndescriptives(data=pbc, vars=c(age, ast, bili), pc=TRUE, dens=TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                \n ─────────────────────────────────────────────────────────── \n                         age         ast         bili        \n ─────────────────────────────────────────────────────────── \n   N                          418         312          418   \n   Missing                      0         106            0   \n   Mean                  50.74155    122.5563     3.220813   \n   Median                51.00068    114.7000     1.400000   \n   Standard deviation    10.44721    56.69952     4.407506   \n   Minimum               26.27789    26.35000    0.3000000   \n   Maximum               78.43943    457.2500     28.00000   \n   25th percentile       42.83231    80.60000    0.8000000   \n   50th percentile       51.00068    114.7000     1.400000   \n   75th percentile       58.24093    151.9000     3.400000   \n ─────────────────────────────────────────────────────────── \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the density plots are plotted separately in the Plot window. They can be viewed using the arrows at the top of the Plot window:\n\nA more flexible way of constructing a density plot is by using the plot() function within R, using the syntax: plot(density(dataframe$variable)), which plots the variable from the dataframe. For example, the default density plot for the age column of the pbc data:\n\nplot(density(pbc$age))\n\n\n\n\n\n\n\n\nThis plot can be improved by using xlab=\" \" and main=\" \" to assign labels for the x-axis and overall title respectively:\n\nplot(density(pbc$age),\n     xlab=\"Age in years\",\n     main=\"Density plot of participant age from pbc study data\")\n\n\n\n\n\n\n\n\n\n\nProducing a boxplot\nLike the density plot, boxplots can be requested in the descriptives function by using box=TRUE.\nThe boxplot function is an alternative, more flexible function, again specifying the dataframe to use and the variable to be plotted as dataframe$variable. Labels can be applied in the same way as the histogram:\n\nboxplot(pbc$age, xlab=\"Age (years)\", \n   main=\"Boxplot of participant age from pbc study data\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTASK\n\n\n\nObtain density plots and boxplots for age, AST and bilirubin.\nBased on these plots, decide whether the mean or the median is the appropriate summary to use for each variable.\n\n\n\n\nSaving data in R\nThere are many ways to save data from R, depending on the type of file you want to save. The recommendation for this course is to save your data using the .rds format, using the saveRDS() function, which takes two inputs: saveRDS(object, file). Here, object is the R object to be saved (usually a data frame), and file is the location for the file to be saved (file name and path, including the .rds suffix).\nIt is not necessary to save our PBC data, as we have made only minor changes to the data that can be replicated by rerunning our script. If you had made major changes and wanted to save your data, you could use:\nsaveRDS(pbc, file=\"pbc_revised.rds\")\n\n\nCopying output from R\nIt is important to note that saving your data or your script in R will not save your output. The easiest way to retain the output of your analyses is to copy the output from the Console into a word processor package (e.g. Microsoft Word) before closing R.\nUnfortunately, by default, R is not ideal for creating publication quality tables. There are many packages that will help in this process, such as R Markdown, huxtable, gt and gtsummary, but their use is beyond the scope of this course. R Markdown for Scientists provides an excellent introduction to R Markdown.\n\n\n\n\n\n\nTASK\n\n\n\nComplete Table 1 for continuous variables using the output generated in this exercise. You should decide on whether to present continuous variables by their means or medians, and present the most appropriate measure of spread. Include footnotes to indicate if any variables contain missing observations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#setting-a-value-to-missing-1",
    "href": "01-intro.html#setting-a-value-to-missing-1",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "1.14 Setting a value to missing",
    "text": "1.14 Setting a value to missing\nAs we saw in Section 1.5, it is important to explore our data to identify any unusual observations. If an error is found, the best method for correcting the error is to go back to the original data e.g. the hard copy questionnaire, to obtain the original value, entering the correct value into R. If the original data is not available or the original data is also incorrect, the erroneous value is often excluded from the dataset.\nConsider a sample dataset: mod01_weight_1000.rds, which contains the weights of 1000 people. A density plot and a boxplot should be examined before we start analysing these data:\n\n\n\n\n\n\n\n\n\nThere is a clear outlying point shown in the boxplot. Although not obvious, the same point is shown in the density plot as a small blip around 700kg. Obviously this point is unusual, and we should investigate.\nWe can view any outlying observations in the dataset using the subset function. You will need to decide if these values are a data entry error or are biologically plausible. If an extreme value or “outlier”, is biologically plausible, it should be included in all analyses.\nFor example, to list any observations from the sample dataset with a weight larger than 200:\n\nsubset(sample, weight&gt;200)\n\n\n\nidweight\n\n58700\n\n\n\nWe see that there is a very high value of 700.2kg. A value as high as 700kg is likely to be a data entry error (e.g. error in entering an extra zero) and is not a plausible weight value. Here, you should check your original data.\nIf you do not have access to the original data, it would be safest to set this value as missing. You do change this in R by using an ifelse statement to recode the incorrect weight of 700.2kg into a missing value. A missing value in R is represented by NA.\nThe form of the ifelse statement is as follows: ifelse(test, value_if_true, value_if_false)\nWe will write code to:\n\ncreate a new column (called weight_clean) in the sample dataframe (i.e. sample$weight_clean)\ntest whether weight is equal to 700.2\n\nif this is true, we will assign weight_clean to be NA\notherwise weight_clean will equal the value of weight\n\n\nPutting it all together:\n\nsample$weight_clean = ifelse(sample$weight==700.2, NA, sample$weight)\n\nNote: if an extreme value lies within the range of biological possibility it should not be set to missing.\nThe same syntax could be used to replace the incorrect value with the correct value. For example, if you do source the original medical records, you might find that the original weight was recorded in medical records as 70.2kg. We could use the same syntax to replace 700.2 by 70.2: sample$weight_clean = ifelse(sample$weight==700.2, NA, sample$weight)\nOnce you have checked your data for errors, you are ready to start analysing your data.\n\nWhat on earth: == ?\nIn R, the test of equality is denoted by two equal signs: ==. So we would use == to test whether an observation is equal to a certain value. Let’s see an example:\n\n# Test whether 6 is equal to 6\n6 == 6\n\n[1] TRUE\n\n# Test whether 6 is equal to 42\n6 == 42\n\n[1] FALSE\n\n\nYou can read the == as “is equal to”. So the code sample$weight == 700.2 is read as: “is the value of weight from the data frame sample equal to 700.2?”. In our ifelse statement above, if this condition is true, we replace weight by 70.2; if it is false, we leave weight as is.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "1  Introduction to biostatistics and data fundamentals",
    "section": "",
    "text": "https://www.abs.gov.au/statistics/understanding-statistics/statistical-terms-and-concepts/data↩︎\nWindows\n\nClick the right mouse button to open a context menu\nShows options relevant to what you clicked on (copy, export, add note)\n\nMac\n\nTwo-button mouse: Press right button\nOne-button mouse/trackpad: Hold Control (Ctrl) while clicking\nFunctions the same as Windows - shows context-specific options\n\n↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biostatistics and data fundamentals</span>"
    ]
  },
  {
    "objectID": "02-probability.html",
    "href": "02-probability.html",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "",
    "text": "Learning objectives\nBy the end of this module you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#learning-objectives",
    "href": "02-probability.html#learning-objectives",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "",
    "text": "Present and report categorical data numerically and graphically\nDescribe the concept of probability\nDescribe the characteristics of a binomial distribution\nCompute probabilities from a binomial distribution using statistical software",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#optional-readings",
    "href": "02-probability.html#optional-readings",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapters 5, 14 and 15. [UNSW Library Link]\nBland (2015); Chapters 6 and 7. [UNSW Library Link]\nGraphics and statistics for cardiology: designing effective tables for presentation and publication, Boers (2018, UNSW Library Link)\nGuidelines for Reporting of Figures and Tables for Clinical Research in Urology, Vickers et al. (2020, UNSW Library Link)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#introduction",
    "href": "02-probability.html#introduction",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nIn Module 1, we saw how to summarise continuous data numerically and graphically. In this module, we will discuss summarising categorical data numerically and graphically. We will also introduce the concept of probability which underpins the theoretical basis of statistics, and then introduce the concept of probability distributions. We will present the binomial distribution, which calculates the probability of observing a certain number of events from multiple observations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#summarising-a-single-categorical-variable-numerically",
    "href": "02-probability.html#summarising-a-single-categorical-variable-numerically",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.2 Summarising a single categorical variable numerically",
    "text": "2.2 Summarising a single categorical variable numerically\nCategorical data are best summarised using a frequency table, where each category is summarised by its frequency: the count of the number of individuals in each category. The relative frequency (the frequency expressed as a proportion or percentage of the total frequency) is usually included give further insight.\n\n\n\n\nTable 2.1: Sex of participants in PBC study\n\n\n\nSexFrequencyRelative frequency (%)Male4410.5Female37489.5\n\n\n\n\n\nIt is sometimes useful to present the cumulative relative frequency, which shows the relative frequency of individuals in a certain category or below (for example, Table 2.2).\n\n\n\n\nTable 2.2: Stage of disease for participants in PBC study\n\n\n\nStage *FrequencyRelative frequency (%)Cumulative relative frequency (%)1215.15.129222.327.4315537.665.0414435.0100.0* Disease stage was missing for 6 participants\n\n\n\n\n\nFrom Table 2.2, we can see that 65.0% of participants had Stage 3 disease or lower.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#summarising-a-single-categorical-variable-graphically",
    "href": "02-probability.html#summarising-a-single-categorical-variable-graphically",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.3 Summarising a single categorical variable graphically",
    "text": "2.3 Summarising a single categorical variable graphically\nA categorical variable is best summarised graphically using a bar chart. For example, we can present the distribution of Stage of Disease graphically using a bar graph (Figure 2.1). Bar graphs, which are suitable for plotting discrete or categorical variables, are defined by the fact that the bars do not touch.\n\n\n\n\n\n\n\n\nFigure 2.1: Bar graph of stage of disease from PBC study\n\n\n\n\n\nPie charts can be an alternative way to summarise a categorical variable graphically, however their use is not recommended for the following reasons:\n\nNot ideal when there are many categories to compare\nThe use of percentages is not appropriate when the sample size is small\nCan be misleading by using different size pies, different rotations and different colours to draw attention to specific groups\n3D and exploding bar charts further distort the effect of perspective and may confuse the reader\n\nPie charts will not be discussed further in this course.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#sec-eda-cat",
    "href": "02-probability.html#sec-eda-cat",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.4 Exploratory data analysis for categorical data",
    "text": "2.4 Exploratory data analysis for categorical data\nAs with continuous data, it is good practice to undertake exploratory data analysis before formally analysing categorical data. You should take a moment and examine a frequency table for categorical variables, to ensure all recorded values are within scope.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#summarising-two-categorical-variables-numerically",
    "href": "02-probability.html#summarising-two-categorical-variables-numerically",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.5 Summarising two categorical variables numerically",
    "text": "2.5 Summarising two categorical variables numerically\nSo far, we have discussed one-way frequency tables, that is, tables that summarise one variable. We can summarise more than two categorical variables in a table – called a cross tabulation, or a two-way (summarising two variables) table.\nUsing our PBC data, we can summarise the two categorical variables: sex and stage of disease. The two-way table of frequencies is shown in Table 2.3.\n\n\n\n\nTable 2.3: Frequency of participants by sex and stage of disease*\n\n\n\n Stage 1234TotalSexMale38161744Female1884139127368Total2192155144412* Disease stage was missing for 6 participants\n\n\n\n\n\nWe can add percentages to two-way tables as either column or row percents. Using Table 2.3 as an example, column percents represent the relative frequencies of sex within each stage (Table 2.4).\n\n\n\n\nTable 2.4: Frequency of participants by sex and stage of disease*, including column percents\n\n\n\n Stage 1234TotalSexMale3 (14%)8 (9%)16 (10%)17 (12%)44 (11%)Female18 (86%)84 (91%)139 (90%)127 (88%)368 (89%)Total21 (100%)92 (100%)155 (100%)144 (100%)412 (100%)* Disease stage was missing for 6 participants\n\n\n\n\n\nConversely, row percents represent the relative frequencies of stage within each sex (Table 2.5).\n\n\n\n\nTable 2.5: Frequency of participants by sex and stage of disease, including row percents\n\n\n\n Stage 1234TotalSexMale3 (7%)8 (18%)16 (36%)17 (39%)44 (100%)Female18 (5%)84 (23%)139 (38%)127 (35%)368 (100%)Total21 (5%)92 (22%)155 (38%)144 (35%)412 (100%)* Disease stage was missing for 6 participants\n\n\n\n\n\n\nTables containing more than two variables\nIt is possible to construct multi-way tables that summarise more than two categorical variables in a single table. However, tables can become complex when more than two variables are incorporated, and you may need to present the information as two tables or incorporate additional rows and columns.\nIn Figure 2.2, characteristics of the sample of prisoners from the NPHDC were presented. This table contains information about sex, age group and Indigenous status from different groups of prisoners; prison entrants, discharges, and prisoners in custody. This type of condensed information is often found in reports and journal articles giving demographic information, by different groups considered in the study.\n\n\n\n\n\n\nFigure 2.2\n\n\n\nWe might also consider a table containing further pieces of information. The table presented in Figure 2.3 (from the health of Australia’s prisoners 2015 report) compares prison entrants and the general community by three variables: age group, Indigenous status, and highest level of completed education.\nCan you see any issues with the presentation of this table?\n\n\n\n\n\n\n\n\nFigure 2.3: Highest level of completed education in prison entrants and the general community\n\n\n\n\n\nSource: Australian Institute of Health and Welfare 2015. The health of Australia’s prisoners 2015. Cat. no. PHE 207. Canberra: AIHW.\nSome issues in this table:\n\nThe title of the table does not contain full information about the variables in the table;\nIt is unclear how the percentages were calculated (which groupings added to 100%);\nThe ages are not labelled as such, thus without reading the text in report it is unclear that these are age groupings.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#summarising-two-categorical-variables-graphically",
    "href": "02-probability.html#summarising-two-categorical-variables-graphically",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.6 Summarising two categorical variables graphically",
    "text": "2.6 Summarising two categorical variables graphically\nInformation from more than one variable can be presented as clustered or multiple bar chart (bars side-by-side) (Figure 2.4). This type of graph is useful when examining changes in the categories separately, but also comparing the grouping variable between the main bar variable. Here we can see that Stage 3 and Stage 4 disease is the most common for both males and females, but there are many more females within each stage of disease.\n\n\n\n\n\n\n\n\nFigure 2.4: Bar graph of stage of disease by sex from PBC study\n\n\n\n\n\nAn alternative bar graph is a stacked or composite bar graph, which retains the overall height for each category, but differentiates the bars by another variable (Figure 2.5).\n\n\n\n\n\n\n\n\nFigure 2.5: Stacked bar graph of stage of disease by sex from PBC study\n\n\n\n\n\nFinally, a stacked relative bar chart (Figure 2.6) displays the proportion of grouping variable for each bar, where each overall bar represents 100%. These graphs allow the reader to compare the proportions between categories. We can easily see from Figure 2.6 that the distribution of sex is similar across each stage of disease.\n\n\n\n\n\n\n\n\nFigure 2.6: Relative frequency of sex within stage of disease from PBC study",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#presentation-guidelines",
    "href": "02-probability.html#presentation-guidelines",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.7 Presentation guidelines",
    "text": "2.7 Presentation guidelines\n\nGuidelines for presenting summary statistics\nWhen reporting summary statistics, it is important not to present results with too many decimal places. Doing so implies that your data have a higher level of precision than they do. For example, presenting a mean blood pressure of 100.2487 mmHg implies that blood pressure can be measured accurately to at least three decimal places.\nThere are a number of guidelines that have been written to help in the presentation of numerical data. Many of these guidelines are based on the number of decimal places, while others are based on the number of significant figures. Briefly, the number of significant figures are “the number of digits from the first non-zero digit to the last meaningful digit, irrespective of the position of the decimal point. Thus, 1.002, 10.02, 100200 (if this number is expressed to the nearest 100) all have four significant digits.” Armitage, Berry, and Matthews (2013)\nA summary of these guidelines that will be used in this course appear in Table 2.6.\n\n\n\n\nTable 2.6: Guidelines for presentation of statistical results\n\n\n\nSummary statisticGuideline (reference)MeanIt is usually appropriate to quote the mean to one extra decimal place compared with the raw data. (Altman)Median, Interquartile range, RangeAs medians, interquartile ranges and ranges are based on individual data points, these values should be presented with the same precision as the original data.PercentagePercentages do not need to be given with more than one decimal place at most. When the sample size is less than 100, no decimal places should be given. (Altman)ProbabilityIt is acceptable to present probabilities to 2 or 3 decimal places. If the probability is presented as a percentage, present the percentage with 0 or 1 decimal place.Standard deviationThe standard deviation should usually be given to the same accuracy as the mean, or with one extra decimal place. (Altman)Standard errorAs per standard deviationConfidence intervalUse the same rule as for the corresponding effect size (be it mean, percentage, mean difference, regression coefficient, correlation coefficient or risk ratio) (Cole)Test statisticTest statistics should not be presented with more than two decimal places.P-valueReport P-values to a single significant figure unless the P-value is close to 0.05 (say, 0.01 to 0.2), in which case, report two significant figures. Do not report `not significant` for P-values of 0.05 or higher. Very low P-values can be reported as P &lt; 0.001 or P &lt; 0.0001. A P-value can indeed be 1, although some investigators prefer to report this as &gt;0.9. (Based on Assel)Difference in meansAs for the estimated meansDifference in proportionsAs for the estimated proportionsOdds ratio / Relative riskHazard and odds ratios are normally reported to two decimal places, although this can be avoided for high odds ratios (Assel)Correlation coefficientOne or two decimal places, or more when very close to ±1   (Cole)Regression coefficientUse one more significant figure than the underlying data   (adapted from Cole)\n\n\n\n\n\n\n\nTable presentation guidelines\nConsider the following guidelines for the appropriate presentation of tables in scientific journals and reports (Woodward, 2013).\n\nEach table (and figure) should be self-explanatory, i.e. the reader should be able to understand it without reference to the text in the body of the report.\n\nThis can be achieved by using complete, meaningful labels for the rows and columns and giving a complete, meaningful title.\nFootnotes can be used to enhance the explanation.\n\nUnits of the variables (and if needed, method of calculation or derivation) should be given and missing records should be noted (e.g. in a footnote).\nA table should be visually uncluttered.\n\nAvoid use of vertical lines.\nHorizontal lines should not be used in every single row, but they can be used to group parts of the table.\nSensible use of white space also helps enormously; use equal spacing except where large spaces are left to separate distinct parts of the table.\nDifferent typefaces (or fonts) may be used to provide discrimination, e.g. use of bold type and/or italics.\n\nThe rows and columns of each table should be arranged in a natural order to help interpretation. For instance, when rows are ordered by the size of the numbers they contain for a nominal variable, it is immediately obvious where relatively big and small contributions come from.\nTables should have a consistent appearance throughout the report so that the paper is easy to follow (and also for an aesthetic appearance). Conventions for labelling and ordering should be the same (for both tables as well as figures) for ease of comparison of different tables (and figures).\nConsider if there is a particular table orientation that makes a table easier to read.\n\nGiven the different possible formats of tables and their complexity, some further guidelines are given in the following excellent references:\n\nGraphics and statistics for cardiology: designing effective tables for presentation and publication, Boers (2018)\nGuidelines for Reporting of Figures and Tables for Clinical Research in Urology, Vickers et al. (2020)\n\n\n\nGraphical presentation guidelines\nConsider the following guidelines for the appropriate presentation of graphs in scientific journals and reports (Woodward, 2013).\n\nFigures should be self-explanatory and have consistent appearance through the report.\nA title should give complete information. Note that figure titles are usually placed below the figure, whereas for tables titles are given above the table.\nAxes should be labelled appropriately\nUnits of the variables should be given in the labelling of the axes. Use footnotes to indicate any calculation or derivation of variables and to indicate missing values\nIf the Y-axis has a natural origin, it should be included, or emphasised if it is not included.\nIf graphs are being compared, the Y-axis should be the same across the graphs to enable fair comparison\nColumns of bar charts should be separated by a space\nThree dimensional graphs should be avoided unless the third dimension adds additional information\n\nSources:\nAltman (1990)\nCole (2015)\nAssel et al. (2019)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#probability",
    "href": "02-probability.html#probability",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.8 Probability",
    "text": "2.8 Probability\nProbability is defined as:\n\nthe chance of an event occurring, where an event is the result of an observation or experiment, or the description of some potential outcome.\n\nProbabilities range from 0 (where the event will never occur) to 1 (where the event will always occur). For example, tossing a coin is an experiment; one event is the coin landing with head up, while the other event is the coin landing tails up. The set of all possible outcomes in an experiment is called the sample space. For example, by tossing a coin you can get either a head or a tail (called mutually exclusive events); and by rolling a die you can get any of the six sides. Thus, for a die the sampling space is: S = {1, 2, 3, 4, 5, 6}\nWith a fair (unbiased) die, the probability of each outcome occurring is 1/6 and its probability distribution is simply a probability of 1/6 for each of the six numbers on a die.\n\nAdditive law of probability\nHow do we work out the probability that one roll of a die will turn out to be a 3 or a 6? To do that, we first need to work out whether the events (3 or 6 on the roll of a die) are mutually exclusive. Events are mutually exclusive if they are events which cannot occur at the same time. For example, rolling a die once and getting a 3 and 6 are mutually exclusive events (you can roll one or the other but not both in a single roll).\nTo obtain the probability of one or the other of two mutually exclusive events occurring, the sum of the probabilities of each is taken. For example, the probability of the roll of a die being a 3 or a 6 is the sum of the probability of the die being 3 (i.e. 1/6) and the probability of the die being 6 (also 1/6). With a fair die:\nProbability of a die roll being 3 or 6 = \\(1/6 + 1/6 = 1/3\\)\nAnother way of putting it is:\nP(die roll =3 or die roll =6) = P(die roll=3) + P(die roll=6) = \\(1/6 + 1/6 = 1/3\\)\n\nExample: Additive law for mutually exclusive events\nConsider that blood type can be organised into the ABO system (blood types A, B, AB or O) An individual may only have one blood type.\nUsing the information from https://www.donateblood.com.au/learn/about-blood let’s consider the ABO blood type system. The frequency distribution (prevalence) of the ABO blood type system in the population represents the probability of each of the outcomes. If we consider all possible blood type outcomes, then the total of the probabilities will sum to 1 (100%).\n\n\n\n\nTable 2.7: Frequency of blood types\n\n\n\nBlood Type% of populationProbabilityA38%0.38B10%0.10AB3%0.03O49%0.49Total100%1.00\n\n\n\n\n\nIn this example we consider: What is the probability that an individual will have either blood group O or A?\nSince blood type is mutually exclusive, the probability that either one or the other occurs is the sum of the individual probabilities. These are mutually exclusive events so we can say P(O or A) = P(O) + P(A)\nThus, the answer is: P(Blood type O) + P(Blood type A) = 0.49 + 0.38 = 0.87\n\n\n\nMultiplicative law of probability\nThe additive law of probability lets us consider the probability of different outcomes in a single experiment. The multiplicative law lets us consider the probability of multiple events occurring in a particular order. For example: if I roll a die twice, what is the probability of rolling a 3 and then a 6?\nThese events are independent: the probability of rolling a 6 on the second roll is not affected by the first roll.\nThe multiplicative law of probability states:\n\nIf A and B are independent, then P(A and B) = P(A) \\(\\times\\) P(B).\n\nSo, the probability of rolling a 3 and then a 6 is: P(3 and 6) = \\(1/6 \\times 1/6 = 1/36\\).\nNote here that the order matters – we are considering the probability of rolling a 3 and then a 6, not the probability of rolling a 6 and then a 3.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#probability-distributions",
    "href": "02-probability.html#probability-distributions",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.9 Probability distributions",
    "text": "2.9 Probability distributions\n\nA probability distribution is a table or a function that provides the probabilities of all possible outcomes for a random event.\n\nFor example, the probability distribution for a single coin toss is straightforward: the probability of obtaining a head is 0.5, and the probability of obtaining a tail is 0.5, and this can be summarised in Table 2.8.\n\n\n\n\nTable 2.8: Probability distribution for a single coin toss\n\n\n\nCoin faceProbabilityHeads0.5Tails0.5\n\n\n\n\n\nSimilarly, the probability distribution for a single roll of a die is straightforward: each face has a probability of 1/6 (Table 2.9).\n\n\n\n\nTable 2.9: Probability distributions for a single roll of a die\n\n\n\nFace of a dieProbability11/621/631/641/651/661/6\n\n\n\n\n\nThings become more complicated when we consider multiple coin-tosses, or rolls of a die. These series of events can be summarised by considering the number of times a certain outcome is observed. For example, the probability of obtaining three heads from five coin tosses.\nProbability distributions can be used in two main ways:\n\nTo calculate the probability of an event occurring. This seems trivial for the coin-toss and die-roll examples above. However, we can consider more complex events, as below.\nTo understand the behaviour of a sample statistic. We will see in Modules 3 and 4 that we can assume the mean of a sample follows a probability distribution. We can obtain useful information about the sample mean by using properties of the probability distribution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#discrete-random-variables-and-their-probability-distributions",
    "href": "02-probability.html#discrete-random-variables-and-their-probability-distributions",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.10 Discrete random variables and their probability distributions",
    "text": "2.10 Discrete random variables and their probability distributions\nRather than thinking of random events, we often use the term random variable to describe a quantity that can have different values determined by chance.\nA discrete random variable is a random variable that can take on only countable values (that is, non-negative whole numbers). An example of a discrete random variable is the number of heads observed in a series of coin tosses.\nA discrete random variable can be summarised by listing all the possible values that the variable can take. As defined earlier, a table, formula or graph that presents these possible values, and their associated probabilities, is called a probability distribution.\nExample: let’s consider the number of heads in a series of three coin tosses. We might observe 0 heads, or 1 head, or 2, or 3 heads. If we let X denote the number of heads in a series of three coin tosses, then possible values of X are 0, 1, 2 or 3.\nWe write the probability of observing x heads as P(X=x). So P(X=0) is the probability that the three tosses has no heads. Similarly, P(X=1) is the probability of observing one head.\nThe possible combinations for three coin tosses are as follows:\n\n\n\n\nTable 2.10: The number of heads from three coin tosses\n\n\n\nPatternNumber of headsTail, Tail, Tail0Head, Tail, Tail1Tail, Head, TailTail, Tail, HeadHead, Head, Tail2Head, Tail, HeadTail, Head, HeadHead, Head, Head3\n\n\n\n\n\nThere are eight possible outcomes from three coin tosses (permutations). If we assume an equal chance of observing a head or a tail, each permutation above is equally likely, and so has a probability of 1/8.\nIf we consider the possibility of observing just one head out of the three tosses, this can happen in three ways (HTT, THT, TTH). So the probability of observing one head is calculated using the additive law: P(X=1) = \\(\\tfrac{1}{8} + \\tfrac{1}{8} + \\tfrac{1}{8} = \\tfrac{3}{8}\\).\nTherefore, the probability distribution for X, the number of heads from three coin tosses, is as follows:\n\n\n\n\nTable 2.11: Probability distribution for the number of heads from three coin tosses\n\n\n\nx (number of heads observed)P(X=x)01/811/8 + 1/8 + 1/8 = 3/821/8 + 1/8 + 1/8 = 3/831/8\n\n\n\n\n\nNote that the probabilities sum to 1.\nThe above example was based on a coin toss, where flipping a head or a tail is equally likely (both have probabilities of 0.5). Let’s consider a case where the probability of an event is not equal to 0.5: having blood type A.\nFrom Table 2.7, the probability that a person has Type A blood is 0.38, and therefore, the probability that a person does not have Type A blood is 0.62 (1–0.38). If we considered taking a random sample of three people, the probability that all three would have Type A blood is 0.38 × 0.38 × 0.38 (using the multiplicative rule above) – and there is only one way this could happen.\nThe number of ways two people out of three could have Type A blood is 3, and each permutation is listed in Table 2.12. The probability of observing each of the three patterns is the same, and can be calculated using the multiplicative rule: 0.38 × 0.38 × 0.62 = 0.0895.\n\n\n\n\nTable 2.12: Combinations and probabilities of Type A blood in three people\n\n\n\nPerson 1Person 2Person 3ProbabilityAAA0.38 × 0.38 × 0.38 = 0.0549AANot A0.38 × 0.38 × 0.62 = 0.0895ANot AA0.38 × 0.62 × 0.38 = 0.0895Not AAA0.62 × 0.38 × 0.38 = 0.0895ANot ANot A0.38 × 0.62 × 0.62 = 0.1461Not AANot A0.62 × 0.38 × 0.62 = 0.1461Not ANot AA0.62 × 0.62 × 0.38 = 0.1461Not ANot ANot A0.62 × 0.62 × 0.62 = 0.2383\n\n\n\n\n\nTable 2.13 gives the probability of each of the blood type combinations we could observe in three people. The probability of observing a certain number of people (say, k) with Type A blood from a sample of three people can be calculated by summing the combinations:\n\n\n\n\nTable 2.13: Probabilities of observing numbers of people with Type A blood in a sample of three people\n\n\n\nNumber of people with Type A bloodProbability of each pattern30.054920.0895 + 0.0895 + 0.0895 = 0.268910.1461 + 0.1461 + 0.1461 = 0.438200.2383",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#binomial-distribution",
    "href": "02-probability.html#binomial-distribution",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.11 Binomial distribution",
    "text": "2.11 Binomial distribution\nThe above are examples of the binomial distribution. The binomial distribution is used when we have a collection of random events, where each random event is binary (e.g. Heads vs Tails, Type A blood vs Not Type A blood, Infected vs Not infected). The binomial distribution calculates (in general terms):\n\nthe probability of observing k successes\nfrom a collection of n trials\nwhere the probability of a success in one trial is p.\n\nThe terms used here can be defined as:\n\na success is simply an event of interest from a binary random event. In the coin-toss example, “success” was tossing a Head. In the blood type example, we were only interested in whether someone was Type A or not Type A, so “success” was a blood of Type A. We tend to use the word “success” to mean “an event of interest”, and “failure” as “an event not of interest”.\nthe number of trials refers to the number of random events observed. In both examples, we observed three events (three coin tosses, three people).\nthe probability of a success (p) simply refers to the probability of the event of interest. In the coin toss example, this was the probability of tossing a Heads (=0.5); for the blood-type example, this was the probability of having Type A blood (0.38).\n\nPutting all this together, we say that we have a binomial experiment. To satisfy the assumptions of a binomial distribution, our experiment must satisfy the following criteria:\n\nThe experiment consists of fixed number (n) of trials.\nThe result of each trial falls into only one of two categories – the event occurred (“success”) or the event did not occur (“failure”).\nThe probability, p, of the event occurring remains constant for each trial.\nEach trial of the experiment is independent of the other trials.\n\nWe have shown in the examples above how we can calculate the probabilities for small experiments (n=3). Once n becomes large, constructing such probability distribution tables becomes difficult. The general formula for calculating the probability of observing k successes from n trials, where each trial has a probability of success of p is given by:\n\\[ P(X=k) = \\frac{n!}{k! (n-k)!} \\times p^k \\times (1-p)^{n-k} \\]\nwhere \\(n! = n \\times (n-1) \\times (n-2) \\times \\dots \\times 2 \\times 1\\).\nNote that this formula is almost never calculated by hand. Instructions for calculating binomial probabilities are given in the jamovi and R notes at the end of this Module.\n\nWorked example\nA population-based survey conducted by the AIHW (2008) of a random sample of the Australian population estimated that in 2007, 19.8% of the Australian population were current smokers.\n\nFrom a random sample of 6 people from the Australian population in 2007, what is the probability that 3 of them will be smokers?\nWhat is the probability that among the six persons, at least 4 will be smokers?\nWhat is the probability that at most, 2 will be smokers?\n\n\n\nSolution\n\nCalculating this single binomial probability is best done using software.\n\nWe can use the applet to calculate this, or use the dbinom function in R with x=3, size=6, and prob=0.198. This gives an answer of 0.08.\n\nIn common language, getting “at least 4” smokers means getting 4, 5 or 6 smokers. Since these are mutually exclusive events, we can apply the additive law to find the probability of getting at least 4 smokers:\n\n\\[ P(X \\ge 4) = P(X=4) + P(X=5) + P(X=6) \\] Using the same binomial probability functions as in the previous question, we could calculate\n\nP(X=4) = 0.0148\nP(X=5) = 0.00146\nP(X=6) = 0.0000603\n\nAnswer: \\(P(X \\ge 4) = 0.0148 + 0.00146 + 0.0000603 = 0.016\\)\nWe can use the applet to calculate this, or in R we can use the pbinom function with the lower.tail=FALSE option.\n\nObserving at most two means observing 0, 1 or 2 smokers. Therefore, the probability of observing at most 2 smokers is:\n\n\nP(X \\(\\le\\) 2) = P(X=0) + P(X=1) + P(X=2)\nP(X=0) = 0.266\nP(X=1) = 0.394\nP(X=2) = 0.243\n\nAnswer: P(X \\(\\le\\) 2) = 0.266+0.394+0.243=0.903\nAgain, we can use the applet to calculate this, or use the pbinom function in R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#producing-a-one-way-frequency-table",
    "href": "02-probability.html#producing-a-one-way-frequency-table",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.12 Producing a one-way frequency table",
    "text": "2.12 Producing a one-way frequency table\nThe simplest way to summarise categorical variables is with a one-way frequency table. These are constructed using Analyses &gt; Exploration. We will illustrate this by summarising the variable sex from the pbc.rds data from the previous module.\n\njamovi has summarised sex here, just as we asked it to, however it has analysed sex as if it was a continuous variable. This is incorrect: sex is a categorical variable. This can be corrected by defining sex to be categorical within Data &gt; Setup:\n\nSelect sex, then choose Nominal as the Measure type. jamovi now lists the levels of sex as 1 and 2. These should be replaced by the categories they represent.\nFrom the mod01_pbc_info.txt file, we see that 1 represents Male, and 2 represents Female. These labels can be added by typing in the appropriate cell. The completed screen should look like this:\n\n\nClicking back to the original summary of sex shows that jamovi is no longer treating sex as a continuous variable, but there is little output in the summary:\n\nClick Frequency tables in the main Descriptives window to request the one-way frequency table:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#producing-a-two-way-frequency-table",
    "href": "02-probability.html#producing-a-two-way-frequency-table",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.13 Producing a two-way frequency table",
    "text": "2.13 Producing a two-way frequency table\nTwo-way tables are constructed using Analyses &gt; Frequencies &gt; Contingency Tables &gt; Independent Samples. Note that both variables must be defined as Nominal variables. As an example, to produce a two-way table of disease stage by sex:\n\nIgnore the output labelled χ2 tests for now.\nRow or column percents can be requested in the Cells section. For example, to calculate the proportion of males within each stage of disease, we would request column percents:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#creating-bar-charts-for-one-categorical-variable",
    "href": "02-probability.html#creating-bar-charts-for-one-categorical-variable",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.14 Creating bar charts for one categorical variable",
    "text": "2.14 Creating bar charts for one categorical variable\nHere we will create the bar chart shown in Figure 2.1 using the mod01_pbc.rds dataset. The x-axis of this graph will be the stage of disease, and the y-axis will show the number of participants in each category.\nBar charts are created in the Exploration tab. We can summarise stage, and request a Frequency table in the usual way. To request a bar chart as well, tick Bar plot within the Plots section:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#creating-bar-charts-for-two-categorical-variables",
    "href": "02-probability.html#creating-bar-charts-for-two-categorical-variables",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.15 Creating bar charts for two categorical variables",
    "text": "2.15 Creating bar charts for two categorical variables\nCreating a clustered bar chart as shown in Figure 2.4 can be done using Analyses &gt; Frequencies &gt; Contingency Tables &gt; Independent Samples. First create the cross-tab of interest, for example, Stage by Sex. Choose Plots &gt; Bar Plot, and ensure the Bar Type is selected as Side by side. Choose the X-axis as required - here we want to see stage on the x-axis, so we choose the x-axis to be columns (this may need to be adjusted according to how your table has been set up):\n To create a stacked bar chart (as in Figure 2.5), choose the Bar Type to be Stacked:\n\nFinally, to create a stacked relative bar chart (as in Figure 2.6), choose the Y-axis to be Percentages within column:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#sec-recoding-data-jmv",
    "href": "02-probability.html#sec-recoding-data-jmv",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.16 Recoding data",
    "text": "2.16 Recoding data\nOne task that is common in statistical computing is to recode variables. For example, we might want to group some categories of a categorical variable, or to present a continuous variable in a categorical way.\nIn this example, we can use the pbc data and recode age into age groups:\n\nLess than 30\n30 to less than 50\n50 to less than 70\n70 or older\n\nRecoding can be done using Data &gt; Transform.\nFirst, click Data to view the spreadsheet, then click in an empty column, then click Setup &gt; NEW TRANSFORMED VARIABLE. We need to specify three things:\n\nThe name of the new variable. Here, we will choose age group.\nThe source variable. Here, we want to recode from age, so choose age.\nThe transform, which is where we define the rules of the recode. Choose Create New Transform.\n\n\nThe transform is built up by specifying the recode conditions. Click + Add recode condition to define the first condition:\n\nHere we want to define all ages less than 30 as “Less than 30”. Complete the recode condition so it appears as: if $source &lt; 30 use \"Less than 30\". Note that the quotation marks around “Less than 30” are required:\n\nAdd another recode condition, which will be applied if the first condition is not satisfied: if $source &lt; 50 use \"30 to less than 50\":\n\nAdd another condition: if $source &lt; 70 use \"50 to less than 70\". There is no need to add a condition for the final condition, simply complete the final line: else use \"70 or more\":\n\nFinally, click the down arrow to dismiss the transform builder, and the up arrow to dismiss the transform dialog.\nWe can examine the new categories by obtaining a frequency table: Analyses &gt; Exploration &gt; Descriptives:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#computing-binomial-probabilities",
    "href": "02-probability.html#computing-binomial-probabilities",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.17 Computing binomial probabilities",
    "text": "2.17 Computing binomial probabilities\njamovi does not have a point-and-click method for computing probabilities from a binomial distribution. Here, instructions are provided for using a third-party applet. This Binomial Distribution Applet has been posted at https://homepage.stat.uiowa.edu/~mbognar/applets/bin.html, and provides a simple and intuitive way to compute probabilities from a binomial distribution.\nThe applet requires three pieces of information:\n\n\\(/n\\): the number of binary trials being considered\n\\(/p\\): the probability of “success” in each trial\n\\(x\\): the number of success we are interested in\n\nWe also need to consider whether we are interested in the probability being equal to, greater than, or less than x.\nTo do the computation for part (a) in Worked Example 2.1:\n\nx is the number of successes, here, the number of smokers (i.e. x=3);\nn is the number of trials (i.e. n=6);\nand p is probability of drawing a smoker from the population, which is 19.8% (i.e. p=0.198).\n\nReplace each of these with the appropriate number into the applet:\n\nTo calculate the probability of at least 4 smokers in part (b), we change the drop-down to “P(X≥x)”, and x to be equal to 4:\n\nTo calculate the probability of at most 2 smokers part (c), we we change the drop-down to “P(X≤x)”, and x to be equal to 2:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#creating-bar-charts-for-one-categorical-variable-1",
    "href": "02-probability.html#creating-bar-charts-for-one-categorical-variable-1",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.18 Creating bar charts for one categorical variable",
    "text": "2.18 Creating bar charts for one categorical variable\nThe simplest way to use the plot() function is by specifying an object to be plotted. To plot a single variable from a data frame, we must define it using: dataframe$variable.\nHere we will create the bar chart shown in Figure 2.1 of the statistics notes using the mod01_pdc.rds dataset. The x-axis of this graph will be the stage of disease, and the y-axis will show the number of participants in each category.\n\nplot(pbc$stage,\n    main = \"Bar graph of stage of disease from PBC study\",\n    ylab = \"Number of participants\"\n)\n\n\n\n\n\n\n\n\nNote that stage is a categorical variable, that has been defined as a factor (in Section 2.17.1.1). You must define categorical data as factors to plot them in a bar graph.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#creating-bar-charts-for-two-categorical-variables-1",
    "href": "02-probability.html#creating-bar-charts-for-two-categorical-variables-1",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.19 Creating bar charts for two categorical variables",
    "text": "2.19 Creating bar charts for two categorical variables\nCreating a clustered bar chart as shown in Figure 2.4 can be done easily using the contTables function in the jmv package. First create a cross-tab from the variables to be plotted, for example, Stage by Sex:\n\ncontTables(data = pbc, rows = sex, cols = stage)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                              \n ─────────────────────────────────────────────────────────────── \n   sex       Stage 1    Stage 2    Stage 3    Stage 4    Total   \n ─────────────────────────────────────────────────────────────── \n   Male            3          8         16         17       44   \n   Female         18         84        139        127      368   \n   Total          21         92        155        144      412   \n ─────────────────────────────────────────────────────────────── \n\n\n χ² Tests                               \n ────────────────────────────────────── \n         Value        df    p           \n ────────────────────────────────────── \n   χ²    0.8779873     3    0.8307365   \n   N           412                      \n ────────────────────────────────────── \n\n\nCreating a clustered bar chart as shown in Figure 2.4 can be done by requesting a bar chart (barplot=TRUE), and the x-axis should be constructed from stage - that is, the column variable:\n\ncontTables(pbc,\n    rows = sex, cols = stage,\n    barplot = TRUE, xaxis = \"xcols\"\n)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                              \n ─────────────────────────────────────────────────────────────── \n   sex       Stage 1    Stage 2    Stage 3    Stage 4    Total   \n ─────────────────────────────────────────────────────────────── \n   Male            3          8         16         17       44   \n   Female         18         84        139        127      368   \n   Total          21         92        155        144      412   \n ─────────────────────────────────────────────────────────────── \n\n\n χ² Tests                               \n ────────────────────────────────────── \n         Value        df    p           \n ────────────────────────────────────── \n   χ²    0.8779873     3    0.8307365   \n   N           412                      \n ────────────────────────────────────── \n\n\n\n\n\n\n\n\n\nIf you want the x-axis to be constructed from the row variable, you would use xaxis = \"xrows\".\nTo create a stacked bar chart (as in Figure 2.5), specify bartype to be stack:\n\ncontTables(pbc,\n    rows = sex, cols = stage,\n    barplot = TRUE, xaxis = \"xcols\", bartype = \"stack\"\n)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                              \n ─────────────────────────────────────────────────────────────── \n   sex       Stage 1    Stage 2    Stage 3    Stage 4    Total   \n ─────────────────────────────────────────────────────────────── \n   Male            3          8         16         17       44   \n   Female         18         84        139        127      368   \n   Total          21         92        155        144      412   \n ─────────────────────────────────────────────────────────────── \n\n\n χ² Tests                               \n ────────────────────────────────────── \n         Value        df    p           \n ────────────────────────────────────── \n   χ²    0.8779873     3    0.8307365   \n   N           412                      \n ────────────────────────────────────── \n\n\n\n\n\n\n\n\n\nFinally, to create a stacked relative bar chart (as in Figure 2.6), specify the y-axis to be a percent (yaxis=\"ypc\"), and the percentage be calculated from the columns of the frequency table (yaxisPc = \"column_pc\"):\n\ncontTables(pbc,\n    rows = sex, cols = stage,\n    barplot = TRUE, bartype = \"stack\", xaxis = \"xcols\",\n    yaxis = \"ypc\", yaxisPc = \"column_pc\"\n)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                              \n ─────────────────────────────────────────────────────────────── \n   sex       Stage 1    Stage 2    Stage 3    Stage 4    Total   \n ─────────────────────────────────────────────────────────────── \n   Male            3          8         16         17       44   \n   Female         18         84        139        127      368   \n   Total          21         92        155        144      412   \n ─────────────────────────────────────────────────────────────── \n\n\n χ² Tests                               \n ────────────────────────────────────── \n         Value        df    p           \n ────────────────────────────────────── \n   χ²    0.8779873     3    0.8307365   \n   N           412                      \n ──────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#importing-data-into-r",
    "href": "02-probability.html#importing-data-into-r",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.20 Importing data into R",
    "text": "2.20 Importing data into R\nWe have described previously how to import data that have been saved as R .rds files. It is quite common to have data saved in other file types, such as Microsoft Excel, or plain text files. In this section, we will demonstrate how to import data from other packages into R.\nThere are two useful packages for importing data into R: haven (for data that have been saved by jamovi, SAS or SPSS) and readxl (for data saved by Microsoft Excel). Additionally, the labelled package is useful in working with data that have been labelled in jamovi.\n\nImporting plain text data into R\nA csv file, or a “comma separated variables” file is commonly used to store data. These files have a very simple structure: they are plain text files, where data are separated by commas. csv files have the advantage that, as they are plain text files, they can be opened by a large number of programs (such as Notepad in Windows, TextEdit in MacOS, Microsoft Excel - even Microsoft Word). While they can be opened by Microsoft Excel, they can be opened by many other programs: the csv file can be thought of as the lingua-franca of data.\nIn this demonstration, we will use data on the weight of 1000 people entered in a csv file called mod02_weight_1000.csv available on Moodle.\nTo confirm that the file is readable by any text editor, here are the first ten lines of the file, opened in Notepad on Microsoft Windows, and TextEdit on MacOS.\n\n\n\n\n\n\n\n\n\nWe can use the read.csv function:\n\nsample &lt;- read.csv(\"data/examples/mod02_weight_1000.csv\")\n\nHere, the read.csv function has the default that the first row of the dataset contains the variable names. If your data do not have column names, you can use header=FALSE in the function.\nNote: there is an alternative function read_csv which is part of the readr package (a component of the tidyverse). Some would argue that the read_csv function is more appropriate to use because of an issue known as strings.as.factors. The strings.as.factors default was removed in R Version 4.0.0, so it is less important which of the two functions you use to import a .csv file. More information about this issue can be found here and here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#sec-recoding-data",
    "href": "02-probability.html#sec-recoding-data",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.21 Recoding data",
    "text": "2.21 Recoding data\nOne task that is common in statistical computing is to recode variables. For example, we might want to group some categories of a categorical variable, or to present a continuous variable in a categorical way.\nIn this example, we can use the pbc data and recode age into age groups:\n\nLess than 30\n30 to less than 50\n50 to less than 70\n70 or older\n\nThe quickest way to recode a continuous variable into categories is to use the cut command which takes a continuous variable, and “cuts” it into groups based on the specified “cutpoints”\n\npbc$agegroup &lt;- cut(pbc$age,\n    breaks = c(0, 30, 50, 70, 100)\n)\n\nNotice that some numbers need to be defined for the lowest (age=0) and highest (age=100) bounds: both a lower and upper limit must be defined for each group.\nIf we examine the new agegroup variable:\n\nsummary(pbc$agegroup)\n\n  (0,30]  (30,50]  (50,70] (70,100] \n       3      192      210       13 \n\n\nwe see that each group has been labelled in the form of (a, b]. This notation is equivalent to: greater than a, and less than or equal to b. The cut function excludes the lower limit, but includes the upper limit. Our age groups have been defined to include the lower limit, and exclude the upper limit (for example, greater than or equal to 30 and less than 50).\nWe can specify this recoding using the right=FALSE option:\n\npbc$agegroup &lt;- cut(pbc$age,\n    breaks = c(0, 30, 50, 70, 100),\n    right = FALSE\n)\n\nsummary(pbc$agegroup)\n\n  [0,30)  [30,50)  [50,70) [70,100) \n       3      192      210       13 \n\n\nFinally, we can specify labels for the groups using the labels option:\n\npbc$agegroup &lt;- cut(pbc$age,\n    breaks = c(0, 30, 50, 70, 100),\n    right = FALSE,\n    labels = c(\n        \"Less than 30\", \"30 to less than 50\",\n        \"50 to less than 70\", \"70 or more\"\n    )\n)\n\nsummary(pbc$agegroup)\n\n      Less than 30 30 to less than 50 50 to less than 70         70 or more \n                 3                192                210                 13",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "02-probability.html#sec-binom-r",
    "href": "02-probability.html#sec-binom-r",
    "title": "2  Categorical data, presentation guidelines and probability distributions",
    "section": "2.22 Computing binomial probabilities using R",
    "text": "2.22 Computing binomial probabilities using R\nThere are two R functions that we can use to calculate probabilities based on the binomial distribution: dbinom and pbinom:\n\ndbinom(x, size, prob) gives the probability of obtaining x successes from size trials when the probability of a success on one trial is prob;\npbinom(q, size, prob) gives the probability of obtaining q or fewer successes from size trials when the probability of a success on one trial is prob;\npbinom(q, size, prob, lower.tail=FALSE) gives the probability of obtaining more than qsuccesses from size trials when the probability of a success on one trial is prob.\n\nTo do the computation for part (a) in Worked Example 2.1, we will use the dbinom function with:\n\nx is the number of successes, here, the number of smokers (i.e. k=3);\nsize is the number of trials (i.e. n=6);\nand prob is probability of drawing a smoker from the population, which is 19.8% (i.e. p=0.198).\n\nReplace each of these with the appropriate number into the formula:\n\ndbinom(x = 3, size = 6, prob = 0.198)\n\n[1] 0.08008454\n\n\nTo calculate the upper tail of probability in part (b), we use the pbinom(lower.tail=FALSE) function. Note that the pbinom(lower.tail=FALSE) function does not include q, so to obtain 4 or more successes, we need to enter q=3:\n\npbinom(q = 3, size = 6, prob = 0.198, lower.tail = FALSE)\n\n[1] 0.01635325\n\n\nFor the lower tail for part (c), we use the pbinom function:\n\npbinom(q = 2, size = 6, prob = 0.198)\n\n[1] 0.9035622",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical data, presentation guidelines and probability distributions</span>"
    ]
  },
  {
    "objectID": "03-precision.html",
    "href": "03-precision.html",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "",
    "text": "Learning objectives\nBy the end of this module you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#learning-objectives",
    "href": "03-precision.html#learning-objectives",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "",
    "text": "Describe the characteristics of a Normal distribution\nCompute probabilities from a Normal distribution using statistical software\nBriefly outline other types of distributions\nExplain the purpose of sampling, different sampling methods and their implications for data analysis\nDistinguish between standard deviation of a sample and standard error of a mean\nCalculate and interpret confidence intervals for a mean",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#optional-readings",
    "href": "03-precision.html#optional-readings",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapters 4 and 6. [UNSW Library Link]\nBland (2015); Sections 3.3 and 3.4, 8.1 to 8.3. [UNSW Library Link]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#introduction",
    "href": "03-precision.html#introduction",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn this module, we will continue our introduction to probability by considering probability distributions for continuous data. We will introduce one of the most important distributions in statistics: the Normal distribution.\nTo describe the characteristics of a population we can gather data about the entire population (as is undertaken in a national census) or we can gather data from a sample of the population. When undertaking a research study, taking a sample from a population is far more cost-effective and less time consuming than collecting information from the entire population. When a sample of a population is selected, summary statistics that describe the sample are used to make inferences about the total population from which the sample was drawn. These are referred to as inferential statistics.\nHowever, for the inferences about the population to be valid, a random sample of the population must be obtained. The goal of using random sampling methods is to obtain a sample that is representative of the target population. In other words, apart from random error, the information derived from the sample is expected to be much the same as the information collected from a complete population census as long as the sample is large enough.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#probability-for-continuous-variables",
    "href": "03-precision.html#probability-for-continuous-variables",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.2 Probability for continuous variables",
    "text": "3.2 Probability for continuous variables\nCalculating the probability for a categorical random variable is relatively straightforward, as there are only a finite number of possible events. However, there are an infinite number of possible values for a continuous variable, and we calculate the probability that the continuous variable lies in a range of values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#normal-distribution",
    "href": "03-precision.html#normal-distribution",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.3 Normal distribution",
    "text": "3.3 Normal distribution\nThe frequency plot for many biological and clinical measurements (for example blood pressure and height) follow a bell shape where the curve is symmetrical about the mean value and has tails at either end. Figure 3.1 1 and Figure 3.2 2 demonstrate this type of distribution.\n\n\n\n\n\n\n\n\nFigure 3.1: Distribution of diastolic blood pressure, 2017–18 Australian Bureau of Statistics National Health Survey\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Distribution of male and female heights\n\n\n\n\n\nThe Normal distribution, also called the Gaussian distribution (named after Johann Carl Friedrich Gauss, 1777–1855), has been shown to fit the frequency distribution of many naturally occurring variables. It is characterised by its bell-shaped, symmetric curve and its tails that approach zero on either side.\nThere are two reasons for the importance of the Normal distribution in biostatistics (Kirkwood and Sterne, 2003). The first is that many variables can be modelled reasonably well using the Normal distribution. Even if the observed data were not Normally distributed, it can often be made reasonably Normal after applying some transformation of the data. The second (and possible most important) reason, is based on the central limit theorem and will be discussed later in this module.\nThe Normal distribution is characterised by two parameters: the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)). The mean defines where the middle of the Normal distribution is located, and the standard deviation defines how wide the tails of the distribution are.\nFor a Normal distribution, about 68% of the observations lie between \\(- \\sigma\\) and \\(\\sigma\\) of the mean; 95% of the observations lie between \\(−1.96 \\times \\sigma\\) and \\(1.96 \\times \\sigma\\) from the mean; and almost all the observations (99.7%) lie between \\(-3 \\times \\sigma\\) and \\(3 \\times \\sigma\\) (Figure 3.3). Also note that the mean is the same as the median, as the curve is symmetric about its mean.\n\n\n\n\n\n\n\n\nFigure 3.3: Characteristics of a Normal distribution",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#the-standard-normal-distribution",
    "href": "03-precision.html#the-standard-normal-distribution",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.4 The Standard Normal distribution",
    "text": "3.4 The Standard Normal distribution\nAs each Normal distribution is defined by its mean and standard deviation, there are an infinite number of possible Normal distributions. However, every Normal distribution can be transformed to what we call the Standard Normal distribution, which has a mean of zero (\\(\\mu = 0\\)) and a standard deviation of one (\\(\\sigma = 1\\)). The Standard Normal distribution is so important that it has been assigned its own symbol: Z.\nEvery observation from a Normal distribution \\(X\\) with a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\) can be transformed to a z-score (also called a Standard Normal deviate) by the formula:\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]\nThe z-score is simply how far an observation lies from the population mean value, scaled by the population standard deviation.\nWe can use z-scores to estimate probabilities, as shown in Worked Example 2.2.\n\nWorked Example\nThis example extends the example of diastolic blood pressure shown in Figure 3.1. Assume that the mean diastolic blood pressure for men is 77.9 mmHg, with a standard deviation of 11. What is the probability that a man selected at random will have high blood pressure (i.e. diastolic blood pressure ≥ 90)?\nTo estimate the probability that diastolic blood pressure ≥ 90 (i.e. the upper tail probability), we first need to calculate the z-score that corresponds to 90 mmHg.\nUsing the z-score formula, with x=90, \\(\\mu\\)=77.9 and \\(\\sigma\\)=11:\n\\[ z = \\frac{90 - 77.9}{11} = 1.1 \\] Thus, a blood pressure of 90 mmHg corresponds to a z-score of 1.1, or a value 1.1 \\(\\times \\sigma\\) above the mean weight of the population.\nFigure 3.4 shows the probability of a diastolic blood pressure of 90 mmHg or more in the population for a z-score of greater than 1.1 on a Standard Normal distribution.\n\n\n\n\n\n\n\n\nFigure 3.4: Area under the Standard Normal curve (as probability) for Z &gt; 1.1\n\n\n\n\n\nUsing software, we find the probability that a person has a diastolic blood pressure of 90 mmHg or more as P(Z ≥ 1.1) = 0.136.\nApart from calculating probabilities, z-scores are most useful for comparing measurements taken from a sample to a known population distribution. It allows measurements to be compared to one another despite being on different scales or having different predicted values.\nFor example, if we take a sample of children and measure their weights, it is useful to describe those weights as z-scores from the population weight distribution for each age and gender. Such distributions from large population samples are widely available. This allows us to describe a child’s weight in terms of how much it is above or below the population average. For example, if mean weights were compared, children aged 5 years would be on average heavier than the children aged 3 years simply because they are older and therefore larger. To make a valid comparison, we could use the Z-scores to say that children aged 3 years tend to be more overweight than children aged 5 years because they have a higher mean z-score for weight.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#assessing-normality",
    "href": "03-precision.html#assessing-normality",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.5 Assessing Normality",
    "text": "3.5 Assessing Normality\nThere are several ways to assess whether a continuous variable is Normally distributed. The best way to assess whether a variable is Normally distributed is to plot its distribution, using a density plot for example. If the density plot looks appoximately bell-shaped and approximately symmetrical, assuming Normality would be reasonable.\nIt may be useful to examine a boxplot of a variable in conjunction with a density plot. However a boxplot in isolation is not as useful as a density plot, as a boxplot only indicates whether a variable is distributed symmetrically (indicated by equal “whiskers”). A boxplot cannot give an indication of whether the distribution is bell-shaped, or flat.\n\nFor your information: There are formal tests that test for Normality. These tests are beyond the scope of this course and are not recommended.\n\nWe can construct a density plot for age in the pbc data introduced in Module 1. We can see that the density plot is approximately approximately bell-shaped and roughly symmetrical. The mean (50.7 years) and median (51 years) are similar, as would be expected for a Normal distribution. Thus, it would be reasonable to assume that age is Normally distributed in this set of data.\n\n\n\n\n\n\n\n\nFigure 3.5: Density plot of participant age from PBC study data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#non-normally-distributed-measurements",
    "href": "03-precision.html#non-normally-distributed-measurements",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.6 Non-Normally distributed measurements",
    "text": "3.6 Non-Normally distributed measurements\nNot all measurements are Normally distributed, and the symmetry of the bell shape may be distorted by the presence of some very small or very large values. Non-Normal distributions such as this are called skewed distributions.\nWhen there are some very large values, the distribution is said to be positively skewed.This often occurs when measuring variables related to time, such as days of hospital stay, where most patients have short stays (say 1 - 5 days) but a few patients with serious medical conditions have very long lengths of hospital stay (say 20 - 100 days).\nIn practice, most parametric summary statistics are quite robust to minor deviations from Normality and non-parametric statistical methods are only required when the sample size is small and/or the data are obviously skewed with some influential outliers.\nWhen the data are markedly skewed, density plots are not all bell-shaped. For example, serum bilirubin measured from participants in the PBC study are presented in Figure 3.6.\n\n\n\n\n\n\n\n\nFigure 3.6: Density plot of serum bilirubin from PBC study data\n\n\n\n\n\nIn the plot of Figure 3.6, there is a tail of values to the right, so we would conclude that the distribution is skewed to the right. The mean (3.2 mg/dL) is much larger than the median (1.4 mg/dL), as expected from a skewed distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#parametric-and-non-parametric-statistical-methods",
    "href": "03-precision.html#parametric-and-non-parametric-statistical-methods",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.7 Parametric and non-parametric statistical methods",
    "text": "3.7 Parametric and non-parametric statistical methods\nMany statistical methods are based on assumptions about the distribution of the variable – these methods are known as parametric statistical methods. Many methods of statistical inferences based on theoretical sampling properties that are derived from a Normal distribution with the characteristics described above. Thus, it is important that measurements approximate to a Normal distribution before these parametric methods are used. The methods are called ‘parametric’ because they are based on the parameters – the mean and standard deviation - that underlie a Normal distribution. Statistics which do not assume a particular distribution are called distribution-free statistics, or ‘non-parametric statistics’.\nIn this course, you will learn about both parametric and non-parametric statistical methods. Parametric summary statistical methods include those based on the mean, standard deviation and range (Module 1), and standard error and 95% confidence interval (Module 3). Parametric statistical tests also include t-tests which will be covered in Modules 4 and 5, and correlation and regression described in Module 8.\nNon-parametric summary statistical methods are often based on ranks, and may use such statistics as the median, mode and inter-quartile range (Module 1). Non-parametric statistical tests that use ranking are described in Module 9.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#other-types-of-probability-distributions",
    "href": "03-precision.html#other-types-of-probability-distributions",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.8 Other types of probability distributions",
    "text": "3.8 Other types of probability distributions\nIn this module we have considered a Normal probability distribution and how to use it to measure the precision of continuously distributed measurements. Data also follow other types of distributions which are briefly described below. In other modules in this course, we will be looking at a range of methods to analyse health data and will refer back to these different distributions.\nNormal approximation of binomial: When the sample size becomes large, it becomes cumbersome to calculate the exact probability of an event using the binomial distribution. Conveniently, with large sample sizes, the binomial distribution approximates a Normal distribution. The mean and SD of a binomial distribution can be used to calculate the probability of the event as though it was from a Normal distribution.\nPoisson distribution: is another distribution which is often used in health research for modelling count data. The Poisson distribution is followed when a number of events happen in a fixed time interval. This distribution is useful for describing data such as deaths in the population in a time period. For example, the number of deaths from breast cancer in one year in women over 50 years old will be an observation from a Poisson distribution. We can also use this to make comparisons of mortality rates between populations.\nMany other probability distributions can be derived for functions which arise in statistical analyses but the chi-squared, t and F distributions are the three distributions that are most widely used. These have many applications, some of which are described in later modules.\nThe chi-squared distribution is a skewed distribution which allows us to determine the probability of a deviation between a count that we observe and a count that we expect for categorical data. One use of this is in conducting statistical tests for categorical data. See Module 7.\nA t-distribution is used when the population standard deviation is not known. The t-distribution is appropriate for small samples (&lt;30) and its distribution is bell shaped similar to a Normal distribution but slightly flatter. The t-distribution is useful for comparing mean values. See Module 4 and Module 5.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#sampling-methods",
    "href": "03-precision.html#sampling-methods",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.9 Sampling methods",
    "text": "3.9 Sampling methods\nMethods have been designed to select participants from a population such that each person in the target population has an equal probability of being chosen. Methods that use this approach are called random sampling methods. Examples include simple random sampling and stratified random sampling.\nIn simple random sampling, every person in the population from which the sample is drawn has the same random chance of being selected into the sample. To implement this method, every person in the population is allocated an ID number and then a random sample of the ID numbers is selected. Software packages can be used to generate a list of random numbers to select the random sample.\nIn stratified sampling, the population is divided into distinct non-overlapping subgroups (strata) according to an important characteristic (e.g. age or sex) and then a random sample is selected from each of the strata. This method is used to ensure that sufficient numbers of people are sampled from each stratum and therefore each subgroup of interest is adequately represented in the sample.\nThe purpose of using random sampling is to minimise selection bias to ensure that the sample enrolled in a study is representative of the population being studied. This is important because the summary statistics that are obtained can then be regarded as valid in that they can be applied (generalised) back to the population.\nA non-representative sample might occur when random sampling is used, simply by chance. However, non-random sampling methods, such as using a study population that does not represent the whole population, will often result in a non-representative sample being selected so that the summary statistics from the sample cannot be generalised back to the population from which the participants were drawn. The effects of non-random error are much more serious than the effects of random error. Concepts such as non-random error (i.e. systematic bias), selection bias, validity and generalisability are discussed in more detail in PHCM9794: Foundations of Epidemiology.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#standard-error-and-precision",
    "href": "03-precision.html#standard-error-and-precision",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.10 Standard error and precision",
    "text": "3.10 Standard error and precision\nModule 1 introduced the mean, variance and standard deviation as measures of central tendency and spread for continuous measurements from a sample or a population. As described in Module 1, we rarely have data on the entire population but we infer information about the population from a sample. For example, we use the sample mean \\(\\bar x\\) as an estimate of the true population mean \\(\\mu\\).\nHowever, a sample taken from a population is usually a small proportion of the total population. If we were to take multiple samples of data and calculate the sample mean for each sample, we would not expect them to be identical. If our samples were very small, we would not be surprised if our estimated sample means were somewhat different from each other. However, if our samples were large, we would expect the sample means to be less variable, i.e. the estimated sample means would be more close to each other, and hopefully, to the true population mean.\n\nThe standard error of the mean\nA point estimate is a single best guess of the true value in the population - taken from our sample of data. Different samples will provide slightly different point estimates. The standard error is a measure of variability of the point estimate.\nIn particular, the standard error of the mean measures the extent to which we expect the means from different samples to vary because of chance due to the sampling process. This statistic is directly proportional to the standard deviation of the variable, and inversely proportional to the size of the sample. The standard error of the mean for a continuously distributed measurement for which the SD is an accurate measure of spread is computed as follows:\n\\[ \\text{SE}(\\bar{x}) = \\frac{\\text{SD}}{\\sqrt{n}} \\]\n Take for example, a set of weights of students attending a university gym in a particular hour. The thirty weights are given below:\n\n\n\n\nTable 3.1: Weight of 30 gym attendees\n\n\n\n65.070.070.067.565.080.070.072.567.562.567.572.560.065.072.577.575.075.075.070.067.577.567.562.575.062.570.075.072.570.0\n\n\n\n\n\nWe can calculate the mean (70.0kg) and the standard deviation (5.04kg). Hence, the standard error of the mean is estimated as:\n\\[ \\text{SE}(\\bar{x}) = \\frac{5.04}{\\sqrt{30}} = 0.92 \\] Because the calculation uses the sample size (n) (i.e. the number of study participants) in the denominator, the SE will become smaller when the sample size becomes larger. A smaller SE indicates that the estimated mean value is more precise.\nThe standard error is an important statistic that is related to sampling variation. When a random sample of a population is selected, it is likely to differ in some characteristic compared with another random sample selected from the same population. Also, when a sample of a population is taken, the true population mean is an unknown value.\nJust as the standard deviation measures the spread of the data around the population mean, the standard error of the mean measures the spread of the sample means. Note that we do not have different samples, only one. It is a theoretical concept which enables us to conduct various other statistical analyses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#central-limit-theorem",
    "href": "03-precision.html#central-limit-theorem",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.11 Central limit theorem",
    "text": "3.11 Central limit theorem\nEven though we now have an estimate of the mean and its standard error, we might like to know what the mean from a different random sample of the same size might be. To do this, we need to know how sample means are distributed. In determining the form of the probability distribution of the sample mean (\\(\\bar{x}\\)), we consider two cases:\n\nWhen the population distribution is unknown:\nThe central limit theorem for this situation states:\n\nIn selecting random samples of size \\(n\\) from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of the sample mean \\(\\bar{x}\\) approaches a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\tfrac{\\sigma}{\\sqrt{n}}\\) as the sample size becomes large.\n\nThe sample size n = 30 and above is a rule of thumb for the central limit theorem to be used. However, larger sample sizes may be needed if the distribution is highly skewed.\n\n\nWhen the population is assumed to be normal:\n\nIn this case the sampling distribution of \\(\\bar{x}\\) is normal for any sample size.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#confidence-interval-of-the-mean",
    "href": "03-precision.html#confidence-interval-of-the-mean",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.12 95% confidence interval of the mean",
    "text": "3.12 95% confidence interval of the mean\nEarlier, we showed that the characteristics of a Standard Normal Distribution are that 95% of the data lie within 1.96 standard deviations from the mean (Figure 3.2). Because the central limit theorem states that the sampling distribution of the mean is approximately Normal in large enough samples, we expect that 95% of the mean values would fall within 1.96 × SE units above and below the measured mean population value.\nFor example, if we repeated the study on weight 100 times using 100 different random samples from the population and calculated the mean weight for each of the 100 samples, approximately 95% of the values for the mean weight calculated for each of the 100 samples would fall within 1.96 × SE of the population mean weight.\nThis interpretation of the SE is translated into the concept of precision as a 95% confidence interval (CI). A 95% CI is a range of values within which we have 95% confidence that the true population mean lies. If an experiment was conducted a very large number of times, and a 95%CI was calculated for each experiment, 95% of the confidence intervals would contain the true population mean.\nThe calculation of the 95% CI for a mean is as follows:\n\\[  \\bar{x} \\pm 1.96 \\times \\text{SE}( \\bar{x} ) \\] This is the generic formula for calculating 95% CI for any summary statistic. In general, the mean value can be replaced by the point estimate of a rate or a proportion and the same formula applies for computing 95% CIs, i.e.\n\\[ 95\\% \\text{ CI} = \\text{point estimate} \\pm 1.96 \\times \\text{SE}(\\text{point estimate)} \\]\nThe main difference in the methods used to calculate the 95% CI for different point estimates is the way the SE is calculated. The methods for calculating 95% CI around proportions and other ratio measures will be discussed in Module 6.\nThe use of 1.96 as a general critical value to compute the 95% CI is determined by sampling theory. For the confidence interval of the mean, the critical value (1.96) is based on normal distribution (true when the population SD is known). However, in practice, statistical packages will provide slightly different confidence intervals because they use a critical value obtained from the t-distribution. The t-distribution approaches a normal distribution when the sample size approaches infinity, and is close to a normal distribution when the sample size is ≥30.The critical values obtained from the t-distribution are always larger than the corresponding critical value from the normal distribution. The difference gets smaller as the sample size becomes larger. For example, when the sample size n=10, the critical value from the t-distribution is 2.26 (rather than 1.96); when n= 30, the value is 2.05; when n=100, the value is 1.98; and when n=1000, the critical value is 1.96.\nThe critical value multiplied by SE (for normal distribution, 1.96 × SE) is called the maximum likely error for 95% confidence.\n\nThe t-distribution and when should I use it?\nThe population standard deviation (\\(\\sigma\\)) is required for calculation of the standard error. Usually, \\(\\sigma\\) is not known and the sample standard deviation (\\(s\\)) is used to estimate it. It is known, however, that the sample standard deviation of a normally distributed variable underestimates the true value of \\(\\sigma\\), particularly when the sample size is small.\nSomeone by the pseudonym of Student came up with the Student’s t distribution with (\\(n-1\\)) degrees of freedom to account for this underestimation. It looks very much like the standardised normal distribution, only that it has fatter tails (Figure 3.7). As the degrees of freedom increase (i.e. as \\(n\\) increases), the t-distribution gradually approaches the standard normal distribution. With a sufficiently large sample size, the Student’s t-distribution closely approximates the standardised normal distribution.\n\n\n\n\n\n\n\n\nFigure 3.7: The normal (Z) and the student’s t-distribution with 2, 5 and 30 degrees of freedom\n\n\n\n\n\nIf a variable \\(X\\) is normally distributed and the population standard deviation \\(\\sigma\\) is known, using the normal distribution is appropriate. However, if \\(\\sigma\\) is not known then one should use the student t-distribution with (\\(n – 1\\)) degrees of freedom.\n\n\nWorked Example 3.1: 95% CI of a mean using individual data\nThe diastolic blood pressure of 733 female Pima indigenous Americans was measured, and a density plot showed that the data were approximately normally distributed. The mean diastolic blood pressure in the sample was 72.4 mmHg with a standard deviation of 12.38 mmHg. These data are saved as mod03_blood_pressure.csv.\nUse Jamovi or R, we can calculate the mean, its Standard Error, and the 95% confidence interval:\n\n\n\nTable 3.2: Summary of blood pressure from female Pima indigenous Americans\n\n\n\n\n\n\n\n\n\n\n\n\nn\nMean\nStandard deviation\nStandard error of the mean\n95% confidence interval of the mean\n\n\n\n\n733\n72.4\n12.38\n0.46\n71.5 to 73.3\n\n\n\n\n\n\nWe can interpret this confidence interval as: we are 95% confident that the true mean of female Pima indigenous Americans lies between 71.5 and 73.3 mmHg.\n\n\nWorked Example 3.2: 95% CI of a mean using summarised data\nThe publication of a study using a sample of 242 participants reported a sample mean systolic blood pressure of 128.4 mmHg and a sample standard deviation of 19.56 mmHg. Find the 95% confidence interval for the mean systolic blood pressure.\nUsing jamovi or R, we obtain a 95% confidence interval from 125.9232 to 130.8768.\nWe are 95% confident that the true mean systolic blood pressure of the population from which the sample was drawn lies between 125.9 kg and 130.9 mmHg.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#generating-new-variables",
    "href": "03-precision.html#generating-new-variables",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.13 Generating new variables",
    "text": "3.13 Generating new variables\nWe commonly need to create new variables based on existing variables in our data. For example, body size is often summarised using the Body Mass Index (BMI). BMI is calculated as: \\(\\frac{\\text{weight (kg)}}{\\text{height (m)}^2}\\).\nIn this demonstration, we will import a selection of records from a large health survey, stored in the file mod03_health_survey.xlsx. The health survey data contains 1140 records, comprising:\n\nsex: 1 = respondent identifies as male; 2 = respondent identifies as female\nheight: height in meters\nweight: weight in kilograms\n\nTo generate a new variable, we use Data &gt; Compute:\n\nclick Data to open the spreadsheet, and click into an empty column\nclick Setup then NEW COMPUTED VARIABLE\nenter the name of the new variable, here BMI\nin the formula (*fx) box enter: weight / height^2 (note: ^2 represents “to the power of 2”, or “squared”)\n\nYou should check the construction of any new variable buy examining a density plot and/or a boxplot:\n\n\n\n\n\n\n\n\n\n\nIn the general population, BMI ranges between about 15 to 30. It appears that BMI has been correctly generated in this example, perhaps with some unsual values that might require investigation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#summarising-data-by-another-variable",
    "href": "03-precision.html#summarising-data-by-another-variable",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.14 Summarising data by another variable",
    "text": "3.14 Summarising data by another variable\nWe will often want to calculate the same summary statistics by another variable. For example, we might want to calculate summary statistics for BMI for males and females separately. We can do this in jamovi by defining sex as a by-variable.\nThis can be done easily in jamovi by defining a Split by variable. For example, to summarise BMI for males and females separately, we use sex as the Split by variable:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#sec-normal-jamovi",
    "href": "03-precision.html#sec-normal-jamovi",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.15 Computing probabilities from a Normal distribution",
    "text": "3.15 Computing probabilities from a Normal distribution\njamovi does not have a point-and-click method for computing probabilities from a Normal distribution. Here, instructions are provided for using a third-party applet. This Normal Distribution Applet has been posted at https://homepage.stat.uiowa.edu/~mbognar/applets/normal.html, and provides a simple and intuitive way to compute probabilities from a Normal distribution. The applet requires three pieces of information:\n\n\\(\\mu\\): the mean of the Normal distribution being considered\n\\(\\sigma\\): the standard deviation of the Normal distribution being considered\n\\(x\\): the value being considered\n\nWe also need to consider whether we are interested in the probability being greater than x, or less than x.\nFor example, to obtain the probability of obtaining 0.5 or greater from a standard normal (i.e. \\(/mu\\)=0, \\(/sigma\\)=1) distribution:\n\nThe Normal curve of interest is shaded, and the probability is provided as 0.30854.\nTo calculate the worked example: Assume that the mean diastolic blood pressure for men is 77.9 mmHg, with a standard deviation of 11. What is the probability that a man selected at random will have high blood pressure (i.e. diastolic blood pressure greater than or equal to 90)?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#sec-cimean-jamovi-ind",
    "href": "03-precision.html#sec-cimean-jamovi-ind",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.16 Calculating a 95% confidence interval of a mean: Individual data",
    "text": "3.16 Calculating a 95% confidence interval of a mean: Individual data\nTo demonstrate the computation of the 95% confidence interval of a mean, we can use the data from mod03_blood_pressure.csv. We can use Exploration &gt; Descriptives to calculate the mean, its standard error and the 95% confidence interval for the mean. Choose dbp as the analysis variable, and select Std. error of Mean and Confidence interval for Mean in the Statistics section:\n\nThe descriptives output appears:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#sec-cimean-jamovi-summ",
    "href": "03-precision.html#sec-cimean-jamovi-summ",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.17 Calculating a 95% confidence interval of a mean: Summarised data",
    "text": "3.17 Calculating a 95% confidence interval of a mean: Summarised data\nFor Worked Example 3.2 where we are given the sample mean, sample standard deviation and sample size, we need to install a new Jamovi module, called esci. To install a new module, click the large + Modules button on the right-hand side of the Jamovi window, and then choose jamovi library:\n\nTo install a new module:\n1 - Ensure that the middle tab, Available is selected; 2 - Type esci in the search bar. The esci module will appear; 3 - Click INSTALL to install the module 4 - Click the up-arrow to exit from the Install Module window\n\nTo calculate the 95% confidence interval, choose esci &gt; Means and Medians &gt; Single Group. Select the Analyze summary data tab, and enter the known information: here 128.4 as the Mean, 19.56 as the Standard deviation and 242 as the Sample size. Choose Extra details to obtain the Standard Error of the mean:\n\nThe 95% confidence interval is listed as the lower limit (LL) and the upper limit (UL):",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#importing-excel-data-into-r",
    "href": "03-precision.html#importing-excel-data-into-r",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.18 Importing Excel data into R",
    "text": "3.18 Importing Excel data into R\nAnother common type of file that data are stored in is a Microsoft Excel file (.xls or .xlsx). In this demonstration, we will import a selection of records from a large health survey, stored in the file mod03_health_survey.xlsx.\nThe health survey data contains 1140 records, comprising:\n\nsex: 1 = respondent identifies as male; 2 = respondent identifies as female\nheight: height in meters\nweight: weight in kilograms\n\nTo import data from Microsoft Excel, we can use the read_excel() function in the readxl package.\n\nlibrary(readxl)\n\nsurvey &lt;- read_excel(\"data/examples/mod03_health_survey.xlsx\")\nsummary(survey)\n\n      sex           height          weight      \n Min.   :1.00   Min.   :1.220   Min.   : 22.70  \n 1st Qu.:1.00   1st Qu.:1.630   1st Qu.: 68.00  \n Median :2.00   Median :1.700   Median : 79.40  \n Mean   :1.55   Mean   :1.698   Mean   : 81.19  \n 3rd Qu.:2.00   3rd Qu.:1.780   3rd Qu.: 90.70  \n Max.   :2.00   Max.   :2.010   Max.   :213.20  \n\n\nWe can see that sex has been entered as a numeric variable. We should transform it into a factor so that we can assign labels to each category:\n\nsurvey$sex &lt;- factor(survey$sex, level=c(1,2), labels=c(\"Male\", \"Female\"))\n\nsummary(survey$sex)\n\n  Male Female \n   513    627 \n\n\nWe also note that height looks like it has been entered as meters, and weight as kilograms.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#generating-new-variables-1",
    "href": "03-precision.html#generating-new-variables-1",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.19 Generating new variables",
    "text": "3.19 Generating new variables\nOur health survey data contains information on height and weight. We often summarise body size using BMI: body mass index which is calculated as: \\(\\frac{\\text{weight (kg)}}{(\\text{height (m)})^2}\\)\nWe can create a new column in our data frame in many ways, such as using the following approach:\ndataframe$new_column &lt;- &lt;formula&gt;\nFor example:\n\nsurvey$bmi &lt;- survey$weight / (survey$height^2)\n\nWe should check the construction of the new variable by examining some records. The head() and tail() functions list the first and last 6 records in any dataset.\n\nhead(survey)\n\n# A tibble: 6 × 4\n  sex    height weight   bmi\n  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Male     1.63   81.7  30.8\n2 Male     1.63   68    25.6\n3 Male     1.85   97.1  28.4\n4 Male     1.78   89.8  28.3\n5 Male     1.73   70.3  23.5\n6 Female   1.57   85.7  34.8\n\ntail(survey)\n\n# A tibble: 6 × 4\n  sex    height weight   bmi\n  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Female   1.65   95.7  35.2\n2 Male     1.8    79.4  24.5\n3 Female   1.73   83    27.7\n4 Female   1.57   61.2  24.8\n5 Male     1.7    73    25.3\n6 Female   1.55   91.2  38.0\n\n\nWe should also check the construction of any new variable buy examining a density plot and/or a boxplot:\ndescriptives(data=survey, vars=bmi, dens=TRUE, box=TRUE)\n\n\n\n\n\n\n\n\n\n\nIn the general population, BMI ranges between about 15 to 30. It appears that BMI has been correctly generated in this example. We should investigate the very low and some of the very high values of BMI, but this will be left for another time.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#summarising-data-by-another-variable-1",
    "href": "03-precision.html#summarising-data-by-another-variable-1",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.20 Summarising data by another variable",
    "text": "3.20 Summarising data by another variable\nWe will often want to calculate the same summary statistics by another variable. For example, we might want to calculate summary statistics for BMI for males and females separately. We can do this in in the descriptives function by defining sex as a splitBy variable:\n\nlibrary(jmv)\ndescriptives(data=survey, vars=bmi, splitBy = sex, dens=TRUE, box=TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                 \n ──────────────────────────────────────────── \n                         sex       bmi        \n ──────────────────────────────────────────── \n   N                     Male           513   \n                         Female         627   \n   Missing               Male             0   \n                         Female           0   \n   Mean                  Male      28.29561   \n                         Female    27.81434   \n   Median                Male      27.39592   \n                         Female    26.66667   \n   Standard deviation    Male      5.204975   \n                         Female    6.380523   \n   Minimum               Male      16.47519   \n                         Female    9.209299   \n   Maximum               Male      57.23644   \n                         Female    52.59516   \n ────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#summarising-a-single-column-of-data",
    "href": "03-precision.html#summarising-a-single-column-of-data",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.21 Summarising a single column of data",
    "text": "3.21 Summarising a single column of data\nIn Module 1, we started with a very simple analysis: reading in six ages, and them using summary() to calculate descriptive statistics. We then went on to use the decriptives() function in the jmv package as more flexible way of calculating descriptive statistics. Let’s revisit this analysis:\n\n# Author: Timothy Dobbins\n# Date: 5 April 2022\n# Purpose: My first R script\nlibrary(jmv)\n\nage &lt;- c(20, 25, 23, 29, 21, 27)\n\n# Use \"summary\" to obtain descriptive statistics\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  20.00   21.50   24.00   24.17   26.50   29.00 \n\n# Use the \"descriptives\" function from jmv to obtain descriptive statistics\ndescriptives(age)\n\nError: Argument 'data' must be a data frame\n\n\nThe summary() function has worked correctly, but the descriptives() function has given an error: Error: Argument 'data' must be a data frame. What on earth is going on here?\nThe error gives us a clue here - the descriptives() function requires a data frame for analysis. We have provided the object age: a vector. As we saw in Section 1.12.6.3, a vector is a single column of data, while a data frame is a collection of columns.\nIn order to summarise a vector using the descriptives() function, we must first convert the vector into a data frame using as.data.frame(). For example:\n\n# Author: Timothy Dobbins\n# Date: 5 April 2024\n# Purpose: My first R script\nlibrary(jmv)\n\nage &lt;- c(20, 25, 23, 29, 21, 27)\n\n# Use \"summary\" to obtain descriptive statistics\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  20.00   21.50   24.00   24.17   26.50   29.00 \n\n# Create a new data frame from the vector age:\n\nage_df &lt;- as.data.frame(age)\n\n# Use \"descriptives\" to obtain descriptive statistics for age_df\ndescriptives(age_df)\n\n\n DESCRIPTIVES\n\n Descriptives                       \n ────────────────────────────────── \n                         age        \n ────────────────────────────────── \n   N                            6   \n   Missing                      0   \n   Mean                  24.16667   \n   Median                24.00000   \n   Standard deviation    3.488075   \n   Minimum               20.00000   \n   Maximum               29.00000   \n ──────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#sec-normal-r",
    "href": "03-precision.html#sec-normal-r",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.22 Computing probabilities from a Normal distribution",
    "text": "3.22 Computing probabilities from a Normal distribution\nWe can use the pnorm function to calculate probabilities from a Normal distribution:\n\npnorm(q, mean, sd) calculates the probability of observing a value of q or less, from a Normal distribution with a mean of mean and a standard deviation of sd. Note that if mean and sd are not entered, they are assumed to be 0 and 1 respectively (i.e. a standard normal distribution.)\npnorm(q, mean, sd, lower.tail=FALSE) calculates the probability of observing a value of more than q, from a Normal distribution with a mean of mean and a standard deviation of sd.\n\nTo obtain the probability of obtaining 0.5 or greater from a standard normal distribution:\n\npnorm(0.5, lower.tail = FALSE)\n\n[1] 0.3085375\n\n\nTo calculate the worked example: Assume that the mean diastolic blood pressure for men is 77.9 mmHg, with a standard deviation of 11. What is the probability that a man selected at random will have high blood pressure (i.e. diastolic blood pressure greater than or equal to 90)?\n\npnorm(90, mean = 77.9, sd = 11, lower.tail = FALSE)\n\n[1] 0.1356661",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#sec-cimean-r-ind",
    "href": "03-precision.html#sec-cimean-r-ind",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.23 Calculating a 95% confidence interval of a mean: individual data",
    "text": "3.23 Calculating a 95% confidence interval of a mean: individual data\nTo demonstrate the computation of the 95% confidence interval of a mean, we can use the data from mod03_blood_pressure.csv:\n\npima &lt;- read.csv(\"data/examples/mod03_blood_pressure.csv\")\n\nWe can examine the data set using the summary command:\n\nsummary(pima)\n\n      dbp        \n Min.   : 24.00  \n 1st Qu.: 64.00  \n Median : 72.00  \n Mean   : 72.41  \n 3rd Qu.: 80.00  \n Max.   :122.00  \n\n\nThe mean and its 95% confidence interval can be obtained many ways in R. We will use the descriptives() function within the jmv package to calculate the standard error of the mean, and a confidence interval, by including se = TRUE and ci = TRUE:\n\nlibrary(jmv)\ndescriptives(data=pima, vars=dbp, se=TRUE, ci=TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                             \n ──────────────────────────────────────── \n                              dbp         \n ──────────────────────────────────────── \n   N                                733   \n   Missing                            0   \n   Mean                        72.40518   \n   Std. error mean            0.4573454   \n   95% CI mean lower bound     71.50732   \n   95% CI mean upper bound     73.30305   \n   Median                            72   \n   Standard deviation          12.38216   \n   Minimum                           24   \n   Maximum                          122   \n ──────────────────────────────────────── \n   Note. The CI of the mean assumes\n   sample means follow a\n   t-distribution with N - 1 degrees\n   of freedom",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#sec-cimean-r-summ",
    "href": "03-precision.html#sec-cimean-r-summ",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "3.24 Calculating a 95% confidence interval of a mean: summarised data",
    "text": "3.24 Calculating a 95% confidence interval of a mean: summarised data\nFor Worked Example 3.2 where we are given the sample mean, sample standard deviation and sample size. R does not have a built-in function to calculate a confidence interval from summarised data, but we can write our own.\nNote: writing your own functions is beyond the scope of this course. You should copy and paste the code provided to do this.\n\n### Copy this section\nci_mean &lt;- function(n, mean, sd, width=0.95, digits=3){\n  lcl &lt;- mean - qt(p=(1 - (1-width)/2), df=n-1) * sd/sqrt(n)\n  ucl &lt;- mean + qt(p=(1 - (1-width)/2), df=n-1) * sd/sqrt(n)\n  \n  print(paste0(width*100, \"%\", \" CI: \", format(round(lcl, digits=digits), nsmall = digits),\n               \" to \", format(round(ucl, digits=digits), nsmall = digits) ))\n\n}\n### End of copy\n\nci_mean(n=242, mean=128.4, sd=19.56, width=0.95)\n\n[1] \"95% CI: 125.923 to 130.877\"\n\nci_mean(n=242, mean=128.4, sd=19.56, width=0.99)\n\n[1] \"99% CI: 125.135 to 131.665\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "03-precision.html#footnotes",
    "href": "03-precision.html#footnotes",
    "title": "3  Continuous probability distributions, sampling and precision",
    "section": "",
    "text": "Source: https://www.aihw.gov.au/reports/risk-factors/high-blood-pressure/contents/high-blood-pressure (accessed March 2021)↩︎\nSource: https://ourworldindata.org/human-height (accessed March 2021)↩︎\nhttps://www.aihw.gov.au/reports/overweight-obesity/overweight-and-obesity/contents/measuring-overweight-and-obesity↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous probability distributions, sampling and precision</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html",
    "href": "04-hypothesis-testing.html",
    "title": "4  An introduction to hypothesis testing",
    "section": "",
    "text": "Learning objectives\nBy the end of this module you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#learning-objectives",
    "href": "04-hypothesis-testing.html#learning-objectives",
    "title": "4  An introduction to hypothesis testing",
    "section": "",
    "text": "Formulate a research question as a hypothesis;\nUnderstand the concepts of a hypothesis test;\nConsider the difference between statistical significance and clinical importance;\nUse 95% confidence intervals to conduct an informal hypothesis test;\nPerform and interpret a one-sample t-test;\nExplain the concept of one and two tailed statistical tests.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#optional-readings",
    "href": "04-hypothesis-testing.html#optional-readings",
    "title": "4  An introduction to hypothesis testing",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapter 8. [UNSW Library Link]\nBland (2015); Sections 9.1 to 9.7; Sections 10.1 and 10.2. [UNSW Library Link]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#introduction",
    "href": "04-hypothesis-testing.html#introduction",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn earlier modules, we examined sampling and how summary statistics can be used to make inferences about a population from which a sample is drawn. In this module, we introduce hypothesis testing as the basis of the statistical tests that are important for reporting results from research and surveillance studies, and that you will be learning in the remainder of this course.\nWe use hypothesis testing to answer questions such as whether two groups have different health outcomes or whether there is an association between a treatment and a health outcome. For example, we may want to know:\n\nwhether a safety program has been effective in reducing injuries in a factory, i.e. whether the frequency of injuries in the group who attended a safety program is lower than in the group who did not receive the safety program;\nwhether a new drug is more effective in reducing blood pressure than a conventional drug, i.e. whether the mean blood pressure in the group receiving the new drug is lower than the mean blood pressure in the group receiving the conventional medication;\nwhether an environmental exposure increases the risk of a disease, i.e. whether the frequency of disease is higher in the group who have been exposed to an environmental factor than in the non-exposed group.\n\nWe may also want to know something about a single group. For example, whether the mean blood pressure of a sample is the same as the general population.\nThese questions can be answered by setting up a null hypothesis and an alternative hypothesis, and performing a hypothesis test (also known as a significance test).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#hypothesis-testing",
    "href": "04-hypothesis-testing.html#hypothesis-testing",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.2 Hypothesis testing",
    "text": "4.2 Hypothesis testing\nHypothesis testing is a statistical technique that is used to quantify the evidence against a null hypothesis. A null hypothesis (H0) is a statement that there is no difference in a summary statistic between groups. For example, a null hypothesis may be stated as follows:\nH0 = there is no difference in mean systolic blood pressure between a group taking a conventional drug and a group taking a newly developed drug\nWe also have an alternative hypothesis that is opposite or contrasting to the null hypothesis. In our example above, the alternative hypothesis for the above null hypothesis is that there is a difference between groups. The alternative hypothesis is usually of most interest to the researcher but in practice, formal statistical tests are used to test the null hypothesis (not the alternative hypothesis). The hypotheses are always in reference to the population from which the sample is drawn, not the sample itself.\nAfter setting up our null and alternative hypotheses, we use the data to generate a test statistic. The particular test statistic differs depending on the type of data being analysed (e.g. continuous or categorical), the study design (e.g. paired or independent) and the question being asked.\nThe test statistic is then compared to a known distribution to calculate the probability of observing a test statistic which is as large or larger than the observed test statistic, if the null hypothesis was true. The probability is known as the P-value.\nInformally, the P-value can be interpreted as the probability of observing data like ours, or more extreme, if the null hypothesis was true.\nIf the P-value is small, it is unlikely that we would observe data like ours or more extreme if the null hypothesis was true. In other words, our data are not consistent with the null hypothesis, and we conclude that we have evidence against the null hypothesis. If the P-value is not small, the probability of observing data like ours or more extreme is not unlikely. We therefore have little or no evidence against the null hypothesis. In hypothesis testing, the null hypothesis cannot be proven or accepted; we can only find evidence to refute the null hypothesis.\nTo summarise:\n\na small P-value gives us evidence against the null hypothesis;\na P-value that is not small provides little or no evidence against null hypothesis;\nthe smaller the P-value, the stronger the evidence against the null hypothesis.\n\nHistorically, a value of 0.05 has been used as a cut-point for finding evidence against the null hypothesis. A P-value less than 0.05 would be interpreted as “statistically significant”, and would allow us to “reject the null hypothesis”. A P-value greater than 0.05 would be interpreted as “not significant”, and we would “fail to reject the null hypothesis”. This arbitrary dichotomy is overly simplistic, and a more nuanced view is now recommended. Recommended interpretations for P-values are given in Table 4.1.\n\n\n\n\nTable 4.1: Interpretation of P-values\n\n\n\nP-valueStrength of evidence&lt;0.001Very strong evidence0.001 to &lt;0.01Strong evidence0.01 to &lt;0.05Evidence0.05 to &lt;0.1Weak evidence≥0.1Little or no evidence\n\n\n\n\n\nP-values are usually generated using statistical software although other methods such as statistical tables or Excel functions can be used to generate test statistics and determine the P-value. In traditional statistics, the probability level was described as a lower-case p but in many journals today, probability is commonly described by upper case P. Both have the same meaning.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#effect-size",
    "href": "04-hypothesis-testing.html#effect-size",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.3 Effect size",
    "text": "4.3 Effect size\nIn hypothesis testing, P-values convey only part of the information about the hypothesis and need to be accompanied by an estimation of the effect size, that is, a description of the magnitude of the difference between the study groups. The effect size is a summary statistic that conveys the size of the difference between two groups. For continuous variables, it is usually calculated as the difference between two mean values.\nIf the variable is binary, the effect size can be expressed as the absolute difference between two proportions (attributable risk), or as an odds ratio or relative risk.\nReporting the effect size enables clinicians and other researchers to judge whether a statistically significant result is also a clinically important finding. The size of the difference or the risk statistic provides information to help health professionals decide whether the observed effect is large and important enough to warrant a change in current health care practice, is equivocal and suggests a need for further research, or is small and clinically unimportant.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#statistical-significance-and-clinical-importance",
    "href": "04-hypothesis-testing.html#statistical-significance-and-clinical-importance",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.4 Statistical significance and clinical importance",
    "text": "4.4 Statistical significance and clinical importance\nWhen applying statistical methods in health and medical research, we need to make an informed decision about whether the effect size that led to a statistically significant finding is also clinically important (see Figure 4.1)). The decision about whether a statistically significant result is also clinically important depends on expert knowledge and is best made by practitioners with experience in the field.\n\n\n\n\n\n\nFigure 4.1: Statistical significance vs. clinical importance (Source: Armitage P, Berry G, Matthews JNS. (2001)\n\n\n\nIt is possible when conducting significance tests, particularly in very large studies, that a small effect is found to be statistically significant. For example, say in a large study of over 1000 patients, a new medication was found to lower blood pressure on average by 1 mmHg more than a currently accepted drug and this was statistically significant (P &lt; 0.05). However, such a small decrease in blood pressure would probably not be considered clinically important. The cost and side effects of prescribing the new medication would need to be weighed against the very small average benefit that would be expected. In this case, although the null hypothesis would be rejected (i.e. the result is statistically significant), the result would not be clinically important. This is the situation described in scenario (c) of Figure 4.1.\nConversely, it is possible to obtain a large, clinically important difference between groups, but a P value that does not demonstrate a statistically significant difference.\nFor example, consider a study to measure the rate of hospital admissions. We may find that 80% of children who present to the Emergency Department are admitted before an intervention is introduced compared to only 65% of children after the intervention. However, the P value may be calculated as 0.11 and is non-significant. This is because only 60 children were surveyed in each period. Here, the reduction in the admission rate by 15% represents a clinically important difference, but not statistically significant. This situation is represented in scenario (d) of Figure 4.1.\nThe important thing to remember is that statistical significance does not always correspond to practical importance. A statistically significant result may be practically unimportant, and a statistically non-significant results may be practically important.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#errors-in-significance-testing",
    "href": "04-hypothesis-testing.html#errors-in-significance-testing",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.5 Errors in significance testing",
    "text": "4.5 Errors in significance testing\nThere are two conclusions we can draw when conducting a hypothesis test: if the P-value is small, there is strong evidence against the null hypothesis and we reject the null hypothesis. If the P-value is not small, there is little evidence against the null hypothesis and we fail to reject the null hypothesis. As discussed above, the “small” cut-point for the P-value is often taken as 0.05. We refer to this value as \\(\\alpha\\) (alpha).\nWe can conduct a thought experiment and compare our hypothesis test conclusion to reality. In reality, either the null hypothesis is true, or it is false. Of course, if we knew what reality was, we would not need to conduct a hypothesis test. But we can compare our possible hypothesis test conclusions to the true (unobserved) reality.\nIf the null hypothesis was true in reality, our hypothesis test can fail to reject the null hypothesis – this would be a correct conclusion. However, the hypothesis test could lead us to rejecting the null hypothesis – this would be an incorrect conclusion. We call this scenario a Type I error, and it has a probability of \\(\\alpha\\).\nThe other situation is where, in reality, the null hypothesis is false. A correct conclusion would be where our hypothesis test rejects the null hypothesis. However, if our hypothesis test fails to reject the null hypothesis, we have made a Type II error. The probability of making a Type II error is denoted \\(\\beta\\) (beta). We will see in Module 10 that \\(\\beta\\) is determined by the size of the study.\nThe error in falsely rejecting the null hypothesis when it is true (type I error), or in falsely accepting the null hypothesis when it is not true (type II error) is summarised in Table 4.2. We will return to these concepts in Module 10, when discussing how to determine the appropriate sample size of a study.\n\n\n\n\nTable 4.2: Comparison of study result with the truth\n\n\n\n EffectNo effectEvidenceCorrectɑNo evidenceβCorrect",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#confidence-intervals-in-hypothesis-testing",
    "href": "04-hypothesis-testing.html#confidence-intervals-in-hypothesis-testing",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.6 Confidence intervals in hypothesis testing",
    "text": "4.6 Confidence intervals in hypothesis testing\nIn Module 3, the 95% confidence interval around a mean value was calculated to show the precision of the summary statistic. The 95% confidence intervals around other summary statistics can also be calculated.\nFor example, if we were comparing the means of two groups, we would want to test the null hypothesis that the difference in means is zero, that there is no true difference between the groups.\nFrom the data from the two groups, we could estimate the difference in means, the standard error of the difference in means and the 95% confidence interval around the difference. To estimate the 95% confidence interval, we use the formula given in Module 3, that is:\n\\[ 95\\% \\text{ CI} = \\text{Difference in means} \\pm 1.96 \\times \\text{SE}(\\text{Difference in means)} \\]\nIt is important to remember that the 95% CI is estimated from the standard error, and that the standard error has a direct relationship to the sample size. For small sample sizes, the standard error is large and the 95% CI becomes wider. Conversely, the larger the sample size, the smaller the standard error and the narrower the 95% CI becomes indicating a more precise estimate of the mean difference.\nThe 95% CI tells us the region in which we are 95% confident that the true difference between the groups in the population lies. If this region contains the null value of no difference, we can say that we are 95% confident that there is no true difference between the groups and therefore we would not reject the null hypothesis. This is shown in the top two estimates in Figure 4.2. If the zero value lies outside the 95% confidence interval, we can conclude that there is evidence of a difference between the groups because we are 95% confident that the difference does not encompass a zero value (as shown in the lower two estimates in Figure 4.2.\n\n\n\n\n\n\n\n\nFigure 4.2: Using confidence intervals as informal hypothesis tests\n\n\n\n\n\nFor relative risk and odds ratio measures, when the 95% CI includes the value of 1 it indicates that we can be 95% confident that the true RR or OR of the association between the study factor and outcome factor includes 1.0 in the source population. This indicates little evidence of an association between the study factor and the outcome factor, e.g. if the results of a study were reported as RR = 1.10 (95% CI 0.95 to 1.25). The P-value can be calculated to assess this (discussed in Module 7).\n\n\n\n\nTable 4.3: Values indicating no effect\n\n\n\nType of outcomeMeasure of effectNull valueContinuousDifference in means0BinaryDifference in proportions0BinaryRelative risk1BinaryOdds ratio1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#one-sample-t-test",
    "href": "04-hypothesis-testing.html#one-sample-t-test",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.7 One-sample t-test",
    "text": "4.7 One-sample t-test\nA one-sample t-test tests whether a sample mean is different to a hypothesised value. The t-distribution and its relation to normal distribution has been discussed in detailed in Module 3.\nIn a one-sample t-test, a t-value is computed as the sample mean divided by the standard error of the mean. The significance of the t-value is then computed using software, or can be obtained from a statistical table.\nThe principles of this test can be used for applications such as testing whether the mean of a sample is different from a known population mean, for example testing whether the IQ of a group of children is different from the population mean of 100 IQ points or testing whether the number of average hours worked in an adult sample is different from the population mean of 38 hours.\n\nWorked Example\nThe mean diastolic blood pressure (BP) of the general US population is known to be 71 mm Hg. The diastolic blood pressure of 733 female Pima indigenous Americans was measured and a density plot showed that the data were approximately normally distributed. The mean diastolic blood pressure in the sample was 72.4 mm Hg with a standard deviation of 12.38 mm Hg.\nWe can use jamovi or R to conduct a one sample t-test using the data available on Moodle (mod04_blood_pressure.csv). The results from this test are summarised below.\n\n\n\nTable 4.4: Summary of blood pressure from female Pima indigenous Americans\n\n\n\n\n\n\n\n\n\n\n\n\nn\nMean\nStandard deviation\nStandard error\n95% confidence interval of the mean\n\n\n\n\n733\n72.4\n12.38\n0.46\n71.5 to 73.3\n\n\n\n\n\n\nThe test statistic for the one-sample t-test is calculated as t732=3.07, with a P-value of 0.002.\nThe mean diastolic blood pressure of females from Pima is estimated as 72.4 mmHg (95% CI: 71.5 to 73.3 mmHg), which is higher than that of the general US population. Note that this interval does not contain the mean of the general US population (71 mm Hg), providing some indication that the mean diastolic blood pressure of female Pima people is higher than that of the general US population.\nThe result from the formal hypothesis test gives strong evidence that the mean diastolic BP of the female Pima people is higher than that of the general US population (t732=3.07, P=0.002).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#one-and-two-tailed-tests",
    "href": "04-hypothesis-testing.html#one-and-two-tailed-tests",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.8 One and two tailed tests",
    "text": "4.8 One and two tailed tests\nMost statistical tests are two tailed tests, that is, we conduct a test that allows for the summary statistic in the group of interest to be either higher or lower than in the comparison group. For a t-test, this requires that we obtain a two-tailed P value which gives us the probability of the t-value being in either one of the two tails of the t-distribution as shown in Figure 4.3. The shaded regions show the t values that indicate a P value less than 0.05.\n\n\n\n\n\n\n\n\nFigure 4.3: P-value for a 2-tailed test\n\n\n\n\n\nOccasionally, one tailed tests are conducted in which the summary statistic in the group of interest can only be higher or lower than the comparison group, i.e. a difference is specified to occur in one direction only. In most studies, two tailed tests of significance are used to allow for the possibility that the effect size could occur in either direction. In clinical trials, this would mean allowing for a result that can indicate a benefit or an adverse effect in response to a new treatment. In epidemiological studies, two tailed tests are used to allow for the fact that exposure to a factor of interest may be adverse or may be beneficial. This conservative approach is usually adopted to prevent missing important effects that occur in the opposite direction to that expected by the researchers.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#a-note-on-p-values-displayed-by-software",
    "href": "04-hypothesis-testing.html#a-note-on-p-values-displayed-by-software",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.9 A note on P-values displayed by software",
    "text": "4.9 A note on P-values displayed by software\nYou will often see P-values generated by statistical software presented as 0.000 or 0.0000. As P-values can never be equal to zero, any P-value displayed in this way should be converted to &lt;0.001 or &lt;0.0001 respectively (i.e. replace the last 0 with a 1, and use the less-than symbol).\nR can display P-values in a very cryptic way: \\(\\text{6.478546e-05}\\) for example. This is translated as:\n\\[\n\\begin{aligned}\n6.478546e-05 &= 6.478546 \\times 10^{-5} \\\\\n  &= 6.478546 \\times 0.00001 \\\\\n  &= 0.00006478546\n\\end{aligned}\n\\]\nSuch a P-value should be presented as P&lt;0.0001.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#decision-tree",
    "href": "04-hypothesis-testing.html#decision-tree",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.10 Decision Tree",
    "text": "4.10 Decision Tree\nIn the following modules in this course, several formal statistical tests will be described to analyse different types of data sets that have been collected to test set null hypotheses. It is important that the correct statistical test is selected to generate P-values and estimate effect size. If an incorrect statistical test is used, the assumptions of the test may be violated, the effect size may be biased and the P value generated may be incorrect.\nSelecting the correct test to use in each situation depends on the study design and the nature of the variables collected. Figure 1 in the Appendix shows a decision tree which enables you to decide the type of test to select based on the nature of the data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#one-sample-t-test-1",
    "href": "04-hypothesis-testing.html#one-sample-t-test-1",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.11 One sample t-test",
    "text": "4.11 One sample t-test\nWe will use data from mod04_blood_pressure.csv to demonstrate how a one-sample t-test is conducted in Jamovi. To perform the test, go to Analyses &gt; T-Tests &gt; One Sample T-Test.\nSelect dbp as the Dependent Variable. Enter the hypothesised mean as the Test value (71 in this example) as shown below.\n\n\n\n\n\n\n\n\n\nThe test statistic (i.e. t and its degrees of freedom) are provided. By default, Jamovi will calculate a P-value for the two-sided test (Ha: mean \\(\\neq\\) 71), which appropriate for this example.\nNote that the one-sample t-test output does not include the mean or the 95% confidence interval of the mean of your variable of interest. This should be obtained using Exploration &gt; Descriptives:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis-testing.html#one-sample-t-test-2",
    "href": "04-hypothesis-testing.html#one-sample-t-test-2",
    "title": "4  An introduction to hypothesis testing",
    "section": "4.12 One sample t-test",
    "text": "4.12 One sample t-test\nWe will use data from mod04_blood_pressure.csv to demonstrate how a one-sample t-test is conducted in R. To test whether the mean diastolic blood pressure of the population from which the sample was drawn is equal to 71, we can use the t.test command:\n\npima &lt;- read.csv(\"data/examples/mod04_blood_pressure.csv\")\nt.test(pima$dbp, mu=71)\n\n\n    One Sample t-test\n\ndata:  pima$dbp\nt = 3.0725, df = 732, p-value = 0.002202\nalternative hypothesis: true mean is not equal to 71\n95 percent confidence interval:\n 71.50732 73.30305\nsample estimates:\nmean of x \n 72.40518 \n\n\nThe output provides:\n\na test statistic (t=3.07);\ndegrees of freedom for the test statistic (df = 732);\na P-value from the two-sided test (P=0.002);\nthe mean of the sample (72.4);\nand the 95% confidence interval of the mean (71.5 to 73.3).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html",
    "href": "05-comparing-two-means.html",
    "title": "5  Comparing the means of two groups",
    "section": "",
    "text": "Learning objectives\nBy the end of this module you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#learning-objectives",
    "href": "05-comparing-two-means.html#learning-objectives",
    "title": "5  Comparing the means of two groups",
    "section": "",
    "text": "Decide whether to use an independent samples t-test or a paired t-test to compare two the means of two groups;\nConduct and interpret the results from an independent samples t-test;\nDescribe the assumptions of an independent samples t-test;\nConduct and interpret the results from a paired t-test;\nDescribe the assumptions of a paired t-test;\nConduct an independent samples t-test and a paired t-test using software;\nReport results and provide a concise summary of the findings of statistical analyses.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#optional-readings",
    "href": "05-comparing-two-means.html#optional-readings",
    "title": "5  Comparing the means of two groups",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Sections 7.1 to 7.5. [UNSW Library Link]\nBland (2015); Section 10.3. [UNSW Library Link]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#introduction",
    "href": "05-comparing-two-means.html#introduction",
    "title": "5  Comparing the means of two groups",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn Module 4, a one-sample t-test was used for comparing a single mean to a hypothesised value. In health research, we often want to compare the means between two groups. For example, in an observational study, we may want to compare cholesterol levels in people who exercise regularly to the levels in people who do not exercise regularly. In a clinical trial, we may want to compare cholesterol levels in people who have been randomised to a dietary modification or to usual care. In this module, we show how to compare the means of two groups where the analysis variable is normally distributed.\nFrom the decision tree presented in the Appendix, we can see that if we have a continuous outcome measure and two categorical groups that are not related, i.e. a binary exposure measurement, the test for such data is an independent samples t-test. The test is also sometimes called a 2-sample t-test.\nIn research, data are often ‘paired’ or ‘matched’, that is the two data points are related to one another. This occurs when measurements are taken:\n\nFrom each participant on two occasions, e.g. at baseline and follow-up in an experimental study or in a longitudinal cohort study;\nFrom related people, e.g. a mother and daughter or a child and their sibling;\nFrom related sites in the same person, e.g. from both limbs, eyes or kidneys;\nFrom matched participants e.g. in a matched case-control study;\nIn cross-over clinical trials where the patient receives both drugs, often in random order.\n\nAn independent samples t-test cannot be used for analysing paired or matched data because the assumption that the two groups are independent is violated. Treating paired or matched measurements as independent samples would artificially inflate the sample size and lead to inaccurate P values. When the data are related in a paired or matched way and the outcome is continuous, a paired t-test is the appropriate statistic to use if the data are normally distributed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#independent-samples-t-test",
    "href": "05-comparing-two-means.html#independent-samples-t-test",
    "title": "5  Comparing the means of two groups",
    "section": "5.2 Independent samples t-test",
    "text": "5.2 Independent samples t-test\nAn independent samples t-test is a parametric test that is used to assess whether the mean values of two groups are different from one another. Thus, the test is used to assess whether two mean values are similar enough to have come from the same population or whether the difference between them is so large that the two groups can be considered to have come from separate populations with different characteristics.\nThe null hypothesis is that the mean values of the two groups are not different, that is:\nH0: (\\(\\mu_1 - \\mu_2\\)) = 0\nRejecting the null hypothesis using an independent samples t-test indicates that the difference between the means of the two groups is large in relation to the variability in the samples and is unlikely to be due to chance or to sampling variation.\n\nAssumptions for an independent samples t-test\nThe assumptions that must be met before an independent samples t-test can be used are:\n\nThe two groups are independent\nThe measurements are independent\nThe analysis variable must be continuous and must be normally distributed in each group\n\nThe first two assumptions are determined by the study design. The two samples must be independent, i.e. if a person is in one group then they cannot be included in the other group, and the measurements within a sample must be independent, i.e. each person must be included in their group once only.\nThe third assumption of normality is important although t-tests are robust to some degree of non-normality as long as there are no influential outliers and, more importantly, if the sample size is large. We examined how to assess normality in Module 2. If the data are not normally distributed, it may be possible to transform them using a mathematical function such as a logarithmic transformation. If not, then we may need to use non-parametric tests. This is examined in Module 9.\nTraditionally, the variance of the analysis variable in each group was assumed to be equal. However, this assumption can be relaxed by using Welch’s variation of the t-test. It has been recommended that this unequal-variances t-test be used in most, if not all situations (West 2021; Delacre, Lakens, and Leys 2017; Ruxton 2006).\n\n\nWorked Example 5.1\nIn an observational study of a random sample of 100 full term babies from the community, birth weight and gender were measured. There were 44 male babies and 56 female babies in the sample. The research question asked whether there was a difference in birth weights between boys and girls. The two groups are independent of each other and therefore an independent samples t-test can be used to test the null hypothesis that there is no difference in weight between the genders.\nExploratory data analysis of the variable of interest in each group should always be obtained before a t-test is undertaken to ensure that the assumptions are met. In particular, the distribution of the analysis variable should be examined for each group, as shown in Figure 5.1. The mod05_birthweight.rds dataset is available on Moodle.\n\n\n\n\n\n\n\n\nFigure 5.1: Distribution of birth weight by gender\n\n\n\n\n\nThe plots show that the data are approximately normally distributed: the density curves are relatively bell shaped and symmetric, and there are no outliers.\nWe can also describe the data using summary statistics:\n\n\n\n\nTable 5.1: Summary of birthweight by gender\n\n\n\nCharacteristicFemaleMaleBirthweightNumber5644Mean (SD)3.59 (0.36)3.42 (0.35)Median (Q1, Q3)3.53 (3.32, 3.88)3.43 (3.15, 3.63)Range2.95 to 4.252.75 to 4.10\n\n\n\n\n\nThe table shows that girls have a mean weight of 3.59 kg (SD 0.36) and boys have a mean weight of 3.42 kg (SD 0.35) with females being heavier than males. The variabilities of birth weight (i.e. the standard deviation) are similar between the two groups.\n\n\nConducting and interpreting an independent samples t-test\nAn independent samples t-test provides us with a t statistic from which we can compute a P value. The computation of the t statistic is as follows:\n\\[t = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{SE({\\overline{x}}_{1} - {\\overline{x}}_{2})}\\]\nwith the standard error and degrees of freedom calculated from software. Note that by using Welch’s t-test, the degrees of freedom will usually not be a whole number, and will appear with decimals.\nLooking at the formula for the t-statistic, we can see that the \\(t\\) is an estimate of how different the mean values are compared to the variability of the difference in means. So \\(t\\) will become larger as the difference in means increases with respect to the variability.\nStatistical software will calculate both the t and P values. If the t-value is large, the P value will be small, providing evidence against the null hypothesis of no difference between the groups.\nTable 5.2 summarises the results of an independent samples t-test using mod05_birthweight.rds. The process of conducting the t-test is summarised for jamovi and R in the following sections.\n\n\n\n\nTable 5.2: Birthweight (kg) by sex\n\n\n\nSexnMean (SE)95% Confidence IntervalFemale563.59 (0.049)3.49 to 3.68Male443.42 (0.053)3.31 to 3.53Difference0.17 (0.072)0.02 to 0.31\n\n\n\n\n\nHere we see that girls are heavier than boys, and the mean difference in weights between the genders is 0.17 kg (95% CI 0.02, 0.31). We are 95% confident that the true mean difference of weight between girls and boys lies between 0.02 and 0.31 kg. Note that this interval does not contain the null value of 0.\nHere we are testing the null hypothesis of no difference in mean birthweights between females and males: a two-sided test. The t-value is calculated as 2.30 with 93.5 degrees of freedom, and yields a two-sided P value of 0.023, providing evidence of a difference in mean birthweight between sex.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#paired-t-tests",
    "href": "05-comparing-two-means.html#paired-t-tests",
    "title": "5  Comparing the means of two groups",
    "section": "5.3 Paired t-tests",
    "text": "5.3 Paired t-tests\nIf the outcome of interest is the difference in the continuously outcome measurement between each pair of observations, a paired t-test is used. In effect, a paired t-test is used to assess whether the mean of the differences between the two related measurements is significantly different from zero. In this sense, a paired t-test is very closely aligned with a one sample t-test.\nWhen using a paired t-test, the variation between the pairs of measurements is the most important statistic. The variation between the participants is of little interest.\nFor related measurements, the data for each pair of values must be entered on the same row of the spreadsheet. Thus, the number of rows in the data sheet is the number of pairs of observations. Thus, the effective sample size is the total number of pairs and not the total number of measurements.\n\nAssumptions for a paired t-test\nThe assumptions for a paired t-test are:\n\nthe outcome variable is continuous\nthe differences between the pair of the measurements are normally distributed\n\nFor a paired samples t-test, it is important to test whether the differences between the two measurements are normally distributed. If the assumptions for a paired t-test cannot be met, a non-parametric equivalent is a more appropriate test to use (Module 9).\n\n\nComputing a paired t-test\nThe null hypothesis for using a paired t-test is as follows:\nH0: Mean (Measurement1 – Measurement2) = 0\nTo compute a t-value, the size of the mean difference between the two measurements is compared to the standard error of the paired differences, i.e.\n\\[t = \\frac{\\overline{d}}{SE(\\overline{d})}\\]\nwith n–1 degrees of freedom, where n is the number of pairs.\nBecause the standard error becomes smaller as the sample size becomes larger, the t-value increases as the sample size increases for the same mean difference.\n\n\nWorked Example 5.2\nA total of 107 people were recruited into a study to assess whether ankle blood pressure measured in two different sites would be the same. For each person, systolic blood pressure (SBP) was measured in two sites: dorsalis pedis and tibialis posterior.\nThe dataset mod05_ankle_bp.xlsx is available on Moodle. First, we need to compute the pairwise difference between SBP measured in the two sites. The distribution of the difference between SBP measured in dorsalis pedis and tibialis posterior is shown in Figure 5.2. The differences approximate a normal distribution and therefore a paired t-test can be used.\n\n\n\n\n\n\n\n\nFigure 5.2: Distribution of differences in ankle SBP between two sites of 107 participants\n\n\n\n\n\nThe paired t-test can be performed using statistical software, with a summary of the results presented in Table 5.3. We can see that the mean SBP is very similar in the two sites.\n\n\n\n\nTable 5.3: Systolic blood pressure (mmHg) measured at two sites on the ankle\n\n\n\nSitenMean (SE)95% Confidence IntervalDorsalis pedis107116.7 (3.46)(109.9 to 123.6)Tibialis posterior107118.0 (3.43)(111.2 to 124.8)Difference107-1.3 (1.31)(-3.9 to 1.3)\n\n\n\n\n\nThe t-value is calculated as −0.96 with 106 degrees of freedom, providing a two-sided P-value of 0.34. Thus these data provide no evidence of a difference in systolic blood pressure between the two sites.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#checking-data-for-the-independent-samples-t-test",
    "href": "05-comparing-two-means.html#checking-data-for-the-independent-samples-t-test",
    "title": "5  Comparing the means of two groups",
    "section": "5.4 Checking data for the independent samples t-test",
    "text": "5.4 Checking data for the independent samples t-test\n\nExamining variable distributions by a second variable\nWe can use Analyses &gt; Exploration &gt; Descriptives to obtain the distribution plots in Figure 5.1. Choose birthweight to appear in the Variables box, and gender as the Split by variable. Choose Density in the Plots section:\nJamovi also produces summary statistics for each level of the Split by variable, and we can select the statistics of interest in the Statistics section as necessary.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#independent-samples-t-test-1",
    "href": "05-comparing-two-means.html#independent-samples-t-test-1",
    "title": "5  Comparing the means of two groups",
    "section": "5.5 Independent samples t-test",
    "text": "5.5 Independent samples t-test\nTo carry out an independent sample t-test, go to Analyses &gt; T-Tests &gt; Independent Samples T-Test. Move birthweight into Dependent variables and gender as the Grouping Variable. Because we don’t assume equal variances of birthweight for males and females, we tick the Welch’s box.\nIn order to obtain an estimate of the difference in means, with its 95% Confidence Interval, tick the relevant boxes in Additional Statistics:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#checking-the-assumptions-for-a-paired-t-test",
    "href": "05-comparing-two-means.html#checking-the-assumptions-for-a-paired-t-test",
    "title": "5  Comparing the means of two groups",
    "section": "5.6 Checking the assumptions for a Paired t-test",
    "text": "5.6 Checking the assumptions for a Paired t-test\nBefore performing a paired t-test, you must check that the assumptions for the test have been met. Using the dataset mod05_ankle_bp.xlsx to show that the difference between the pair of measurements between the sites is normally distributed, we first need to compute a new variable of the differences.\nTo create a new column at the end of your dataset, click a cell in the first empty column, then choose Data &gt; Compute. We want to compute difference as sbp_dp − sbp_tp, so we enter this in the formula box as below:\n\nSpecify the name of the variable to be created: here difference\nClick the f x button to display a list of variable names in your dataset\nDouble-click the variable sbp_dp to bring it into the formula box\nType - to represent “minus”\nDouble-click the variable sbp_tp to bring it into the formula box (do not worry if your formula is underlined in red, this is simply a spell-check)\nClick the up arrow to close the Compute dialog box\n\n(Note that steps 3 to 5 could also be completed by typing the formula.)\nYou will see a new column called difference which represents the difference between the two blood pressures.\n\n\n\n\n\n\n\n\n\nA density plot of the differences can be constructed in the usual way.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#paired-t-test",
    "href": "05-comparing-two-means.html#paired-t-test",
    "title": "5  Comparing the means of two groups",
    "section": "5.7 Paired t-Test",
    "text": "5.7 Paired t-Test\nUsing the same blood pressure data as previously, choose Analyses &gt; T-Tests &gt; Paired Samples T-Test. Select sbp_dp and sbp_tp as the Paired Variables.\nTo obtain more informative output, select Mean difference and Confidence interval as additional statistics. The dialog box will look like:\n\n\n\n\n\n\n\n\n\nWith the following output:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#checking-data-for-the-independent-samples-t-test-1",
    "href": "05-comparing-two-means.html#checking-data-for-the-independent-samples-t-test-1",
    "title": "5  Comparing the means of two groups",
    "section": "5.8 Checking data for the independent samples t-test",
    "text": "5.8 Checking data for the independent samples t-test\n\nExamining variable distributions by a second variable\nWe can use the a splitBy variable in the descriptives function in the jmv package to obtain summary statistics for each level of a grouping variable. Further, specifying dens = TRUE will produce density plots for the analysis variable for each level of the grouping variable.\nFor example, to create the distribution plots in Figure 5.1, we can use\n\nlibrary(jmv)\n\nbwt &lt;- readRDS(\"data/examples/mod05_birthweight.rds\")\n\ndescriptives(data = bwt, vars = birthweight, splitBy = gender, dens = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                    \n ─────────────────────────────────────────────── \n                         gender    birthweight   \n ─────────────────────────────────────────────── \n   N                     Female             56   \n                         Male               44   \n   Missing               Female              0   \n                         Male                0   \n   Mean                  Female       3.587411   \n                         Male         3.421364   \n   Median                Female       3.530000   \n                         Male         3.430000   \n   Standard deviation    Female      0.3629788   \n                         Male        0.3536165   \n   Minimum               Female       2.950000   \n                         Male         2.750000   \n   Maximum               Female       4.250000   \n                         Male         4.100000   \n ───────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#independent-samples-t-test-2",
    "href": "05-comparing-two-means.html#independent-samples-t-test-2",
    "title": "5  Comparing the means of two groups",
    "section": "5.9 Independent samples t-test",
    "text": "5.9 Independent samples t-test\nWe can use the ttestIS() (t-test, independent samples) function from the jmv package to perform the independent samples t-test. We include the meanDiff=TRUE and ci=TRUE options to obtain the difference in means, with its 95% confidence interval. We can request a Welch’s test (which does not assume equal variances) by the welchs=TRUE option:\n\nttestIS(data = bwt, vars = birthweight, group = gender, meanDiff = TRUE, ci = TRUE, welchs = TRUE)\n\n\n INDEPENDENT SAMPLES T-TEST\n\n Independent Samples T-Test                                                                                                          \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                                 Statistic    df          p            Mean difference    SE difference    Lower         Upper       \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   birthweight    Student's t     2.296556    98.00000    0.0237731          0.1660471       0.07230265    0.02256481    0.3095293   \n                  Welch's t       2.303840    93.54377    0.0234458          0.1660471       0.07207403    0.02293328    0.3091609   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   Note. Hₐ μ &lt;sub&gt;Female&lt;/sub&gt; ≠ μ &lt;sub&gt;Male&lt;/sub&gt;",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#checking-the-assumptions-for-a-paired-t-test-1",
    "href": "05-comparing-two-means.html#checking-the-assumptions-for-a-paired-t-test-1",
    "title": "5  Comparing the means of two groups",
    "section": "5.10 Checking the assumptions for a Paired t-test",
    "text": "5.10 Checking the assumptions for a Paired t-test\nBefore performing a paired t-test, you must check that the assumptions for the test have been met. Using the dataset mod05_ankle_bp.xlsx to show that the difference between the pair of measurements between the sites is normally distributed, we first need to compute a new variable of the differences and examine its distribution.\n\nlibrary(readxl)\n\nsbp &lt;- read_excel(\"data/examples/mod05_ankle_bp.xlsx\")\nsbp$diff &lt;- sbp$sbp_dp - sbp$sbp_tp\ndescriptives(data = sbp, vars = diff, dens = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                        \n ─────────────────────────────────── \n                         diff        \n ─────────────────────────────────── \n   N                           107   \n   Missing                       0   \n   Mean                  -1.261682   \n   Median                 0.000000   \n   Standard deviation     13.56489   \n   Minimum               -40.00000   \n   Maximum                65.00000   \n ─────────────────────────────────── \n\n\n\n\n\n\n\n\n\nWhile there is a large difference in blood pressure (around 60 mmHg) that warrants further checking, the curve is roughly symmetric with an approximately Normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "05-comparing-two-means.html#paired-t-test-1",
    "href": "05-comparing-two-means.html#paired-t-test-1",
    "title": "5  Comparing the means of two groups",
    "section": "5.11 Paired t-Test",
    "text": "5.11 Paired t-Test\nTo perform a paired t-test we will use the dataset mod05_ankle_bp.xlsx. We can perform a paired t-test using the ttestPS() function within the jmv package, where we defined the paired observations as: `pairs=list(list(i1 = ‘variable1’, i2 = ‘variable2’))\n\nttestPS(data = sbp, pairs = list(list(i1 = \"sbp_dp\", i2 = \"sbp_tp\")), meanDiff = TRUE, ci = TRUE)\n\n\n PAIRED SAMPLES T-TEST\n\n Paired Samples T-Test                                                                                                                   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                                      statistic     df          p            Mean difference    SE difference    Lower        Upper      \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   sbp_dp    sbp_tp    Student's t    -0.9621117    106.0000    0.3381832          -1.261682         1.311368    -3.861596    1.338232   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   Note. Hₐ μ &lt;sub&gt;Measure 1 - Measure 2&lt;/sub&gt; ≠ 0\n\n\nThe syntax of the ttestPS function is a little cumbersome. The t.test function can be used as an alternative:\n\nt.test(sbp$sbp_dp, sbp$sbp_tp, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  sbp$sbp_dp and sbp$sbp_tp\nt = -0.96211, df = 106, p-value = 0.3382\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.861596  1.338232\nsample estimates:\nmean difference \n      -1.261682",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparing the means of two groups</span>"
    ]
  },
  {
    "objectID": "06-proportions.html",
    "href": "06-proportions.html",
    "title": "6  Summary statistics for binary data",
    "section": "",
    "text": "Learning objectives\nBy the end of this module you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#learning-objectives",
    "href": "06-proportions.html#learning-objectives",
    "title": "6  Summary statistics for binary data",
    "section": "",
    "text": "Compute and interpret 95% confidence intervals for proportions;\nConduct and interpret a significance test for a one-sample proportion;\nUse statistical software to compute 95% confidence intervals for a difference in proportions, a relative risk and an odds ratio.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#optional-readings",
    "href": "06-proportions.html#optional-readings",
    "title": "6  Summary statistics for binary data",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapter 16 [UNSW Library Link]\nBland (2015); Section 8.6, Section 13.7 [UNSW Library Link]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#introduction",
    "href": "06-proportions.html#introduction",
    "title": "6  Summary statistics for binary data",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nIn Modules 4 and 5, we discussed methods used to analyse continuous data. In Modules 6 and 7, we will focus on analysing categorical data.\nIn health research, we often collect information that can be put into two categories, e.g. male and female, disease present or disease absent etc. Binary categorical variables such as these are summarised using proportions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#calculating-proportions-and-95-confidence-intervals",
    "href": "06-proportions.html#calculating-proportions-and-95-confidence-intervals",
    "title": "6  Summary statistics for binary data",
    "section": "6.2 Calculating proportions and 95% confidence intervals",
    "text": "6.2 Calculating proportions and 95% confidence intervals\n\nCalculating a proportion\nWe need two pieces of information to calculate a proportion: \\(n\\), the number of trials, and \\(k\\), the number of ‘successes’. Note that we use the term ‘success’ to describe the outcome of interest, recognising that a success may be a adverse outcome such as death or disease.\nThe following formula is used to calculate the proportion, \\(p\\):\n\\[ p = k / n \\]\nThe proportion, \\(p\\), is a number that lies between 0 and 1. Proportions and their confidence intervals can easily be converted to percentages by multiplying by 100 once computed.\nAs for all summary statistics, it is useful to compute the precision of the estimate as a 95% confidence interval (CI) to indicate the range of values in which are 95% confident that the true population value lies. In this module, we present two methods for computing a 95% confidence interval around a proportion.\n\n\nCalculating the 95% confidence interval of a proportion (Wald method)\nThe Wald method for calculating the 95% confidence interval is based on assuming that the proportion, \\(p\\), is Normally distributed. This assumption is reasonable if the sample is sufficiently large (for example, if \\(n&gt;30\\)) and if \\(n \\times (1-p)\\) and \\(n \\times p\\) are both larger than 5.\nThe Wald method for calculating a 95% confidence interval is given by:\n\\[\\text{95\\% CI} = p \\pm (1.96 \\times \\text{SE}(p))\\]\nwhere the standard error of a proportion is computed as:\n\\[\\text{SE}(p) = \\sqrt{\\frac{p \\times (1 - p)}{n}}\\]\n\n\nWorked Example 6.1\nIn a cross-sectional study of children living in a rural village, 47 children from a random sample of 215 children were found to have scabies. Here \\(n=215\\) and \\(k=47\\), so the proportion of children with scabies is estimated as:\n\\[ p = \\frac{47}{215} = 0.2186 \\]\nGiven the large sample size and the number of children with the rarer outcome is larger than 5, the Wald method is used to calculate the standard error of the proportion as:\n\\[{\\text{SE}\\left( p \\right) = \\sqrt{\\frac{0.2186 \\times (1 - 0.2186)}{215}}\n}{= 0.02819}\\]\nThen, the 95% confidence interval is estimated as:\n\\[\\text{95\\% CI} = 0.2186 \\pm 1.96 \\times 0.02819\\]\n\\[= 0.1634 \\text{ to } 0.2739\\]\nThe prevalence of scabies among children in the village is 21.9% (95% CI 16.3%, 27.4%). These values tell us that we are 95% confident that the true prevalence of scabies among children in the village is between 16.3% and 27.4%.\n\n\nCalculating the 95% confidence interval of a proportion (Wilson method)\nAnother method to calculate the confidence interval of a proportion is the Wilson (sometimes also called the ‘score’) method. We can use it in situations where it is not appropriate to use the normal approximation to the binomial distribution as described above i.e. if the sample size is small (\\(n &lt; 30\\)) or the number of subjects with the rarer outcome is 5 or fewer. This method much more difficult to implement by hand than the standard confidence interval, and so we will not discuss the hand calculation using the mathematical equation in this course. Instead, we use statistical software to do this (see the jamovi or R notes for detail).\nWhen using software, our worked example provides a 95% confidence interval of the prevalence of scabies of 16.9% to 27.9%.\n\n\nWald vs Wilson methods\nThe Wald method, which assumes that the underlying proportion follows a Normal distribution, is easy to calculate and follows the form of other confidence intervals. The Wilson method, which is difficult to calculate by hand, has nicer mathematical properties. There are also a number of other methods for calculating confidence intervals for proportions, but we do not discuss these in this course.\nA paper by Brown, Cai and DasGupta (Brown, Cai, and DasGupta (2001)) has compared the properties of the Wald and Wilson methods (among others) and concluded that the Wilson method is preferred over the Wald method. Therefore, we recommend the Wilson method be used to calculate 95% confidence intervals for a proportion. Note that it is not possible to compute a Wilson confidence interval using jamovi. The interval calculated by jamovi is the Clopper-Pearson interval.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#hypothesis-testing-for-one-sample-proportion",
    "href": "06-proportions.html#hypothesis-testing-for-one-sample-proportion",
    "title": "6  Summary statistics for binary data",
    "section": "6.3 Hypothesis testing for one sample proportion",
    "text": "6.3 Hypothesis testing for one sample proportion\nWe can carry out a hypothesis test to compare a sample proportion to a hypothesised proportion. In much the same way as a one sample t-test was used in Module 5 to test a sample mean against a hypothesised mean, we can perform a one-sample test to test a sample proportion against a hypothesised proportion. The significance test will provide a P-value to assess the evidence against the null hypothesis, while the 95% confidence interval will provide the range in which we are 95% confident that the true proportion lies.\nFor example, we can test the following null hypothesis:\nH0: sample proportion is not different from the hypothesised proportion\nMuch like constructing a 95% confidence interval, there are two main options when performing a hypothesis test on a single proportion: the first assumes that the proportion follows a Normal distribution, while the second relaxes this assumption.\n\nz-test for testing one sample proportion\nThe first step in the z-test is to calculate a z-statistic, which is then used to calculate a P-value. The z-statistic is calculated as the difference between the population proportion and the sample proportion divided by the standard error of the population proportion, i.e.\n\\[\nz = \\frac{(p_{sample} - p_{population})}{\\text{SE}(p_{population})}\n\\]\nThis z-statistic is then compared to the standard Normal distribution to calculate the P-value.\n\n\nWorked Example 6.2\nA national census in a country shows that 20% of the population are smokers. A survey of a community within the country that has received a public health anti-smoking intervention shows that 54 of 300 people sampled are smokers (18%). We can calculate a 95% confidence interval around this proportion using the Wilson method, which is calculated as 14.1% to 22.7%.\nThe researchers are interested in whether the proportion of smoking in this community is the same as the population prevalence of smoking of 20%. The null hypothesis can be written as: H0: the proportion of smokers in the community is 20% (the same as in the national census).\nWe can test this by calculating a z-statistic:\n\\[\n\\begin{aligned}\nz &= \\frac{(0.18 - 0.20)}{\\sqrt{\\frac{0.20 × (1 - 0.20)}{300}}} \\\\\n&= -0.87\n\\end{aligned}\n\\]\nThe P-value for the test above can be obtained from a Normal distribution table as \\(P = 2 × 0.192 = 0.38\\) (using statistical software). This indicates that there is insufficient evidence to conclude that there is a difference between the proportion of smokers in the community and the country. This is consistent with our 95% confidence interval which crosses the null value of 20%.\n\n\n\n\n\n\n\n\n\n\n\nBinomial test for testing one sample proportion\nWe can use the binomial distribution to obtain an exact P-value for testing a single proportion. Historically, this was a time consuming process with much hand calculation. These days, statistical software performs the calculations quickly and efficiently, and is the preferred method.\n\n\nWorked example 6.3\nThe file mod06_smoking_status.rds contains the data for this example. In the data file, smokers are coded as 1 and non-smokers are coded as 0. In jamovi, we can perform the binomial test, while in R, we can use the prop.test function to perform a z-test, or the binom.test function to perform the binomial test.\nThe z-test provides a two-sided P-value of 0.39, while the binomial test gives a two-sided P-value of 0.43. Both tests provide little evidence against the hypothesis that the prevalence of smoking in the community is 20%.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#contingency-tables",
    "href": "06-proportions.html#contingency-tables",
    "title": "6  Summary statistics for binary data",
    "section": "6.4 Contingency tables",
    "text": "6.4 Contingency tables\nAs introduced in PHCM9794: Foundations of Epidemiology, 2-by-2 contingency tables can be used to examine associations between two binary variables, most commonly an exposure and an outcome. The traditional form of a 2-by-2 contigency table is given in Table 6.1.\n\n\n\n\nTable 6.1: Traditional format for presenting a contingency table\n\n\n\n Outcome presentOutcome absentTotalExposure presentaba+bExposure absentcdc+dTotala+cb+dN\n\n\n\n\n\nWhen using a statistics program, it is recommended that the outcome and exposure variables are coded by assigning ‘absent’ as 0 and ‘present’ as 1, for example ‘No’ = 0 and ‘Yes’ = 1. This coding ensures that measures of association, such as the odds ratio or relative risk, are computed correctly. While R does not strictly require this coding to be followed, it is good practice nonetheless.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#a-brief-summary-of-epidemiological-study-types",
    "href": "06-proportions.html#a-brief-summary-of-epidemiological-study-types",
    "title": "6  Summary statistics for binary data",
    "section": "6.5 A brief summary of epidemiological study types",
    "text": "6.5 A brief summary of epidemiological study types\nIn this section, we wil present a very brief summary of three study types commonly used in population health research. This topic is covered in much more detail in PHCM9794: Foundations of Epidemiology, and more detail can be found in Chapter 4 of Essential Epidemiology (3rd or 4th edition) Webb, Bain and Page (Webb, Bain, and Page (2016)).\n\nRandomised controlled trial\nA randomised controlled trial addresses the research question: what is the effect of an intervention on an outcome. In the simplest form of a randomised controlled trial, a group of participants is randomly allocated to a group that receives the treatment of interest or to a control group that does not receive the treatment of interest. Participants are followed up over time, and the outcome is measured at the conclusion of the study.\n\n\n\n\n\nThe design of a randomised controlled trial [Figure 4.1, Essential Epidemiology]\n\n\n\n\n\n\nCohort study\nA cohort study is an observational study that addresses the research question: what is the effect of an exposure on an outcome. This research question is similar to that studied in a randomised controlled trial, but the exposure is defined by the participants’ circumstances, and not manipulated by the researchers. In a cohort study, participants without the outcome of interest are enrolled, followed over time, and information on their exposure to a factor is measured (either at baseline or over time). At the conclusion of the study, information on the outcome is measured to identify new (incident) cases.\n\n\n\n\n\nThe design of a cohort study [Figure 4.2, Essential Epidemiology]\n\n\n\n\n\n\nCase control study\nWhile the randomised controlled trial and cohort study begin with a population without the outcome, a case-control study begins by assembling a group with the outcome of interest (cases), and a group without the outcome of interest (controls). The researchers then ask the cases and controls about their previous exposures.\n\n\n\n\n\nThe design of a case-control trial [Figure 4.3, Essential Epidemiology]\n\n\n\n\n\n\nCross-sectional study\nIn a cross-sectional study, the exposure and the outcome are measured at the same time. While this results in a study that is relatively quick to conduct, it does not allow for any temporal relationships to be assessed.\n\n\n\n\n\nThe design of a cross-sectional study [Figure 4.4, Essential Epidemiology]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#measures-of-effect-for-epidemiological-studies",
    "href": "06-proportions.html#measures-of-effect-for-epidemiological-studies",
    "title": "6  Summary statistics for binary data",
    "section": "6.6 Measures of effect for epidemiological studies",
    "text": "6.6 Measures of effect for epidemiological studies\nWe can calculate a relative measure of association between an exposure and an outcome as either a relative risk or odds ratio. The relative risk is a direct comparison of the risk in the exposed group with the risk in the non-exposed group, and can only be calculated for a cohort study (including a randomised controlled trial) or a cross-sectional study (where it is also called a prevalence ratio).\nFor cohort studies, randomised controlled trials and cross-section studies, we can calculate an absolute measure of association between an exposure and an outcome as a difference in proportions (also known as an attributable risk).\nFor case-control studies, as we sample participants based on their outcome, we can not estimate the risk of the outcome. Hence, calculating a relative risk or risk difference is inappropriate. Instead of calculating risks in a case-control study, we instead calculate odds, where the odds of an event are calculated as the number with the event divided by the number without the event.\n\n\n\n\nTable 6.2: Contingency table for a case-control study\n\n\n\n CasesControlsTotalExposure presentaba+bExposure absentcdc+dTotala+cb+dN\n\n\n\n\n\nIn the example in Table Table 6.2, we can calculate the odds of being exposed in the cases as \\(a \\div c\\). Similarly, we can calculate the odds of being exposed in the controls as \\(b \\div d\\). We can the calculate the odds ratio as:\n\\[\n\\begin{aligned}\n\\text{Odds ratio} &= (a \\div c) \\div (b \\div d) \\\\\n&= \\frac{a \\times d}{b \\times c} \\\\\n&= \\frac{ad}{bc}\n\\end{aligned}\n\\]\nNote that some authors say we should think of the odds ratio being based on the odds of being a case in the exposed group compared to the odds of being a case in the unexposed group. Here, the exposed group comprises cells “a” and “b”, so the odds of being a case in the exposed group is (a/b). Similarly, for the unexposed group, the odds of being exposed is (c/d). So our odds ratio becomes (a/b) / (c/d). If we rearrange this, we get the same odds ratio as above: (ad)/(bc).\nThe interpretation of an odds ratio is discussed in detail in PHCM9794: Foundations of Epidemiology, and an excerpt is presented here: The meaning of the calculated odds ratio as a measure of association between exposure and outcome is the same as for the rate ratio (relative risk) where:\n\nAn odds ratio &gt;1 indicates that exposure is positively associated with disease (i.e. the exposure may be a cause of disease);\nAn odds ratio &lt; 1 indicates that exposure is negatively associated with disease (i.e. the exposure may be protective against disease); and\nAn odds ratio = 1 indicates no association between the exposure and the outcome.\n\nIn some situations, related to how well controls are recruited into this study, the odds ratio is a close approximation of the relative risk. Therefore, you may see in some published papers of case control studies the OR interpreted as you would interpret a RR. This should be avoided in this course.\nMore information about the problems of interpreting odds-ratios as relative risks has been presented by Deeks (1998) and Schmidt and Kohlmann (2008).\n\nWorked Example 6.4\nA randomised controlled trial was conducted among a group of patients to estimate the side effects of a drug. Fifty patients were randomly allocated to receive the active drug and 50 patients were allocated to receive a placebo drug. The outcome measured was the experience of nausea. The data is given in the file mod06_nausea.rds.\nA summary table can be constructed as in Table 6.3.\n\n\n\n\nTable 6.3: Nausea status by drug exposure\n\n\n\n NauseaNo nauseaTotalActive drug153550Placebo44650Total1981100\n\n\n\n\n\nWe can use jamovi or R to calculate the relative risk (RR=3.75) and its 95% confidence interval (1.34 to 10.51). This tells us that nausea is 3.75 times more likely to occur in the active drug group compared with the placebo group. Because this is a randomised controlled trial, the relative risk would be an appropriate measure of association.\nWe can confirm the estimated relative risk:\n\\[\n\\begin{aligned}\n\\text{RR} &= \\frac{a / (a+b)}{c / (c+d)} \\\\\n  &= \\frac{15 / (15+35)}{4 / (4+46)} \\\\\n  &= \\frac{0.3}{0.08} \\\\\n  &= 3.75\n\\end{aligned}\n\\]\n\n\nWorked Example 6.5\nA case-control study investigated the association between human papillomavirus and oropharyngeal cancer (D'Souza, et al. NEJM 2007), and the results appear in Table 6.4.\n\n\n\n\nTable 6.4: Association between human papillomavirus and oropharyngeal cancer\n\n\n\n CasesControlsTotalHPV Positive571471HPV Negative43186229Total100200300\n\n\n\n\n\nThe odds ratio is the odds of being HPV positive in cases (those with oropharyngeal cancer) compared to the odds of being HPV positive in the controls (those without oropharyngeal cancer):\n\\[\n\\begin{aligned}\n\\text{OR} &= \\frac{a / c}{b /d} \\\\\n  &= \\frac{57 / 43}{14 / 186} \\\\\n  &= 17.6\n\\end{aligned}\n\\]\nWe can use jamovi or R to estimate the odds ratio and its 95% confidence interval. The odds ratio is estimated as 17.6, and its 95% confidence interval is estimated as 9.0 to 34.5.\nThe interpretation of the confidence intervals for both the relative risk and the odds ratio is the same as for the confidence intervals around other summary measures in that it shows the region in which we are 95% confident that the true population estimate lies.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#confidence-intervals-for-proportions",
    "href": "06-proportions.html#confidence-intervals-for-proportions",
    "title": "6  Summary statistics for binary data",
    "section": "6.7 95% confidence intervals for proportions",
    "text": "6.7 95% confidence intervals for proportions\nTo analyse proportions in jamovi, we use Frequencies &gt; One Sample Proportion Tests &gt; 2 Outcomes | Binomial Test. The procedure is slightly different if we are using individual level vs summary data. Here, the procedure will be illustrated as if we have summary data.\nIn Worked Example 6.1, 47 children were found to have scabies and 168 (i.e. 215 - 47) were found not to have scabies. We need to enter two columns of data into jamovi: the first indicating whether a count is for scabies or no scabies, and the second representing the number in each category.\nOur data are entered as follows:\n\nNote that there is no need to name the columns here; using A and B is fine.\nBefore we analyse these data, we need to tell jamovi that column B represents the count of each category. We do this by using Data &gt; Weights and defining B as the weight variable. This essentially says that the first row represents 47 observations, and the second row represents 168 observations:\n\nTo estimate the proportion with scabies, we use Frequencies &gt; One Sample Proportion Tests &gt; 2 Outcomes | Binomial Test, defining Column A as the analysis variable and requesting confidence intervals:\n\nNote that the jamovi provides the proportion with scabies as well as the proportion without scabies.\n\nBinomial test for testing one sample proportion\nA binomial test can be performed using a similar approach. Here we consider Worked example 6.3, testing whether a sample is consistent with a true smoking proportion of 20%. mod06_smoking_status.rds contains individual level data, so we do not need to use weighting.\nAfter opening the data, click Analyses &gt; Frequencies &gt; One Sample Proportion Tests &gt; 2 Outcomes | Binomial Test. Set the Test value as 0.2, and tick the Confidence intervals box:\n\nThe P-value for testing whether the true proportion of smokers is 20% is provided as P=0.43:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval",
    "href": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval",
    "title": "6  Summary statistics for binary data",
    "section": "6.8 Computing a relative risk and its 95% confidence interval",
    "text": "6.8 Computing a relative risk and its 95% confidence interval\nTo calculate relative risks, odds ratios and risk differences correctly, we must define the positive exposure and positive outcome to be the first level of a factor. When defining an exposure for example, we should define the active treatment or the positive exposure as the first category. When defining an outcome, we should define the category of interest (e.g. disease, or side effect) as the first category.\nWe will use Worked Example 6.4 to demonstrate calculating a relative risk and its 95% CI, by opening mod06_nausea.rds.\nBefore analysing these data, we should check that the exposure (group) and outcome (side_effect) variables have been set up correctly, with the correct level chosen to be of interest. In this example, we will define Active as the first level in the group factor, and Nausea to be the first level of the side_effect factor. Let’s consider the exposure variable first. Click Data &gt; Setup - this will open the following window:\n\nNotice the Levels section, highlighted in red. This means that that the group variable has been entered in jamovi with Placebo as the first category, and Active as the second. This ordering means that jamovi will incorrectly consider Placebo as “Exposed”, and Active as “Unexposed”. We need to change the ordering, so that Active is the first level, and Placebo is second.\nTo re-order the levels:\n\nclick the Placebo cell in the Levels box, then\nclick the down arrow to move Placebo to be the second level:\n\n\nThe Levels section should appear as below:\n\nClick the side_effect column to investigate the ordering of the outcome variable. Repeat the process to set Nausea as the first level, and No nausea as the second variable.\nTo construct the 2-by-2 table and calculate a relative risk, we use Analyses &gt; Frequencies &gt; Independent samples. Define Rows as the exposure variable (group), and Columns as the outcome (side_effect). We can request the row-percents by ticking Row in the Cells section, and request the relative risk and confidence interval by ticking Relative risk in the Statistics section:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#computing-other-measures-of-effect",
    "href": "06-proportions.html#computing-other-measures-of-effect",
    "title": "6  Summary statistics for binary data",
    "section": "6.9 Computing other measures of effect",
    "text": "6.9 Computing other measures of effect\nAn Odds ratio or Difference in proportions can be requested in the Statistics section.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#working-with-summarised-data",
    "href": "06-proportions.html#working-with-summarised-data",
    "title": "6  Summary statistics for binary data",
    "section": "6.10 Working with summarised data",
    "text": "6.10 Working with summarised data\nIf you only have the cross-tabulated data (i.e. the summarised or aggregated data), you will need to enter your data into a new spreadsheet. For example, to recreate the above analyses, we could re-write the 2-by-2 table as follows:\n\n\n\nGroup\nSide effect\nCount\n\n\n\n\nActive\nNausea\n15\n\n\nActive\nNo nausea\n35\n\n\nPlacebo\nNausea\n4\n\n\nPlacebo\nNo nausea\n46\n\n\n\nWe can enter these data in a new spreadsheet, entering the exposure and outcome using the values of 0 or 1. By convention, we use 1 to represent the exposed category, and 0 to represent the unexposed category. Similarly, we use 1 to represent the outcome category of interest, and 0 to represent the outcome category not of interest. Our entered data would look as follows:\n\nThe variable names can be changed in the variables tab in the usual way. It is good practice to label the levels of the exposure and outcome variables - this can be done in Data &gt; Setup. Click the variable to be defined, and type the labels of each level in the Levels section.\nHere, the variable group is defined as:\n\n1 represents Active\n0 represents Placebo\n\nThe Setup screen is completed as follows:\n\nThe side_effect variable should be set up using a similar approach, with the final spreadsheet looking like:\n\nThe analysis is conducted in the same way as for individual data, but we must now specify the Counts field:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#confidence-intervals-for-proportions-1",
    "href": "06-proportions.html#confidence-intervals-for-proportions-1",
    "title": "6  Summary statistics for binary data",
    "section": "6.11 95% confidence intervals for proportions",
    "text": "6.11 95% confidence intervals for proportions\nWe can use the BinomCI(x=, n=, method=) function within the DescTools package to compute 95% confidence intervals for proportions. Here we specify x: the number of successes, n: the sample size, and optionally, the method (which defaults to Wilson’s method).\n\nlibrary(DescTools)\n\nBinomCI(x=47, n=215, method='wald')\n\n           est    lwr.ci    upr.ci\n[1,] 0.2186047 0.1633595 0.2738498\n\nBinomCI(x=47, n=215, method='wilson')\n\n           est    lwr.ci    upr.ci\n[1,] 0.2186047 0.1685637 0.2785246",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#significance-test-for-single-proportion",
    "href": "06-proportions.html#significance-test-for-single-proportion",
    "title": "6  Summary statistics for binary data",
    "section": "6.12 Significance test for single proportion",
    "text": "6.12 Significance test for single proportion\nWe can use the binom.test function to perform a significance test for a single proportion: binom.test(x=, n=, p=). Here we specify x: the number of successes, n: the sample size, and p: the hypothesised proportion (which defaults to 0.5 if nothing is entered).\n\nbinom.test(x=54, n=300, p=0.2)\n\n\n    Exact binomial test\n\ndata:  54 and 300\nnumber of successes = 54, number of trials = 300, p-value = 0.4273\nalternative hypothesis: true probability of success is not equal to 0.2\n95 percent confidence interval:\n 0.1382104 0.2282394\nsample estimates:\nprobability of success \n                  0.18 \n\n\nNote that the binom.test function also produces a 95% confidence interval around the estimated proportion. This confidence interval is based on the inferior Wald method: the confidence interval derived from the Wilson method is preferred.\nWe can also conduct a z-test for a single proportion:\n\nprop.test(x=54, n=300, p=0.2, correct=FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  54 out of 300, null probability 0.2\nX-squared = 0.75, df = 1, p-value = 0.3865\nalternative hypothesis: true p is not equal to 0.2\n95 percent confidence interval:\n 0.1406583 0.2274332\nsample estimates:\n   p \n0.18",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval-1",
    "href": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval-1",
    "title": "6  Summary statistics for binary data",
    "section": "6.13 Computing a relative risk and its 95% confidence interval",
    "text": "6.13 Computing a relative risk and its 95% confidence interval\nWe will use Worked Example 6.4 to demonstrate calculating a relative risk and its 95% CI:\n\nlibrary(jmv)\n\ndrug &lt;- readRDS(\"data/examples/mod06_nausea.rds\")\n\nsummary(drug)\n\n     group       side_effect\n Placebo:50   No nausea:81  \n Active :50   Nausea   :19  \n\n\nBy using the head() function to view the first six lines of data, we see that both group and side_effect have been entered as factors. Notice the order in which the factor levels are presented: group has the Placebo level defined as the first level, and the Active level defined as the second; side_effect has No nausea defined as the first level, and the Nausea level defined as the second.\nWe will use jmv to calculate relative risks, odds ratios and risk differences. To calculate these estimates correctly, we must define the positive exposure and positive outcome to be the first level of a factor. When defining an exposure for example, we should define the active treatment or the positive exposure as the first category. When defining an outcome, we should define the category of interest (e.g. disease, or side effect) as the first category.\nIn this example, we will define Active as the first level in the group factor, and Nausea to be the first level of the side_effect factor.\nWe can do this using the relevel() function, which re-orders the levels of a factor so that the level specified is defined as the first level, and the others are moved down:\n\n# Define \"Active\" as the first level of group:\ndrug$group &lt;- relevel(drug$group, ref=\"Active\")\n\n\n# Define \"Nausea\" as the first level of side_effect:\ndrug$side_effect &lt;- relevel(drug$side_effect, ref=\"Nausea\")\n\nUpon re-leveling the factors, we can check that the levels of interest have been defined as the first levels:\n\nsummary(drug)\n\n     group       side_effect\n Active :50   Nausea   :19  \n Placebo:50   No nausea:81  \n\n\nTo construct the 2-by-2 table and calculate a relative risk, we use the contTables() function in jmv. We request the row-percents using pcRow = TRUE and the relative risk and confidence interval using relRisk = TRUE:\n\ncontTables(data=drug, \n           rows=group, cols=side_effect, \n           pcRow=TRUE, relRisk = TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                                 \n ────────────────────────────────────────────────────────────────── \n   group                      Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────────── \n   Active     Observed               15           35           50   \n              % within row     30.00000     70.00000    100.00000   \n                                                                    \n   Placebo    Observed                4           46           50   \n              % within row      8.00000     92.00000    100.00000   \n                                                                    \n   Total      Observed               19           81          100   \n              % within row     19.00000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\n Comparative Measures                                    \n ─────────────────────────────────────────────────────── \n                    Value         Lower       Upper      \n ─────────────────────────────────────────────────────── \n   Relative risk    3.750000 ᵃ    1.337540    10.51370   \n ─────────────────────────────────────────────────────── \n   ᵃ Rows compared\n\n\n\nIf you only have the cross-tabulated data (i.e. aggregated), you will need to enter your data into a new data frame. For example, to recreate the above analyses, we can re-write the 2-by-2 table as follows:\n\n\n\nGroup\nside_effect\nNumber\n\n\n\n\nActive\nNausea\n15\n\n\nActive\nNo nausea\n35\n\n\nPlacebo\nNausea\n4\n\n\nPlacebo\nNo nausea\n46\n\n\n\nWe can enter these data in a dataframe, comprising three vectors, as follows:\n\ndrug_aggregated &lt;- data.frame(\n  group = c(\"Active\", \"Active\", \"Placebo\", \"Placebo\"),\n  side_effect = c(\"Nausea\", \"No nausea\", \"Nausea\", \"No nausea\"),\n  n = c(15, 35, 4, 46)\n)\n\nWe need to define group and side_effect as factors. Here we must define the levels in the order we want the categories to appear in the table. Note that as group and side_effect are entered as text variables, we can omit labels command when defining the factors, and the factor will be labelled using the text entry:\n\ndrug_aggregated$group &lt;- factor(drug_aggregated$group, \n                                levels=c(\"Active\", \"Placebo\"))\n\ndrug_aggregated$side_effect &lt;- factor(drug_aggregated$side_effect, \n                                      levels=c(\"Nausea\", \"No nausea\"))\n\nWe can calculate the relative risk using the summarised data in the same was done previously. However, we need to include the number of observations in each cell using the counts command:\n\ncontTables(data=drug_aggregated,\n           rows=group, cols=side_effect, count=n,\n           pcRow=TRUE, relRisk = TRUE)\n\n\nCONTINGENCY TABLES\n\n Contingency Tables                                                 \n ────────────────────────────────────────────────────────────────── \n   group                      Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────────── \n   Active     Observed        15.000000     35.00000     50.00000   \n              % within row     30.00000     70.00000    100.00000   \n                                                                    \n   Placebo    Observed         4.000000     46.00000     50.00000   \n              % within row      8.00000     92.00000    100.00000   \n                                                                    \n   Total      Observed        19.000000     81.00000    100.00000   \n              % within row     19.00000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\n Comparative Measures                                    \n ─────────────────────────────────────────────────────── \n                    Value         Lower       Upper      \n ─────────────────────────────────────────────────────── \n   Relative risk    3.750000 ᵃ    1.337540    10.51370   \n ─────────────────────────────────────────────────────── \n   ᵃ Rows compared",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#computing-a-difference-in-proportions-and-its-95-confidence-interval",
    "href": "06-proportions.html#computing-a-difference-in-proportions-and-its-95-confidence-interval",
    "title": "6  Summary statistics for binary data",
    "section": "6.14 Computing a difference in proportions and its 95% confidence interval",
    "text": "6.14 Computing a difference in proportions and its 95% confidence interval\nWe can use the contTables function to obtain a difference in proportions and its 95% CI, by specifying diffProp=TRUE:\n\ncontTables(data=drug, \n           rows=group, cols=side_effect, \n           pcRow=TRUE, diffProp=TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                                 \n ────────────────────────────────────────────────────────────────── \n   group                      Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────────── \n   Active     Observed               15           35           50   \n              % within row     30.00000     70.00000    100.00000   \n                                                                    \n   Placebo    Observed                4           46           50   \n              % within row      8.00000     92.00000    100.00000   \n                                                                    \n   Total      Observed               19           81          100   \n              % within row     19.00000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\n Comparative Measures                                                      \n ───────────────────────────────────────────────────────────────────────── \n                                  Value          Lower         Upper       \n ───────────────────────────────────────────────────────────────────────── \n   Difference in 2 proportions    0.2200000 ᵃ    0.07238986    0.3676101   \n ───────────────────────────────────────────────────────────────────────── \n   ᵃ Rows compared",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "06-proportions.html#computing-an-odds-ratio-and-its-95-confidence-interval",
    "href": "06-proportions.html#computing-an-odds-ratio-and-its-95-confidence-interval",
    "title": "6  Summary statistics for binary data",
    "section": "6.15 Computing an odds ratio and its 95% confidence interval",
    "text": "6.15 Computing an odds ratio and its 95% confidence interval\nWe can use the contTables function to obtain an odds ratio and its 95% CI, by specifying odds=TRUE. Here we will use the summarised HPV data from Module 6.\n\nhpv &lt;- data.frame(\n  hpv = c(\"HPV +\", \"HPV +\", \"HPV -\", \"HPV -\"),\n  cancer = c(\"Case\", \"Control\", \"Case\", \"Control\"),\n  n = c(57, 14, 43, 186)\n)\n\nhpv$cancer &lt;- factor(hpv$cancer, levels=c(\"Case\", \"Control\"))\nhpv$hpv &lt;- factor(hpv$hpv, levels=c(\"HPV +\", \"HPV -\"))\n\ncontTables(data=hpv, \n           rows=hpv, cols=cancer, count=n,\n           odds = TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                               \n ──────────────────────────────────────────────── \n   hpv      Case         Control      Total       \n ──────────────────────────────────────────────── \n   HPV +     57.00000     14.00000     71.00000   \n   HPV -     43.00000    186.00000    229.00000   \n   Total    100.00000    200.00000    300.00000   \n ──────────────────────────────────────────────── \n\n\n χ² Tests                               \n ────────────────────────────────────── \n         Value       df    p            \n ────────────────────────────────────── \n   χ²    92.25660     1    &lt; .0000001   \n   N          300                       \n ────────────────────────────────────── \n\n\n Comparative Measures                               \n ────────────────────────────────────────────────── \n                 Value       Lower       Upper      \n ────────────────────────────────────────────────── \n   Odds ratio    17.61130    8.992580    34.49041   \n ──────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics for binary data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html",
    "href": "07-testing-proportions.html",
    "title": "7  Hypothesis testing for categorical data",
    "section": "",
    "text": "Learning objectives\nBy the end of this module you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#learning-objectives",
    "href": "07-testing-proportions.html#learning-objectives",
    "title": "7  Hypothesis testing for categorical data",
    "section": "",
    "text": "Use and interpret the appropriate test for testing associations between categorical data;\nConduct and interpret an appropriate test for independent proportions;\nConduct and interpret a test for paired proportions;",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#optional-readings",
    "href": "07-testing-proportions.html#optional-readings",
    "title": "7  Hypothesis testing for categorical data",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapter 17. [UNSW Library Link]\nBland (2015); Chapter 13. [UNSW Library Link]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#introduction",
    "href": "07-testing-proportions.html#introduction",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nIn Module 6, we estimated the 95% confidence intervals of proportions and measures of association for categorical data and conducted a significance test comparing a sample proportion to a known value.\nWhen both the outcome variable and the exposure variable are categorical, a chi-squared test can be used as a formal statistical test to assess whether the exposure and outcome are related. The P-value obtained from a chi-squared test gives the probability of obtaining the observed association (or more extreme) if there is in fact no association between the exposure and outcome.\nIn this Module, we also include tests for a difference in proportion for paired data.\n\nWorked Example\nWe are using the randomised controlled trial as given in Worked Example 6.4 on the nauseating side effect of a drug.\nThe research question is whether the active drug resulted in a different rate of nausea than the placebo drug. This is equivalent to testing whether there is an association between nausea and type of drug received (active or placebo). Thus, we will test the null hypothesis that the experience of nausea and the treatment are not related to one another. The null hypothesis is:\n\nH0: The proportion with nausea in the active drug group is the same as the proportion with nausea in the placebo drug group.\n\nThe alternative hypothesis can be stated as:\n\nHa: The proportion with nausea in the active drug group is different to the proportion with nausea in the placebo drug group.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-test-for-independent-proportions",
    "href": "07-testing-proportions.html#chi-squared-test-for-independent-proportions",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.2 Chi-squared test for independent proportions",
    "text": "7.2 Chi-squared test for independent proportions\nA chi-squared test is used to test the null hypothesis that of no association between two categorical variables. First a contingency table is drawn up and then we estimate the counts of each cell (i.e. a, b, c and d) that would be expected if the null hypothesis was true. The row and column totals are used to calculate expected counts in each cell of the contingency table as follows:\nExpected count = (Row count × Column count) / Total count\nStatistical software will do this for us, as described in the jamovi or R sections in this Module.\nA chi-squared value is then calculated to compare the expected counts (E) in each cell with the observed (actual) cell counts (O). The calculation is as follows:\n\\(\\chi ^ 2 = \\sum \\frac{(O - E)}{E} ^2\\)\nwith [Number of rows \\(-\\) 1] \\(\\times\\) [Number of columns \\(-\\) 1] degrees of freedom.\nAs for many statistics, the deviations between the observed and expected values are squared to prevent the negative and positive values balancing one another out.\nIf the expected counts are close to the observed counts, the chi-squared statistic will be close to zero, and the P-value will be close to 1. The larger the difference between the observed and expected counts, the larger the chi-squared statistic becomes (and the smaller the P-value). A large chi-squared statistic provides more evidence of an association between the exposure and outcome.\n\nAssumptions for using a Pearson’s chi-squared test\nThe assumptions that must be met when using Pearson’s chi-squared test are that:\n\neach observation must be independent;\neach participant is represented in the table once only;\nat least 80% of the expected cell counts should exceed a value of five;\nall expected cell counts should exceed a value of one.\n\nThe first two assumptions are dictated by the study design. The last two assumptions relate to the numbers in the cells and should be explored when running the test. There should not be too many cells with low expected counts.\n\n\nWorked Example 7.1\nWe will revisit Worked Example 6.4, investigating the relationship between nausea and drug exposure:\n\n\n\nTable 7.1: Nausea status by drug exposure\n\n\n\n\n\n\nNausea\nNo nausea\nTotal\n\n\n\n\nActive\n15 (30%)\n35 (70%)\n50 (100%)\n\n\nPlacebo\n4 (8%)\n46 (92%)\n50 (100%)\n\n\nTotal\n19 (19%)\n81 (81%)\n100 (100%)\n\n\n\n\n\n\nWe can see from the row percentages that 8% of patients in the placebo group experienced nausea compared to 30% of patients in the active group. If no association existed, we would expect to find approximately the same percent of patients with nausea in each group. Statistical software can calculate the values we would expect if there was no association between nausea and drug exposure (i.e. the expected counts):\n\n\n\nTable 7.2: Expected counts of nausea status by drug exposure\n\n\n\n\n\n\nNausea\nNo nausea\nTotal\n\n\n\n\nActive\n9.5\n40.5\n50\n\n\nPlacebo\n9.5\n40.5\n50\n\n\nTotal\n19\n81\n100\n\n\n\n\n\n\nFor the data being considered from Worked Example 7.1 all cells have an expected count greater than 5 and that the minimum cell count is 9.5. Therefore, it is appropriate to use the Pearson’s Chi-Squared test. Note that the ‘Expected’ counts are higher for the groups with ‘No nausea’ because ‘No nausea’ is more prevalent in the sample than ‘Nausea’.\nThe chi-squared statistic is calculated as 7.86 with 1 df, giving a P-value of 0.005. Combining these results with the estimated relative risk (from Module 6), we can state:\nThe proportion with nausea in those who received the active drug is 30%, compared to 8% in those who received the placebo drug. Nausea was more frequent in those who received the active drug (Relative Risk = 3.75, 95% CI: 1.34 to 10.51). There is strong evidence that the proportion with nausea differs between the two groups (\\(\\chi ^2\\) = 7.86 with 1 df, P=0.005).\n\n\nFisher’s exact test\nIf small expected cell counts are present, Fisher’s exact test can be used instead. More information on Fisher’s exact test can be found in Chapter 13 of An Introduction to Medical Statistics, Bland (2015), or Section 17.3 of Essential Medical Statistics, Kirkwood and Sterne (2001). The computation of Fisher’s exact test is complex, and best conducted by statistical software.\nA reasonable question could be posed: why not conduct Fisher’s exact test by default? The answer to this is complex.\nFisher’s exact test has quite a restrictive assumption: we assume that the totals of the rows and columns are fixed before we conduct the study.\nFrom Worked Example 7.1, this would be saying that we knew we would end up with 50 people in the active treatment arm, and 50 people in the placebo. This seems reasonable, we can design our study to randomise equal groups. However, Fisher’s exact test also assumes that we know we will obtain 19 people with nausea and 81 people without nausea. We cannot possibly know this before we do the study.\nIn the case where we cannot assume that the totals of the rows and columns are fixed before we conduct the study, it can be shown that Fisher’s exact test will be conservative (we will be less likely to reject the null hypothesis when it is false, or in other words, the P-value will be larger than it should be).\nWhile there are other tests that perform better than Fisher’s exact test, most of the time we live with this conservative test when we have to (i.e. for small expected cell counts) because Fisher’s exact test is so widely known.\nPragmatically, we use the standard (Pearson) chi-square when we can, and Fisher’s exact test only when we have small expected cell counts.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-tests-for-tables-larger-than-2-by-2",
    "href": "07-testing-proportions.html#chi-squared-tests-for-tables-larger-than-2-by-2",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.3 Chi-squared tests for tables larger than 2-by-2",
    "text": "7.3 Chi-squared tests for tables larger than 2-by-2\nChi-squared tests can also be used for tables larger than a 2-by-2 dimension. When a contingency table larger than 2-by-2 is used, say a 4-by-2 table if there were 4 exposure groups, the Pearson’s chi-squared can still be used.\n\nWorked Example 7.2\nThe file mod07_allergy.rds contain information about the severity of allergic reaction, coded as absent, slight, moderate or severe. We can test the hypothesis that the severity of allergy is not different between males and females. To do this we can use a two-way tabulation to obtain Table 7.3 which shows the counts, expected counts and the percent of females and males who fall into each severity group for allergy. The table shows that the percentage of males is higher in each of the categories of severity (slight, moderate, severe) than the percentage of females.\n\n\n\nTable 7.3: Association between sex and allergy severity\n\n\n\n\n\nObserved counts\n\n\n\n\n\n\n\n\n\n\nSex\nNon-allergenic\nSlight allergy\nModerate allergy\nSevere allergy\nTotal\n\n\n\n\nFemale\n150 (62.0%)\n50 (20.7%)\n27 (11.2%)\n15 (6.2%)\n242 (100%)\n\n\nMale\n137 (53.1%)\n70 (27.1%)\n32 (12.4%)\n19 (7.4%)\n258 (100%)\n\n\nTotal\n287 (57.4%)\n120 (24.0%)\n59 (11.8%)\n34 (6.8%)\n500 (100.0%)\n\n\n\n\n\n\n\n\nExpected counts\n\n\n\n\n\n\n\n\n\n\nSex\nNon-allergenic\nSlight allergy\nModerate allergy\nSevere allergy\nTotal\n\n\n\n\nFemale\n138.9\n58.1\n28.6\n16.5\n242.0\n\n\nMale\n148.1\n61.9\n30.4\n17.5\n258.0\n\n\nTotal\n287.0\n120.0\n59.0\n34.0\n500.0\n\n\n\n\n\n\n\n\nThe Pearson chi-squared statistic is calculated as 4.31, with 3 degrees of freedom, providing a P-value of 0.23. Therefore, there is little evidence of an association between gender and the severity of allergy.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#mcnemars-test-for-categorical-paired-data",
    "href": "07-testing-proportions.html#mcnemars-test-for-categorical-paired-data",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.4 McNemar’s test for categorical paired data",
    "text": "7.4 McNemar’s test for categorical paired data\nIf a binary categorical outcome is measured in a paired study design, McNemar’s statistic is used. This statistic is a form of chi-square applied to a paired situation. A Pearson’s chi-squared test cannot be used because the measurements are not independent. However, McNemar’s test can be used to assess whether there is a significant change in proportions between two time points or between two conditions, or whether there is a significant difference in proportions between matched cases and controls.\nFor McNemar’s test, the data are displayed as shown in Table 7.4. Cells ‘a’ and ‘d’ called concordant cells because the response was the same at both baseline and follow-up or between matched cases and controls. Cells ‘b’ and ‘c’ are called discordant cells because the responses between the pairs were different. For a follow-up study, the participants in cell ‘c’ had a positive response at baseline and a negative response at follow-up. Conversely, the participants in cell ‘b’ had a negative response at baseline and a positive response at follow-up.\nFor other types of paired data such as twins or matched cases and controls, the data are similarly displayed with the responses of one of the pairs in the columns and the responses for the other of the pairs in the rows. For paired data, the grand total ‘N’ is always the number of pairs and not the total number of participants.\n\n\n\n\n\nTable 7.4: Table layout for testing matched proportions\n\n\n\n Negative at follow-upPositive at follow-upTotalNegative at baselineaba + bPositive at baselinecdc + dTotala + cb + dN\n\n\n\n\n\n\nWorked Example 7.3\nTwo drugs labelled A and B have been administered to patients in random order so that each patient acts as their own control. The dataset mod07_drug_response.rds is available on Moodle. The null hypothesis is as follows:\n\nH0: The proportion of patients who do better on drug A is the same as the proportion of patients who do better on drug B\n\nCounts and overall percentages are presented in . From the “Total” row in the table, we can see that the number of patients who respond to drug A is 41 (68%) and from the “Total” column the number who respond to drug B is less at 35 (58%), that is there is a difference of 10%.\n\nWorked Example 7.3: Paired data\n\n\n\n\n\n\n\n\n\nResponse to Drug B\nNo response to Drug B\nTotal\n\n\n\n\nResponse to Drug A\n21 (35%)\n20 (33%)\n41 (68%)\n\n\nNo response to Drug A\n14 (23%)\n5 (8%)\n19 (32%)\n\n\nTotal\n35 (58%)\n25 (42%)\n60 (100%)\n\n\n\nThe difference in the paired proportions is calculated using the simple equation:\n\\[ p_{A} - p_{B} = \\frac{(b - c)}{N} \\]\nHere, \\(p_{A} - p_{B} = \\frac{(20 - 14)}{60} = 0.1\\)\nThe cell counts show that 20 patients responded to Drug A but not to drug B, and 14 patients responded to Drug B but not to drug A. McNemar’s statistic is computed from these two discordant pairs (labelled as ‘b’ and ‘c’) as follows:\n\\[ X^2 = \\frac{(b-c)^2}{b+c} \\]\nwith 1 degree of freedom. Using our worked example, the McNemar’s chi-squared statistic is calculated as 1.06 with 1 degree of freedom, giving a P-value of 0.3.\nNote that some packages also calculate an “Exact P-Value”. The standard McNemar’s chi-squared statistic is generally recommended, unless the sum of the discordant cells is small (Kirkwood and Sterne define small as less than 10; Section 21.3, Kirkwood and Sterne 2001)). Here, \\(b + c = 34\\), so reporting the standard McNemar’s chi-squared statistic is appropriate.\nAs described above, the difference in proportions can be calculated. A 95% confidence interval for this difference can be obtained using statistical software.\nIn this study of 60 participants, where each participant received both drugs, 41 (68%) responded to Drug A and 35 (58%) responded to Drug B. The difference in the proportions responding is estimated as 10% (95% CI -11% to 31%). There is no evidence that the response differed between the two drugs (McNemar’s chi-square=1.06 with 1 degree of freedom, P=0.3).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#summary",
    "href": "07-testing-proportions.html#summary",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.5 Summary",
    "text": "7.5 Summary\nIn Module 6, we estimated proportions and measures of association for categorical data and conducted a one-sample test of proportions. In this module, we conduct significance tests for two or more independent proportions using the chi-squared test. The chi-squared test can also be used to conduct a significance test when there are more than two categories in both variables. The McNemar’s test is used when we have paired data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#pearsons-chi-squared-test-for-individual-level-data",
    "href": "07-testing-proportions.html#pearsons-chi-squared-test-for-individual-level-data",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.6 Pearson’s chi-squared test for individual-level data",
    "text": "7.6 Pearson’s chi-squared test for individual-level data\nConducting a Pearson’s chi-squared test is done automatically when producing a 2-by-2 table in jamovi. All that we need to do is check that the assumptions of the Pearson’s chi-squared test are met, by examining the Expected counts in each cell.\nWorked Example 7.1 is completed as in Module 6, but we first examine the Expected counts (remember that we need to change the order of the levels for the exposure and outcome variables):\n\nAfter confirming that there are no cells with small expected frequencies, we can interpret the chi-square test. The last section reports the chi-squared test statistic which has a value of 7.86 with 1 degree of freedom and a P-value of 0.005.\nIf there are small values of expected frequencies, Fisher’s exact test can be requested in the Statistics section:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#pearsons-chi-squared-test-for-summarised-data",
    "href": "07-testing-proportions.html#pearsons-chi-squared-test-for-summarised-data",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.7 Pearson’s chi-squared test for summarised data",
    "text": "7.7 Pearson’s chi-squared test for summarised data\nWhen you only have the summarised date (for example, the cross-tabulated data), you need to enter the summarised data manually as we did in Module 6. After defining the Count variable, the Pearson chi-squared test is calculated automatically.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-test-for-tables-larger-than-2-by-2",
    "href": "07-testing-proportions.html#chi-squared-test-for-tables-larger-than-2-by-2",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.8 Chi-squared test for tables larger than 2-by-2",
    "text": "7.8 Chi-squared test for tables larger than 2-by-2\nUse the data in mod07_allergy.rds. We use similar steps as described above for a 2-by-2 table:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#mcnemars-test-for-paired-proportions",
    "href": "07-testing-proportions.html#mcnemars-test-for-paired-proportions",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.9 McNemar’s test for paired proportions",
    "text": "7.9 McNemar’s test for paired proportions\nTo perform this test in jamovi, we will use the dataset mod07_drug_response.rds. As with all 2-by-2 tables, we should check that the variables are set up with the first level of the variable being the outcome of interest using Data &gt; Setup:\n\nBoth variables, druga and drugb are set up with “No” being the first level. We need to change the order and define “Yes” as the first level (See Module 6 jamovi notes on how to do this):\n\nThe McNemar’s test of paired proportions can be conducted at Analyses &gt; Frequencies &gt; Contingency Tables &gt; Paired Samples &gt; McNemar test:\n\nNote that jamovi does not currently provide an estimate of the difference in paired proportions, or a confidence interval for the difference.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#pearsons-chi-squared-test-for-individual-level-data-1",
    "href": "07-testing-proportions.html#pearsons-chi-squared-test-for-individual-level-data-1",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.10 Pearson’s chi-squared test for individual-level data",
    "text": "7.10 Pearson’s chi-squared test for individual-level data\nWe will demonstrate how to use R to conduct a Pearson chi-squared test using Worked Example 7.1.\n\nlibrary(jmv)\n\nnausea &lt;- readRDS(\"data/examples/mod06_nausea.rds\")\n\nhead(nausea)\n\n    group side_effect\n1 Placebo      Nausea\n2 Placebo      Nausea\n3 Placebo      Nausea\n4 Placebo      Nausea\n5 Placebo   No nausea\n6 Placebo   No nausea\n\nstr(nausea$group)\n\n Factor w/ 2 levels \"Placebo\",\"Active\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"label\")= chr \"Group\"\n\nstr(nausea$side_effect)\n\n Factor w/ 2 levels \"No nausea\",\"Nausea\": 2 2 2 2 1 1 1 1 1 1 ...\n - attr(*, \"label\")= chr \"Side effect\"\n\n\nThe columns group and side_effect have been entered as factors, with “Placebo” and “No nausea” as the first levels. We should use the relevel() command to re-order the factor levels.\n\nnausea$group &lt;- relevel(nausea$group, ref = \"Active\")\nnausea$side_effect &lt;- relevel(nausea$side_effect, ref = \"Nausea\")\n\nstr(nausea$group)\n\n Factor w/ 2 levels \"Active\",\"Placebo\": 2 2 2 2 2 2 2 2 2 2 ...\n\nstr(nausea$side_effect)\n\n Factor w/ 2 levels \"Nausea\",\"No nausea\": 1 1 1 1 2 2 2 2 2 2 ...\n\n\nAfter confirming the factors are appropriately defined, we can construct our 2-by-2 table and view the expected frequencies.\n\ncontTables(\n    data = nausea,\n    rows = group, cols = side_effect,\n    exp = TRUE\n)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                             \n ────────────────────────────────────────────────────────────── \n   group                  Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────── \n   Active     Observed           15           35           50   \n              Expected     9.500000     40.50000     50.00000   \n                                                                \n   Placebo    Observed            4           46           50   \n              Expected     9.500000     40.50000     50.00000   \n                                                                \n   Total      Observed           19           81          100   \n              Expected    19.000000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\nAfter confirming that there are no cells with small expected frequencies, we can interpret the chi-square test. The last section reports the chi-squared test statistic which has a value of 7.86 with 1 degree of freedom and a P-value of 0.005.\nIf there are small values of expected frequencies, Fisher’s exact test can be requested using fisher = TRUE:\n\ncontTables(\n    data = nausea,\n    rows = group, cols = side_effect,\n    fisher = TRUE\n)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                          \n ─────────────────────────────────────────── \n   group      Nausea    No nausea    Total   \n ─────────────────────────────────────────── \n   Active         15           35       50   \n   Placebo         4           46       50   \n   Total          19           81      100   \n ─────────────────────────────────────────── \n\n\n χ² Tests                                               \n ────────────────────────────────────────────────────── \n                          Value       df    p           \n ────────────────────────────────────────────────────── \n   χ²                     7.862248     1    0.0050478   \n   Fisher's exact test                      0.0094886   \n   N                           100                      \n ──────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#pearsons-chi-squared-test-for-summarised-data-1",
    "href": "07-testing-proportions.html#pearsons-chi-squared-test-for-summarised-data-1",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.11 Pearson’s chi-squared test for summarised data",
    "text": "7.11 Pearson’s chi-squared test for summarised data\nWhen you only have the summarised date (for example, the cross-tabulated data), you need to enter the summarised data manually. As we did in Module 6, the 2-by-2 table can be entered as four lines of data:\n\ndrug_aggregated &lt;- data.frame(\n    group = c(\"Active\", \"Active\", \"Placebo\", \"Placebo\"),\n    side_effect = c(\"Nausea\", \"No nausea\", \"Nausea\", \"No nausea\"),\n    n = c(15, 35, 4, 46)\n)\n\nThe contTables() function is used in the usual way, specifying count=n.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-test-for-tables-larger-than-2-by-2-1",
    "href": "07-testing-proportions.html#chi-squared-test-for-tables-larger-than-2-by-2-1",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.12 Chi-squared test for tables larger than 2-by-2",
    "text": "7.12 Chi-squared test for tables larger than 2-by-2\nUse the data in mod07_allergy.rds. We use similar steps as described above for a 2-by-2 table.\n\nallergy &lt;- readRDS(\"data/examples/mod07_allergy.rds\")\n\nhead(allergy)\n\n  id asthma hdmallergy catallergy infection    sex maternalasthma\n1  1     No        Yes         No       Yes Female             No\n2  2    Yes         No         No        No Female             No\n3  3    Yes         No         No        No Female             No\n4  4     No         No         No        No   Male             No\n5  4    Yes        Yes        Yes        No Female             No\n6  5    Yes        Yes        Yes        No Female             No\n  allergy_severity\n1 Moderate allergy\n2     Non-allergic\n3     Non-allergic\n4     Non-allergic\n5 Moderate allergy\n6 Moderate allergy\n\n\n\ncontTables(\n    data = allergy,\n    rows = allergy_severity, cols = sex,\n    pcCol = TRUE\n)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                                             \n ────────────────────────────────────────────────────────────────────────────── \n   allergy_severity                       Female       Male         Total       \n ────────────────────────────────────────────────────────────────────────────── \n   Non-allergic        Observed                 150          137          287   \n                       % within column     61.98347     53.10078     57.40000   \n                                                                                \n   Slight allergy      Observed                  50           70          120   \n                       % within column     20.66116     27.13178     24.00000   \n                                                                                \n   Moderate allergy    Observed                  27           32           59   \n                       % within column     11.15702     12.40310     11.80000   \n                                                                                \n   Severe allergy      Observed                  15           19           34   \n                       % within column      6.19835      7.36434      6.80000   \n                                                                                \n   Total               Observed                 242          258          500   \n                       % within column    100.00000    100.00000    100.00000   \n ────────────────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    4.308913     3    0.2299813   \n   N          500                      \n ─────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "07-testing-proportions.html#mcnemars-test-for-paired-proportions-1",
    "href": "07-testing-proportions.html#mcnemars-test-for-paired-proportions-1",
    "title": "7  Hypothesis testing for categorical data",
    "section": "7.13 McNemar’s test for paired proportions",
    "text": "7.13 McNemar’s test for paired proportions\nTo perform this test in R, we will use the dataset mod07_drug_response.rds.\n\ndrug &lt;- readRDS(\"data/examples/mod07_drug_response.rds\")\n\nhead(drug)\n\n  druga drugb\n1   Yes   Yes\n2   Yes   Yes\n3   Yes   Yes\n4   Yes   Yes\n5   Yes   Yes\n6   Yes   Yes\n\n\nAs usual, we should check that the variables being tabulated are factors, with the first level of the factor being the outcome of interest.\n\nstr(drug$druga)\n\n Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n - attr(*, \"label\")= chr \"Response to Drug A\"\n\nstr(drug$drugb)\n\n Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n - attr(*, \"label\")= chr \"Response to Drug B\"\n\n\nHere we see that the first level of the factor is “No” - we need to use the relevel() function to re-order the levels so “Yes” is the first level:\n\ndrug$druga &lt;- relevel(drug$druga, ref = \"Yes\")\ndrug$drugb &lt;- relevel(drug$drugb, ref = \"Yes\")\n\nstr(drug$druga)\n\n Factor w/ 2 levels \"Yes\",\"No\": 1 1 1 1 1 1 1 1 1 1 ...\n\nstr(drug$drugb)\n\n Factor w/ 2 levels \"Yes\",\"No\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWe can use the contTablesPaired() function within the jmv library to conduct McNemar’s test of paired proportions:\n\ncontTablesPaired(data = drug, rows = druga, cols = drugb)\n\n\n PAIRED SAMPLES CONTINGENCY TABLES\n\n Contingency Tables              \n ─────────────────────────────── \n   druga    Yes    No    Total   \n ─────────────────────────────── \n   Yes       21    20       41   \n   No        14     5       19   \n   Total     35    25       60   \n ─────────────────────────────── \n\n\n McNemar Test                          \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    1.058824     1    0.3034837   \n   N           60                      \n ───────────────────────────────────── \n\n\nNote that contTablesPaired() does not calculate an exact P-value.\nTo estimate the proportion in each of the paired samples, its difference, and the 95% confidence interval of the difference, we can use the mcNemarDiff() function which can be downloaded here.\n\n### Copied from gist.githubusercontent.com\nmcNemarDiff &lt;- function(data, var1, var2, digits = 3) {\n    if (!requireNamespace(\"epibasix\", quietly = TRUE)) {\n        stop(\"This function requires epibasix to be installed\")\n    }\n\n    tab &lt;- table(data[[var1]], data[[var2]])\n    p1 &lt;- (tab[1, 1] + tab[1, 2]) / sum(tab)\n    p2 &lt;- (tab[1, 1] + tab[2, 1]) / sum(tab)\n    pd &lt;- epibasix::mcNemar(tab)$rd\n    pd.cil &lt;- epibasix::mcNemar(tab)$rd.CIL\n    pd.ciu &lt;- epibasix::mcNemar(tab)$rd.CIU\n    print(paste0(\n        \"Proportion 1: \",\n        format(round(p1, digits = digits), nsmall = digits),\n        \"; Proportion 2: \", format(round(p2, digits = digits), nsmall = digits)\n    ))\n    print(paste0(\n        \"Difference in paired proportions: \",\n        format(round(pd, digits = digits), nsmall = digits),\n        \"; 95% CI: \", format(round(pd.cil, digits = digits), nsmall = digits),\n        \" to \", format(round(pd.ciu, digits = digits), nsmall = digits)\n    ))\n}\n### End copy\n\nmcNemarDiff(data = drug, var1 = \"druga\", var2 = \"drugb\", digits = 2)\n\n[1] \"Proportion 1: 0.68; Proportion 2: 0.58\"\n[1] \"Difference in paired proportions: 0.10; 95% CI: -0.11 to 0.31\"\n\n\nIn this study of 60 participants, where each participant received both drugs, 41 (68%) responded to Drug A and 35 (58%) responded to Drug B. The difference in the proportions responding is estimated as 10% (95% CI -11% to 31%). There is no evidence that the response differed between the two drugs (McNemar’s chi-squared = 1.06 with 1df, P=0.30).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis testing for categorical data</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html",
    "href": "08-correlation-and-regression.html",
    "title": "8  Correlation and simple linear regression",
    "section": "",
    "text": "Learning objectives\nBy the end of this module you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#learning-objectives",
    "href": "08-correlation-and-regression.html#learning-objectives",
    "title": "8  Correlation and simple linear regression",
    "section": "",
    "text": "Explore the association between two continuous variables using a scatter plot;\nEstimate and interpret correlation coefficients;\nEstimate and interpret parameters from a simple linear regression;\nAssess the assumptions of simple linear regression;\nTest a hypothesis using regression coefficients.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#optional-readings",
    "href": "08-correlation-and-regression.html#optional-readings",
    "title": "8  Correlation and simple linear regression",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapter 10. [UNSW Library Link]\nBland (2015); Chapter 11. [UNSW Library Link]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#introduction",
    "href": "08-correlation-and-regression.html#introduction",
    "title": "8  Correlation and simple linear regression",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nIn Module 5, we saw how to test whether the means from two groups are equal - in other words, whether a continuous variable is related to a categorical variable. Sometimes we are interested in how closely two continuous variables are related. For example, we may want to know how closely blood cholesterol levels are related to dietary fat intake in adult men. To measure the strength of association between two continuously distributed variables, a correlation coefficient is used.\nWe may also want to predict a value of a continuous measurement from another continuous measurement. For example, we may want to know predict values of lung capacity from height in a community of adults. A regression model allows us to use one measurement to predict another measurement.\nAlthough both correlation coefficients and regression models can be used to describe the degree of association between two continuous variables, the two methods provide different information. It is important to note that both methods summarise the strength of an association between variables, and do not imply a causal relationship.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#notation",
    "href": "08-correlation-and-regression.html#notation",
    "title": "8  Correlation and simple linear regression",
    "section": "8.2 Notation",
    "text": "8.2 Notation\nIn this module, we will be focussing on the association between two variables, denoted \\(x\\) and \\(y\\).\nThere may be cases where it does not matter which variable is denoted \\(x\\) and which is denoted \\(y\\), however this is rare. We are usually interested in whether one variable is associated with another. If we believe that a change in \\(x\\) will lead to a change in \\(y\\), or that \\(y\\) is influenced by \\(x\\), we define \\(y\\) as the outcome variable and \\(x\\) as the explanatory variable.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#correlation",
    "href": "08-correlation-and-regression.html#correlation",
    "title": "8  Correlation and simple linear regression",
    "section": "8.3 Correlation",
    "text": "8.3 Correlation\nWe use correlation to measure the strength of a linear relationship between two variables. Before calculating a correlation coefficient, a scatter plot should first be obtained to give an understanding of the nature of the relationship between the two variables.\n\nWorked Example\nThe file mod08_lung_function.csv has information about height and lung function collected from a random sample of 120 adults. Information was collected on height (cm) and lung function, which was measured as forced vital capacity (FVC), measured in litres. We can obtain a scatter-plot shown in Figure 8.1, where the outcome variable (\\(y\\)) is plotted on the vertical axis, and the explanatory variable (\\(x\\)) is plotted on the horizontal axis.\nFigure 8.1 shows that as height increases, lung function also increases, which is as expected. One or two of the data points are separated from the rest of the data but are not so far away as to be considered outliers because they do not seem to stand out of other observations.\n\n\n\n\n\n\n\n\nFigure 8.1: Association between height and lung function in 120 adults\n\n\n\n\n\n\n\nCorrelation coefficients\nA correlation coefficient (r) describes how closely the variables are related, that is the strength of linear association between two continuous variables. The range of the coefficient is from +1 to −1 where +1 is a perfect positive association, 0 is no association and −1 is a perfect inverse association. In general, an absolute (disregarding the sign) r value below 0.3 indicates a weak association, 0.3 to &lt; 0.6 is fair association, 0.6 to &lt; 0.8 is a moderate association, and \\(\\ge\\) 0.8 indicates a strong association.\nThe correlation coefficient is positive when large values of one variable tend to occur with large values of the other, and small values of one variable (y) tend to occur with small values of the other (x) (Figure 8.2 (a and b)). For example, height and weight in healthy children or age and blood pressure.\nThe correlation coefficient is negative when large values of one variable tend to occur with small values of the other, and small values of one variable tend to occur with large values of the other (Figure 8.2 (c and d)). For example, percentage immunised against infectious diseases and under-five mortality rate.\n\n\n\n\n\n\n\n\nFigure 8.2: Scatter plots demonstrating strong and weak, positive and negative associations\n\n\n\n\n\nIt is possible to calculate a P-value associated with a correlation coefficient to test whether the correlation coefficient is different from zero. However, a correlation coefficient with a large P-value does not imply that there is no relationship between \\(x\\) and \\(y\\), because the correlation coefficient only tests for a linear association and there may be a non-linear relationship such as a curved or irregular relationship.\nThe assumptions for using a Pearson’s correlation coefficient are that:\n\nobservations are independent;\nboth variables are continuous variables;\nthe relationship between the two variables is linear.\n\nThere is a further assumption that the data follow a bivariate normal distribution. This assumes: y follows a normal distribution for given values of x; and x follows a normal distribution for given values of y. This is quite a technical assumption that we do not discuss further.\nThere are two types of correlation coefficients– the correct one to use is determined by the nature of the variables as shown in Table 8.1.\n\n\n\n\nTable 8.1: Correlation coefficients and their application\n\n\n\nCorrelation coefficientApplicationPearson’s correlation coefficient: rBoth variables are continuous and a bivariate normal distribution can be assumedSpearman’s rank correlation: ρBivariate normality cannot be assumed. Also useful when at least one of the variables is ordinal\n\n\n\n\n\nSpearman’s \\(\\rho\\) is calculated using the ranks of the data, rather than the actual values of the data. We will see further examples of such methods in Module 9, when we consider non-parametric tests, which are often based on ranks.\nCorrelation coefficients are often presented in the form of a correlation matrix which can display the correlation between a number of variables in a single table (Table 8.2).\n\n\n\nTable 8.2: Correlation matrix for Height and FVC\n\n\n\n\n\n\n\n\n\n\n\nHeight\nFVC\n\n\nHeight\n1\n0.70\nP &lt; 0.0001\n\n\nFVC\n0.70\nP &lt; 0.0001\n1\n\n\n\n\n\n\nThis correlation matrix shows that the Pearson’s correlation coefficient between height and lung function is 0.70 with P&lt;0.0001 indicating very strong evidence of a linear association between height and FVC. A correlation matrix sometimes includes correlations between the same variable, indicated as a correlation coefficient of 1. For example, \\(Height\\) is perfectly correlated with itself (i.e. has a correlation coefficient of 1). Similarly, \\(FVC\\) is perfectly correlated with itself.\nCorrelation coefficients are rarely used as important statistics in their own right because they do not fully explain the relationship between the two variables and the range of the data has an important influence on the size of the coefficient. In addition, the statistical significance of the correlation coefficient is often over interpreted because a small correlation which is of no clinical importance can become statistically significant even with a relatively small sample size. For example, a poor correlation of 0.3 will be statistically significant if the sample size is large enough.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#linear-regression",
    "href": "08-correlation-and-regression.html#linear-regression",
    "title": "8  Correlation and simple linear regression",
    "section": "8.4 Linear regression",
    "text": "8.4 Linear regression\nThe nature of a relationship between two variables is more fully described using regression, where the relationship is described by a straight line.\nFigure 8.3 shows our lung data with a fitted regression line.\n\n\n\n\n\n\n\n\nFigure 8.3: Association between height and lung function in 120 adults\n\n\n\n\n\nThe line through the plot is called the line of ‘best fit’ because the size of the deviations between the data points and the line is minimised in estimating the line.\n\nRegression equations\nThe mathematical equation for the line explains the relationship between two variables: \\(y\\), the outcome variable, and \\(x\\), the explanatory variable. The equation of the regression line is as follows:\n\\[y = \\beta_{0} + \\beta_{1}x\\]\nThis line is shown in Figure 8.4 using the notation shown in Table 8.3.\n\n\n\n\n\n\n\n\nFigure 8.4: Coefficients of a linear regression equation\n\n\n\n\n\n\n\n\n\nTable 8.3: Notation for linear regression equation\n\n\n\nSymbolInterpretationyThe outcome variablexThe explanatory variableβ0Intercept of the regression lineβ1Slope of the regression line\n\n\n\n\n\nThe intercept is the point at which the regression line intersects with the y-axis when the value of \\(x\\) is zero. In most cases, the intercept does not have a biologically meaningful interpretation as the explanatory variable cannot take a value of zero. In our working example, the intercept is not meaningful as it is not possible for an adult to have a height of 0cm.\nThe slope of the line is the predicted change in the outcome variable \\(y\\) as the explanatory explanatory variable \\(x\\) increases by 1 unit.\nAn important concept is that regression predicts an expected value of \\(y\\) given an observed value of \\(x\\): any error around the explanatory variable is not taken into account.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#regression-coefficients-estimation",
    "href": "08-correlation-and-regression.html#regression-coefficients-estimation",
    "title": "8  Correlation and simple linear regression",
    "section": "8.5 Regression coefficients: estimation",
    "text": "8.5 Regression coefficients: estimation\nThe regression parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are true, unknown quantities (similar to \\(\\mu\\) and \\(\\sigma\\)), which are estimated using statistical software using the method of least squares. This method estimates the intercept and the slope, and also their variability (i.e. standard errors). Software is always used to estimate the regression parameters from a set of data.\nUsing the method of least squares:\n\nthe intercept is estimated as \\(b_0\\);\nthe slope is estimated as \\(b_1\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#regression-coefficients-inference",
    "href": "08-correlation-and-regression.html#regression-coefficients-inference",
    "title": "8  Correlation and simple linear regression",
    "section": "8.6 Regression coefficients: inference",
    "text": "8.6 Regression coefficients: inference\nWe can use the estimated regression coefficients and their variability to calculate 95% confidence intervals. Here, a t-value from a t-distribution with \\(n - 2\\) degrees of freedom is used:\n\n95% confidence interval for intercept: \\(b_0 \\pm t_{n-2} \\times SE(b_0)\\)\n95% confidence interval for slope: \\(b_1 \\pm t_{n-2} \\times SE(b_1)\\)\n\nNote that as the constant (\\(b_0\\)) is not often biologically plausible, the 95% confidence interval for the constant is often not reported.\nThe significance of the estimated slope (and less commonly, intercept) can be tested using a t-test. The null hypotheses and the alternative hypothesis for testing the slope of a simple linear regression model are:\n\nH0: \\(\\beta_1 = 0\\)\nH1: \\(\\beta_1 \\ne 0\\)\n\nTo test the null hypothesis for the regression coefficient \\(\\beta_1\\), the following t-test is used:\n\\[t = b_1 /SE(b_1)\\]\nThis will give a t statistic which can be referred to a t distribution with n − 2 degrees of freedom to calculate the corresponding P-value.\nTable 8.4 shows the estimated regression coefficients for our working example.\n\n\n\n\nTable 8.4: Estimated regression coefficients\n\n\n\nTermEstimateStandard errort valueP value95% Confidence intervalIntercept-18.872.194t=-8.60, 118df&lt;0.001-23.22 to -14.53Height0.140.013t=10.58, 118df&lt;0.0010.11 to 0.17\n\n\n\n\n\nFrom this output, we see that the slope is estimated as 0.14 with an estimated intercept of -18.87. Therefore, the regression equation is estimated as:\nFVC (L) = − 18.87 + (0.14 \\(\\times\\) Height in cm)\nThere is very strong evidence of a linear association between FVC and height in cm (P &lt; 0.001).\nThis equation can be used to predict FVC for a person of a given height. For example, the predicted FVC for a person 165 cm tall is estimated as:\nFVC = − 18.87347 + (0.1407567 \\(\\times\\) 165.0) = 4.40 L.\nNote that for the purpose of prediction we have kept all the decimal places in the coefficients to avoid rounding error in the intermediate calculation.\n\nFit of a linear regression model\nAfter fitting a linear regression model, it is important to know how well the model fits the observed data. One way of assessing the model fit is to compute a statistic called coefficient of determination, denoted by \\(R^2\\). It is the square of the Pearson correlation coefficient \\(r: r^2 = R^2\\). Since the range of \\(r\\) is from −1 to 1, \\(R^2\\) must lie between 0 and 1.\n\\(R^2\\) can be interpreted as the proportion of variability in y that can be explained by variability in x. Hence, the following conditions may arise:\nIf \\(R^2 = 1\\), then all variation in y can be explained by variation of x and all data points fall on the regression line.\nIf \\(R^2 = 0\\), then none of the variation in y is related to x at all, and the variable x explains none of the variability in y.\nIf \\(0 &lt; R^2 &lt;1\\), then the variability of y can be partially explained by the variability in x. The larger the \\(R^2\\) value, the better is the fit of the regression model.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#assumptions-for-linear-regression",
    "href": "08-correlation-and-regression.html#assumptions-for-linear-regression",
    "title": "8  Correlation and simple linear regression",
    "section": "8.7 Assumptions for linear regression",
    "text": "8.7 Assumptions for linear regression\nRegression is robust to moderate degrees of non-normality in the variables, provided that the sample size is large enough and that there are no influential outliers. Also, the regression equation describes the relationship between the variables and this is not influenced as much by the spread of the data as the correlation coefficient is.\nThe assumptions that must be met when using linear regression are as follows:\n\nobservations are independent;\nthe relationship between the explanatory and the outcome variable is linear;\nthe residuals are normally distributed.\n\nA residual is defined as the difference between the observed and predicted outcome from the regression model. If the predicted value of the outcome variable is denoted by \\(\\hat y\\) then:\n\\[ \\text{Residual} = \\text{observed} - \\text{predicted} = y - \\hat y\\]\nIt is important for regression modelling that the data are collected in a period when the relationship remains constant. For example, in building a model to predict normal values for lung function the data must be collected when the participants have been resting and not exercising and people taking bronchodilator medications that influence lung capacity should be excluded. In regression, it is not so important that the variables themselves are normally distributed, but it is important that the residuals are. Scatter plots and specific diagnostic tests can be used to check the regression assumptions. Some of these will not be covered in this introductory course but will be discussed in detail in the Regression Methods in Biostatistics course.\nThe distribution of the residuals should always be checked. Large residuals can indicate unusual points or points that may exert undue influence on the estimated regression slope.\nThe distribution of the residuals from the model is shown in Figure 8.5. The residuals are approximately normally distributed, with no outlying values.\n\n\n\n\n\n\n\n\nFigure 8.5: Distribution of regression residuals",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#multiple-linear-regression",
    "href": "08-correlation-and-regression.html#multiple-linear-regression",
    "title": "8  Correlation and simple linear regression",
    "section": "8.8 Multiple linear regression",
    "text": "8.8 Multiple linear regression\nIn the above example, we have only used a simple linear regression model of two continuous variables. Other more complex models can be built from this e.g. if we wanted to look at the effect of gender (male vs. female) as binary indicator in the model while adjusting for the effect of height. In that case we would include both the variables in the model as explanatory variables. In the same way we can include any number of explanatory variables (both continuous and categorical) in the model: this is called a multivariable model. Multivariable models are often used for building predictive equations, for example by using age, height, gender and smoking history to predict lung function, or to adjust for confounding and detect effect modification to investigate the association between an exposure and an outcome factor.\nMultiple regression has an important role in investigating causality in epidemiology. The exposure variable under investigation must stay in the model and the effects of other variables which can be confounders or effect-modifiers are tested. The biological, psychological or social meaning of the variables in the model and their interactions are of great importance for interpreting theories of causality.\nOther multivariable models include binary logistic regression for use with a binary outcome variable, or Cox regression for survival analyses. These models, together with multiple regression, will be taught in PHCM9517: Regression Methods in Biostatistics.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#creating-a-scatter-plot",
    "href": "08-correlation-and-regression.html#creating-a-scatter-plot",
    "title": "8  Correlation and simple linear regression",
    "section": "8.9 Creating a scatter plot",
    "text": "8.9 Creating a scatter plot\nWe will demonstrate using Jamovi for correlation and simple linear regression using the dataset mod08_lung_function.csv.\nTo create a scatter plot to explore the association between height and FVC, we use Scatterplot within the Exploration menu:\n\n\n\n\n\n\n\n\n\nChoose Height for the X-Axis, and FVC for the Y-Axis, and the scatter-plot appears in the Output window:\n\n\n\n\n\n\n\n\n\nTo add a fitted line, click the Linear option in the Regression Line section:\n\n\n\n\n\n\n\n\n\nTo save your graph, right-click the graph and choose Image &gt; Export, and be sure to save your file as a PNG file:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#calculating-a-correlation-coefficient",
    "href": "08-correlation-and-regression.html#calculating-a-correlation-coefficient",
    "title": "8  Correlation and simple linear regression",
    "section": "8.10 Calculating a correlation coefficient",
    "text": "8.10 Calculating a correlation coefficient\nTo calculate the Pearson’s correlation using the dataset mod08_lung_function.csv choose: Correlation Matrix within the Regression menu.\nSelect the two variables, FVC and Height in the Variables box, and a correlation matrix will be constructed, similar to Table 8.2.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model",
    "href": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model",
    "title": "8  Correlation and simple linear regression",
    "section": "8.11 Fitting a simple linear regression model",
    "text": "8.11 Fitting a simple linear regression model\nTo fit a simple linear regression model, choose Regression &gt; Linear Regression\nSelect FVC as the Dependent variable, and Height as a Covariate (Jamovi refers to continuous explanatory variables as covariates). To obtain the 95% confidence interval for the regression coefficients, scroll down to the Model Coefficients section and click the Confidence interval option for Estimate.\n\n\n\n\n\n\n\n\n\nYou will notice that the Jamovi output does not provide the degrees of freedom for the regression coefficient t-statistic. This is equivalent to the degrees of freedom in the preceding correlation matrix: in this case, 118 df.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression",
    "href": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression",
    "title": "8  Correlation and simple linear regression",
    "section": "8.12 Plotting residuals from a simple linear regression",
    "text": "8.12 Plotting residuals from a simple linear regression\nTo plot the residuals, we first need to save them using the Save option within the Linear Regression command box:\n\n\n\n\n\n\n\n\n\nThis creates a new column of Residuals within our dataset:\n\n\n\n\n\n\n\n\n\nYou can now check the assumption that the residuals are normally distributed by creating a density plot of the residuals using Exploration &gt; Descriptives, as shown in Figure 8.5.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#creating-a-scatter-plot-1",
    "href": "08-correlation-and-regression.html#creating-a-scatter-plot-1",
    "title": "8  Correlation and simple linear regression",
    "section": "8.13 Creating a scatter plot",
    "text": "8.13 Creating a scatter plot\nWe can use the plot function to create a scatter plot to explore the association between height and FVC, assigning meaningful labels with the xlab and ylab commands:\n\nplot(x=lung$Height, y=lung$FVC, \n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\")\n\n\n\n\n\n\n\n\nTo add a fitted line, we can use the abline() function which adds a straight line to the plot. The equation of this straight line will be determined from the estimated regression line, which we specify with the lm() function, which fits a linear model.\nThe basic syntax of the lm() function is: lm(y ~ x) where y represents the outcome variable, and x represents the explanatory variable. Putting this all together:\n\nplot(x=lung$Height, y=lung$FVC,\n     xlab=\"Height (cm)\",\n     ylab=\"Forced vital capacity (L)\")\n\nabline(lm(lung$FVC ~ lung$Height))\n\n\n\n\n\n\n\n\nNote: to obtain output similar to Jamovi output, we can use the scatr package:\n\n# Install scatr if required:\n# install.package(\"scatr\")\nlibrary(scatr)\n\nscat(data=lung, x=\"Height\", y=\"FVC\", line=\"linear\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#calculating-a-correlation-coefficient-1",
    "href": "08-correlation-and-regression.html#calculating-a-correlation-coefficient-1",
    "title": "8  Correlation and simple linear regression",
    "section": "Calculating a correlation coefficient",
    "text": "Calculating a correlation coefficient\nWe can use the corrMatrix function in the Jamovi package to calculate a Pearson’s correlation coefficient:\n\ncorrMatrix(data=lung, vars=c(Height, FVC))\n\n\n CORRELATION MATRIX\n\n Correlation Matrix                                   \n ──────────────────────────────────────────────────── \n                            Height        FVC         \n ──────────────────────────────────────────────────── \n   Height    Pearson's r             —                \n             df                      —                \n             p-value                 —                \n                                                      \n   FVC       Pearson's r     0.6976279            —   \n             df                    118            —   \n             p-value        &lt; .0000001            —   \n ────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model-1",
    "href": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model-1",
    "title": "8  Correlation and simple linear regression",
    "section": "8.14 Fitting a simple linear regression model",
    "text": "8.14 Fitting a simple linear regression model\nWe can use the lm function to fit a simple linear regression model, specifying the model as y ~ x where y represents the outcome variable, and x represents the explanatory variable. Using mod08_lung_function.csv, we can quantify the relationship between FVC and height:\n\nlm(FVC ~ Height, data=lung)\n\n\nCall:\nlm(formula = FVC ~ Height, data = lung)\n\nCoefficients:\n(Intercept)       Height  \n   -18.8735       0.1408  \n\n\nThe default output from the lm function is rather sparse. We can obtain much more useful information by defining the linear regression model as an object, then using the summary() function:\n\nmodel &lt;- lm(FVC ~ Height, data=lung)\nsummary(model)\n\n\nCall:\nlm(formula = FVC ~ Height, data = lung)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.01139 -0.23643 -0.02082  0.24918  1.31786 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -18.87347    2.19365  -8.604 3.89e-14 ***\nHeight        0.14076    0.01331  10.577  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3965 on 118 degrees of freedom\nMultiple R-squared:  0.4867,    Adjusted R-squared:  0.4823 \nF-statistic: 111.9 on 1 and 118 DF,  p-value: &lt; 2.2e-16\n\n\nFinally, we can obtain 95% confidence intervals for the regression coefficients using the confint function:\n\nconfint(model)\n\n                  2.5 %      97.5 %\n(Intercept) -23.2174967 -14.5294441\nHeight        0.1144042   0.1671092\n\n\nNote that the output for R looks slightly different from the Jamovi output, the numerical values are identical.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression-1",
    "href": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression-1",
    "title": "8  Correlation and simple linear regression",
    "section": "8.15 Plotting residuals from a simple linear regression",
    "text": "8.15 Plotting residuals from a simple linear regression\nWe can use the resid function to obtain the residuals from a saved model. These residuals can then be plotted using a density plot in the usual way:\n\nresiduals &lt;- resid(model)\nplot(density(residuals))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html",
    "href": "09-non-parametrics.html",
    "title": "9  Analysing non-normal data",
    "section": "",
    "text": "Learning objectives\nBy the end of this module you will be able to:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#learning-objectives",
    "href": "09-non-parametrics.html#learning-objectives",
    "title": "9  Analysing non-normal data",
    "section": "",
    "text": "Transform non-normally distributed variables;\nExplain the purpose of non-parametric statistics and key principles for their use;\nCalculate ranks for variables;\nConduct and interpret a non-parametric independent samples significance test;\nConduct and interpret a non-parametric paired samples significance test;\nCalculate and interpret the Spearman rank correlation coefficient.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#optional-readings",
    "href": "09-non-parametrics.html#optional-readings",
    "title": "9  Analysing non-normal data",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapter 13. [UNSW Library Link]\nBland (2015); Chapter 12. [UNSW Library Link]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#introduction",
    "href": "09-non-parametrics.html#introduction",
    "title": "9  Analysing non-normal data",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nIn general, parametric statistics are preferred for reporting data because the summary statistics (mean, standard deviation, standard error of the mean etc) and the tests used (t-tests, correlation, regression etc) are familiar and the results are easy to communicate. However, non-parametric tests can be used if data are not normally distributed. Non-parametric tests make fewer assumptions about the distribution of the data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#transforming-non-normally-distributed-variables",
    "href": "09-non-parametrics.html#transforming-non-normally-distributed-variables",
    "title": "9  Analysing non-normal data",
    "section": "9.2 Transforming non-normally distributed variables",
    "text": "9.2 Transforming non-normally distributed variables\nWhen a variable has a skewed distribution, one possibility is to transform the data to a new variable to try and obtain a normal or near normal distribution. Methods to transform non-normally distributed data include logarithmic transformation of each data point, or using the square root or the square or the inverse (i.e. 1/x) etc.\n\nWorked Example\nWe have data from 132 patients who had a hospital stay following admission to ICU available on Moodle (mod09_infection.rds). The distribution of the length of stay for these patients is shown in the density plot in Figure 9.1. As is common with variables that record time, the data are skewed with many patients having relatively short stays and a few patients having very long hospital stays. Clearly, it would be inappropriate to use parametric statistical methods for these data.\n\n\n\n\n\n\n\n\nFigure 9.1: Length of hospital stay for 132 patients\n\n\n\n\n\nWhen data are positively skewed, as shown in Figure 9.1, a logarithmic transformation can often make the data closer to being normally distributed. This is the most common transformation used. You should note, however, that the logarithmic function cannot handle 0 or negative values. One way to deal with zeros in a set of data is to add 1 to each value before taking the logarithm.\nWe would generate a new variable, as shown in the jamovi or R notes. As the minimum length of stay in these sample data was 0, we have added 1 to each length of stay before taking the logarithm. The distribution of the logarithm of (length of stay + 1) is shown in Figure 9.2.\n\n\n\n\n\n\n\n\nFigure 9.2: Distribution of log transformed (length of stay + 1)\n\n\n\n\n\nThe distribution now appears much more bell shaped. Table 9.1 shows the descriptive statistics for length of stay before and after logarithmic transformation. Before transformation, the SD is almost as large as the mean value which indicates that the data are skewed and that these statistics are not an accurate description of the centre and spread of the data.\n\n\n\nTable 9.1: Summary statistics for untransformed and transformed length of stay\n\n\n\n\n\n\n\n\n\n\n\nLength of stay\nlog(Length of stay + 1)\n\n\n\n\nMean (Standard deviation)\n38.1 (35.78)\n3.41 (0.715)\n\n\nMean: 95% confidence interval\n31.9 to 44.2\n3.29 to 3.53\n\n\nMedian [Interquartile range]\n27 [21 to 42]\n3.3 [3.1 to 3.8]\n\n\nRange\n0 to 244\n0 to 5.5\n\n\n\n\n\n\nThe mean and standard deviation of the transformed length of stay are in log base e (i.e. ln) units. If we raise the mean of the log of length of stay to the power of \\(e\\), it returns a value of 30.2 days (\\(e^{3.41}=30.2\\)).\nTechnically, this is called the geometric mean of the data, and it has a different interpretation to the usual mean, the arithmetic mean. This is a much better estimate in this case of the “average” length of stay than the mean of 38.1 days (95% CI 31.9, 44.2 days) obtained from the non-transformed positively skewed data. Note that, if you have added 1 to your data to deal with 0 values, the back-transformed estimate is approximately equal to the geometric mean.\nThis set of data also includes a variable summarising whether a patient acquired a nosocomial infection (also known as healthcare-associated infections), which are infections that develop while undergoing medical treatment but were absent at the time of admission.\nIf we were testing the hypothesis that there was a difference in length of stay between groups (status of nosocomial infection), t-tests should not be used with length of stay, but could be used for the log transformed variable, which is approximately normally distributed. The output from the t-test of the log-transformed length of stay is shown in Table 9.2. This is done using the t-test shown in Module 5.\n\n\n\nTable 9.2: Summary statistics for transformed length of stay\n\n\n\n\n\nNosocomial infection\nn\nMean (SE)\n95% Confidence interval\n\n\n\n\nNo\n106\n3.33 (0.068)\n3.19 to 3.46\n\n\nYes\n26\n3.73 (0.136)\n3.45 to 4.01\n\n\nDifference (Yes - No)\n\n0.39 (0.153)\n0.09 to 0.70\n\n\n\n\n\n\nHere, a two-sample t-test gives a test statistic of 2.59 with 130 degrees of freedom, and a P-value of 0.01.\nAs explained above, the estimated statistics would need to be converted back to the units in which the variable was measured. From Table 9.2, we can take the exponential of the corresponding log-transformed values:\n\nthe geometric mean of the infected group is approximately 41.5 days with a 95% confidence interval from 31.4 to 55.0 days.\nthe geometric mean of the uninfected group is approximately 27.9 days with a 95% confidence interval from 24.4 to 31.9 days.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-significance-tests",
    "href": "09-non-parametrics.html#non-parametric-significance-tests",
    "title": "9  Analysing non-normal data",
    "section": "9.3 Non-parametric significance tests",
    "text": "9.3 Non-parametric significance tests\nIt is often not possible or sensible to transform a non-normal distribution, for example if there are too many zero values or when we simply want to compare groups using the unit in which the measurement was taken (e.g. length of stay). For this, non-parametric significance tests can be used but the general idea behind these tests is that the data values are replaced by ranks. This also protects against outliers having too much influence.\n\nRanking variables\nTable 9.3 shows how ranks are calculated for the first 21 patients in the length-of-stay data. First the data are sorted in order of their magnitude (from the lowest value to the highest) ignoring the group variable. Each data point is then assigned a rank. Data points that are equal are assigned the mean of their ranks. Thus, the two lengths of stay of 11 days share the ranks 4 and 5, and have a mean rank of 4.5. Similarly, there are 5 people with a length of stay of 14 days and these share the ranks 9 to 13, the mean of which is 11. Once ranks are computed they are assigned to each of the two groups and summed within each group.\n\n\n\n\nTable 9.3: Transforming data to ranks (first 21 participants)\n\n\n\nIDInfectionLength of stayRankInfection=noInfection=Yes32No01.01.033No12.02.012No93.03.022No114.54.516No114.54.528Yes126.06.027No137.57.520No137.57.524No1411.011.011No1411.011.0130No1411.011.010No1411.011.025No1411.011.019No1515.515.530No1515.515.523No1515.515.514No1515.515.515No1720.520.513No1720.520.521Yes1720.520.517No1720.520.5\n\n\n\n\n\nBy assigning ranks to individuals, we lose information about their actual values and this makes it more difficult to detect a difference. However, outliers and extreme values in the data are brought back closer to the data so that they are less influential. For this reason, non-parametric tests have less power than parametric tests and they require much larger differences in the data to show statistical significance between groups.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-test-for-two-independent-samples-wilcoxon-ranked-sum-test",
    "href": "09-non-parametrics.html#non-parametric-test-for-two-independent-samples-wilcoxon-ranked-sum-test",
    "title": "9  Analysing non-normal data",
    "section": "9.4 Non-parametric test for two independent samples (Wilcoxon ranked sum test)",
    "text": "9.4 Non-parametric test for two independent samples (Wilcoxon ranked sum test)\nThe non-parametric equivalent to an independent samples t-test (Module 5) is the Wilcoxon ranked sum test, also known as the Mann-Whitney U test. This can be obtained using the Mann-Whitney U option in jamovi, and the wilcox.test in R.\nThe assumption for this test is that the distributions of the two populations have the same general shape. If this assumption is met, then this test evaluates the null hypothesis that the medians of the two populations are equal. This test does not assume that the populations are normally distributed, nor that their variances are equal.\nConducting the Wilcoxon ranked sum test for our length of stay data yields a P-value of 0.014, providing evidence of a difference in the median length of stay between the groups.\nThis P-value should be provided alongside non-parametric summary statistics such as medians and inter-quartile ranges. In our example, we can obtain the median length of stay values of 24 (Interquartile Range: 19 to 40 days) in the group with no infection and 37 (Interquartile Range: 24 to 50 days) in the group with infection.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-test-for-paired-data-wilcoxon-signed-rank-test",
    "href": "09-non-parametrics.html#non-parametric-test-for-paired-data-wilcoxon-signed-rank-test",
    "title": "9  Analysing non-normal data",
    "section": "9.5 Non-parametric test for paired data (Wilcoxon signed-rank test)",
    "text": "9.5 Non-parametric test for paired data (Wilcoxon signed-rank test)\nThere are two types of non-parametric tests for paired data, called the Sign test and the Wilcoxon signed rank test. In practice, the Sign test is rarely used and will not be discussed in this course.\nIf the differences between two paired measurements are not normally distributed, a non-parametric equivalent of a paired t-test (Module 5) should be used. The equivalent test is the Wilcoxon matched-pairs signed rank test, also simply called the Wilcoxon matched-pairs test. This test is resistant to outliers in the data, however the proportion of outliers in the sample should be small. This test evaluates the null hypothesis that the median of the paired differences is equal to zero.\nIn this test, the absolute differences between the paired scores are ranked and the difference scores that are equal to zero (i.e. scores where there is no difference between the pairs) are excluded. Note that the power of the test (the ability to detect an effect if there truly is an effect) reduces in the presence of zero differences, as the effective sample size (the number of non-zero differences) is reduced.\n\nWorked Example\nA crossover trial is done to compare symptom scores for two drugs in 11 people with arthritis (higher scores indicate more severe symptoms). The data are contained in datafile file mod09_arthritis.csv. The data are shown in Table 9.4.\n\n\n\n\nTable 9.4: Arthritis symptom scores for 11 patients after administering two drugs\n\n\n\nPatient IDScore: Drug 1Score: Drug 2Difference (Drug 2 - Drug 1)134122753341481025682661-572648374958310910111781\n\n\n\n\n\nThe data shows that there is 1 person who has a negative difference, where the symptom score on drug 2 that is smaller than that for drug 1 (i.e., drug 2 is better than drug 1); and 10 people who have a positive difference. No one has the same score for both drugs.\nBefore doing the analysis let us examine the distribution of the difference of symptom scores between the two drugs. As in Module 5, we first need to compute the difference between the symptom scores. To examine the distribution, we construct a distribution plot as shown in Figure 9.3.\n\n\n\n\n\n\n\n\nFigure 9.3: Distribution of difference in symptom scores between Drug 1 and Drug 2\n\n\n\n\n\nThe plot shows that the differences are not normally distributed. The data looks weirdly negatively skewed with a gap in values around -2.5. Therefore, it would not be appropriate to conduct a paired t-test. Hence, we conduct a non-parametric paired test (Wilcoxon matched-pairs signed-rank test).\nThe P-value obtained from this test is 0.049. Thus, there is evidence of a difference in symptom scores between the two drugs.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-estimates-of-correlation",
    "href": "09-non-parametrics.html#non-parametric-estimates-of-correlation",
    "title": "9  Analysing non-normal data",
    "section": "9.6 Non-parametric estimates of correlation",
    "text": "9.6 Non-parametric estimates of correlation\nEstimating correlation using Pearson’s correlation coefficient can be problematic when bivariate Normality cannot be assumed, or in the presence of outliers or skewness. There are two commonly used non-parametric alternatives to Pearson’s correlation coefficient: Spearman’s rank correlation (\\(\\rho\\) or rho), and Kendall’s rank correlation (\\(\\tau\\) or tau).\nWhen estimating the correlation between x and y, Spearman’s rank correlation essentially replaces the observations x and y by their ranks, and calculates the correlation between the ranks. Kendall’s rank correlation compares the ranks between every possible combination of pairs of data to measure concordance: whether high values for x tend to be associated with high values for y (positively correlated) or low values of y (negatively correlated).\nIn terms of which is the more appropriate measure to use, the following passage from An Introduction to Medical Statistics (Bland (2015)) provides some guidance:\n\n“Why have two different rank correlation coefficients? Spearman’s \\(\\rho\\) is older than Kendall’s \\(\\tau\\), and can be thought of as a simple analogue of the product moment correlation coefficient, Pearson’s r. Kendall’s \\(\\tau\\) is a part of a more general and consistent system of ranking methods, and has a direct interpretation, as the difference between the proportions of concordant and discordant pairs. In general, the numerical value of \\(\\rho\\) is greater than that of \\(\\tau\\). It is not possible to calculate \\(\\tau\\) from \\(\\rho\\) or \\(\\rho\\) from \\(\\tau\\), they measure different sorts of correlation. \\(\\rho\\) gives more weight to reversals of order when data are far apart in rank than when there is a reversal close together in rank, \\(\\tau\\) does not. However, in terms of tests of significance, both have the same power to reject a false null hypothesis, so for this purpose it does not matter which is used.”\n\nWe will illustrate estimating rank correlation using the data mod08_lung_function.csv, which has information about height and lung function collected from a sample of 120 adults.\nThe Spearman rank correlation coefficient is estimated as 0.75, demonstrating a positive association between height and FVC. The Kendall rank correlation coefficient is estimated as 0.56, again demonstrating a positive association between height and FVC.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#summary",
    "href": "09-non-parametrics.html#summary",
    "title": "9  Analysing non-normal data",
    "section": "9.7 Summary",
    "text": "9.7 Summary\nIn this module, we have presented methods to conduct a hypothesis test with data that are not normally distributed. Non-parametric methods do not assume any distribution for the data and use significance tests based on ranks or sign (or both). A non-parametric test is always less powerful than its equivalent parametric test if the data are normally distributed and so whenever possible parametric significance tests should be used. In some cases when data are not normally distributed with a reasonably large sample size, the data can be transformed (most commonly by log transformation) to make the distribution normal. A parametric significance test should then be used with the transformed data to test the hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#transforming-non-normally-distributed-variables-1",
    "href": "09-non-parametrics.html#transforming-non-normally-distributed-variables-1",
    "title": "9  Analysing non-normal data",
    "section": "9.8 Transforming non-normally distributed variables",
    "text": "9.8 Transforming non-normally distributed variables\nOne option for dealing with a non-normally distributed varaible is to transform it into its square, square root or logarithmic value. The new transformed variable may be normally distributed and therefore a parametric test can be used. First we check the distribution of the variable for normality, e.g. by plotting a density plot.\nYou can calculate a new, transformed, variable using Data &gt; Compute. For example, to create a new column of data based on the log of length of stay:\n\nclick into an empty column at the end of the spreadsheet\nclick Data &gt; Compute\nprovide a name for the new variable, here log(length of stay + 1)\nenter the formula for the new variable in the fx box, here LN(los + 1)\nhit enter to create the new column:\n\n\nYou can now check whether this logged variable is normally distributed as described in Module 2, for example by plotting a density plot as shown in Figure 9.2.\nTo obtain the back-transformed mean, we can use a calculator to calculate the exponential mean:\n\\(e^{3.407232} = 30.18\\)\nIf your transformed variable is approximately normally distributed, you can apply parametric tests such as the t-test. In the Worked Example 9.1 dataset, the variable infect (presence of nosocomial infection) is a binary categorical variable. To test the hypothesis that patients with nosocomial infection have a different length of stay to patients without infection, you can conduct a t-test on the log(length of stay + 1) variable. You will need to back transform your mean values, as shown in Worked Example 9.1 in the course notes when reporting your results.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-ranked-sum-test",
    "href": "09-non-parametrics.html#wilcoxon-ranked-sum-test",
    "title": "9  Analysing non-normal data",
    "section": "9.9 Wilcoxon ranked-sum test",
    "text": "9.9 Wilcoxon ranked-sum test\nThe Wilcoxon ranked-sum test will be demonstrated using the length of stay data in mod09_infection.rds. Here, our continuous variable is los and the grouping variable is infect. Note that jamovi uses calls the Wilcoxon ranked-sum test the Mann-Whitney U test. The two tests are equivalent.\nThe Wilcoxon ranked-sum test is conducted in Analyses &gt; T-Tests &gt; Independent Samples T-Test in jamovi. The screen is set up in the same way as for a two-sample t-test. To conduct the Wilcoxon ranked-sum test, untick the Student’s box and click the Mann-Whitney U box in the Tests section:\n\nNote that the result appears without any descriptive analyses. You should obtain the appropriate descriptive statistics in the usual way.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test",
    "href": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test",
    "title": "9  Analysing non-normal data",
    "section": "9.10 Wilcoxon matched-pairs signed-rank test",
    "text": "9.10 Wilcoxon matched-pairs signed-rank test\nThe Wilcoxon ranked-sum test is conducted in jamovi using Analyses &gt; T-Tests &gt; Paired Samples T-Test.\nWe will demonstrate using the dataset on the arthritis drug cross-over trial (mod09_arthritis.csv). Like the paired t-test the paired data need to be in separate columns.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#estimating-rank-correlation-coefficients",
    "href": "09-non-parametrics.html#estimating-rank-correlation-coefficients",
    "title": "9  Analysing non-normal data",
    "section": "9.11 Estimating rank correlation coefficients",
    "text": "9.11 Estimating rank correlation coefficients\nThe analyses for Spearman’s and Kendall’s rank correlation are conducted in similar ways using Regression &gt; Correlation Matrix:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#transforming-non-normally-distributed-variables-2",
    "href": "09-non-parametrics.html#transforming-non-normally-distributed-variables-2",
    "title": "9  Analysing non-normal data",
    "section": "9.12 Transforming non-normally distributed variables",
    "text": "9.12 Transforming non-normally distributed variables\nOne option for dealing with a non-normally distributed varaible is to transform it into its square, square root or logarithmic value. The new transformed variable may be normally distributed and therefore a parametric test can be used. First we check the distribution of the variable for normality, e.g. by plotting a histogram.\nYou can calculate a new, transformed, variable using standard commands. For example, to create a new column of data based on the log of length of stay:\n\nlibrary(jmv)\n\nhospital &lt;- readRDS(\"data/examples/mod09_infection.rds\")\n\nhospital$ln_los &lt;- log(hospital$los+1)\ndescriptives(data=hospital, vars=c(los, ln_los))\n\n\n DESCRIPTIVES\n\n Descriptives                                    \n ─────────────────────────────────────────────── \n                         los         ln_los      \n ─────────────────────────────────────────────── \n   N                          132          132   \n   Missing                      0            0   \n   Mean                  38.05303     3.407232   \n   Median                27.00000     3.332205   \n   Standard deviation    35.78057    0.7149892   \n   Minimum               0.000000     0.000000   \n   Maximum               244.0000     5.501258   \n ─────────────────────────────────────────────── \n\n\nYou can now check whether this logged variable is normally distributed as described in Module 2, for example by plotting a histogram as shown in Figure 9.2.\nTo obtain the back-transformed mean, we can use the exp command to anti-log the mean:\n\nexp(3.407232)\n\n[1] 30.18159\n\n\nIf your transformed variable is approximately normally distributed, you can apply parametric tests such as the t-test. In the Worked Example 9.1 dataset, the variable infect (presence of nosocomial infection) is a binary categorical variable. To test the hypothesis that patients with nosocomial infection have a different length of stay to patients without infection, you can conduct a t-test on the ln_los variable. You will need to back transform your mean values, as shown in Worked Example 9.1 in the course notes when reporting your results.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-ranked-sum-test-1",
    "href": "09-non-parametrics.html#wilcoxon-ranked-sum-test-1",
    "title": "9  Analysing non-normal data",
    "section": "9.13 Wilcoxon ranked-sum test",
    "text": "9.13 Wilcoxon ranked-sum test\nWe use the wilcox.test function to perform the Wilcoxon ranked-sum test:\nwilcox.test(continuous_variable ~ group_variable, data=df)\nThe Wilcoxon ranked-sum test will be demonstrated using the length of stay data in mod09_infection.rds. Here, out continuous variable is los and the grouping variable is infect.\n\nwilcox.test(los ~ infect, data=hospital)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  los by infect\nW = 949, p-value = 0.01413\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test-1",
    "href": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test-1",
    "title": "9  Analysing non-normal data",
    "section": "9.14 Wilcoxon matched-pairs signed-rank test",
    "text": "9.14 Wilcoxon matched-pairs signed-rank test\nThe wilcox.test function can also be used to conduct the Wilcoxon matched-pairs signed-rank test. The specification of the variables is a little different, in that each variable is specified as dataframe$variable:\nwilcox.test(df$continuous_variable_1, df$continuous_variable_1, paired=TRUE)\nWe will demonstrate using the dataset on the arthritis drug cross-over trial (mod09_arthritis.csv). Like the paired t-test the paired data need to be in separate columns.\n\narthritis &lt;- read.csv(\"data/examples/mod09_arthritis.csv\")\n\nwilcox.test(arthritis$drug_1, arthritis$drug_2, \n            paired=TRUE)\n\nWarning in wilcox.test.default(arthritis$drug_1, arthritis$drug_2, paired =\nTRUE): cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  arthritis$drug_1 and arthritis$drug_2\nV = 10.5, p-value = 0.04898\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "09-non-parametrics.html#estimating-rank-correlation-coefficients-1",
    "href": "09-non-parametrics.html#estimating-rank-correlation-coefficients-1",
    "title": "9  Analysing non-normal data",
    "section": "9.15 Estimating rank correlation coefficients",
    "text": "9.15 Estimating rank correlation coefficients\nThe analyses for Spearman’s and Kendall’s rank correlation are conducted in similar ways:\n\nlung &lt;- read.csv(\"data/examples/mod08_lung_function.csv\")\n\ncor.test(lung$Height, lung$FVC, method=\"spearman\")\n\nWarning in cor.test.default(lung$Height, lung$FVC, method = \"spearman\"): Cannot\ncompute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  lung$Height and lung$FVC\nS = 72796, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7472186 \n\ncor.test(lung$Height, lung$FVC, method=\"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  lung$Height and lung$FVC\nz = 8.818, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5611396",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysing non-normal data</span>"
    ]
  },
  {
    "objectID": "98.1-appendix1.html",
    "href": "98.1-appendix1.html",
    "title": "Appendix",
    "section": "",
    "text": "Analysis flowchart",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Altman, Douglas G. 1990. Practical Statistics for\nMedical Research. 1st ed. Boca Raton, Fla:\nChapman and Hall/CRC.\n\n\nArmitage, Peter, Geoffrey Berry, and J. N. S. Matthews. 2013.\nStatistical Methods in Medical\nResearch. 4th ed. Wiley-Blackwell.\n\n\nAssel, Melissa, Daniel Sjoberg, Andrew Elders, Xuemei Wang, Dezheng Huo,\nAlbert Botchway, Kristin Delfino, et al. 2019. “Guidelines for\nReporting of Statistics for Clinical Research in Urology.”\nBJU International 123 (3): 401–10. https://doi.org/10.1111/bju.14640.\n\n\nAustralian Bureau of Statistics. Thu, 10/10/2024 - 11:30. “Causes\nof Death, Australia, 2023.”\nhttps://www.abs.gov.au/statistics/health/causes-death/causes-death-australia/latest-release.\n\n\nAustralian Institute of Health and Welfare. 2024. “Australia’s\nMothers and Babies.” Australian Institute of Health and\nWelfare.\nhttps://www.aihw.gov.au/reports/mothers-babies/australias-mothers-babies/contents/about.\n\n\n———. 2025. “Australia’s Health.”\nhttps://www.aihw.gov.au/reports-data/australias-health.\n\n\nBland, Martin. 2015. An Introduction to Medical\nStatistics. 4th Edition. Oxford, New York: Oxford University\nPress.\n\n\nBoers, Maarten. 2018. “Graphics and Statistics for Cardiology:\nDesigning Effective Tables for Presentation and Publication.”\nHeart 104 (3): 192–200. https://doi.org/10.1136/heartjnl-2017-311581.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001.\n“Interval Estimation for a Binomial\nProportion.” Statistical Science 16 (2): 101–17.\nhttps://www.jstor.org/stable/2676784.\n\n\nCole, T. J. 2015. “Too Many Digits: The Presentation of Numerical\nData.” Archives of Disease in Childhood 100 (7): 608–9.\nhttps://doi.org/10.1136/archdischild-2014-307149.\n\n\nDeeks, Jon. 1998. “When Can Odds Ratios Mislead?”\nBMJ 317 (7166): 1155. https://doi.org/10.1136/bmj.317.7166.1155a.\n\n\nDelacre, Marie, Daniël Lakens, and Christophe Leys. 2017. “Why\nPsychologists Should by Default Use Welch’s\nt-Test Instead of Student’s t-Test” 30\n(1): 92. https://doi.org/10.5334/irsp.82.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of\nMedical Statistics. 2nd edition. Malden, Mass:\nWiley-Blackwell.\n\n\nRuxton, Graeme D. 2006. “The Unequal Variance t-Test Is an\nUnderused Alternative to Student’s t-Test and the\nMann–Whitney U Test.” Behavioral\nEcology 17 (4): 688–90. https://doi.org/10.1093/beheco/ark016.\n\n\nSchmidt, Carsten Oliver, and Thomas Kohlmann. 2008. “When to Use\nthe Odds Ratio or the Relative Risk?” International Journal\nof Public Health 53 (3): 165–67. https://doi.org/10.1007/s00038-008-7068-3.\n\n\nTherneau, Terry M., and Patricia M. Grambsch. 2010. Modeling\nSurvival Data: Extending the Cox\nModel. New York Berlin Heidelberg: Springer.\n\n\nVickers, Andrew J., Melissa J. Assel, Daniel D. Sjoberg, Rui Qin, Zhiguo\nZhao, Tatsuki Koyama, Albert Botchway, et al. 2020. “Guidelines\nfor Reporting of Figures and\nTables for Clinical Research in\nUrology.” European Urology, May. https://doi.org/10.1016/j.eururo.2020.04.048.\n\n\nWebb, Penny, Chris Bain, and Andrew Page. 2016. Essential\nEpidemiology: An Introduction for\nStudents and Health Professionals. 3rd\nedition. Cambridge: Cambridge University Press.\n\n\nWest, Robert M. 2021. “Best Practice in Statistics:\nUse the Welch t-Test When Testing the\nDifference Between Two Groups.” Annals of Clinical\nBiochemistry 58 (4): 267–69. https://doi.org/10.1177/0004563221992088.",
    "crumbs": [
      "References"
    ]
  }
]